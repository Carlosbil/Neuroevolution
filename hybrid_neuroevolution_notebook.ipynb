{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Synchronous Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements the complete hybrid neuroevolution process synchronously, without the need for databases or external Kafka services. The system combines genetic algorithms with convolutional neural networks to evolve optimal architectures.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **Synchronous processing**: Complete workflow executed in a single session\n",
    "- **Configurable dataset**: Supports MNIST by default or custom dataset\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of CNN architectures\n",
    "2. Evaluate fitness of each individual\n",
    "3. Select best architectures (top 50%)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import time  # Added for timeout functionality\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import threading  # Added for concurrent model training\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 10,            # Population size\n",
    "    'max_generations': 30,            # Maximum number of generations\n",
    "    'fitness_threshold': 99.9,        # Target fitness (% accuracy)\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.35,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.35,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection\n",
    "    'dataset': 'MNIST',             # Options: 'MNIST', 'CIFAR10', 'CUSTOM'\n",
    "\n",
    "    # Dataset parameters (auto-configured based on dataset)\n",
    "    'num_channels': 3,                # Input channels (1=grayscale, 3=RGB)\n",
    "    'px_h': 32,                       # Image height\n",
    "    'px_w': 32,                       # Image width\n",
    "    'num_classes': 10,                # Number of classes\n",
    "    'batch_size': 128,                # Batch size\n",
    "    'test_split': 0.35,               # Validation percentage (for CUSTOM)\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 12,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.01,            # Base learning rate (used only if genome doesn't override)\n",
    "    'early_stopping_patience': 1000,  # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping (new)\n",
    "    'epoch_patience': 3,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.2,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Allowed architecture range\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 7,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 7,\n",
    "    'min_filters': 2,\n",
    "    'max_filters': 256,\n",
    "    'min_fc_nodes': 128,\n",
    "    'max_fc_nodes': 2048,\n",
    "\n",
    "    # Custom dataset configuration (only used if dataset='CUSTOM')\n",
    "    'dataset_path': None,             # Custom dataset path\n",
    "}\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIGS = {\n",
    "    'MNIST': {\n",
    "        'num_channels': 1,\n",
    "        'px_h': 28,\n",
    "        'px_w': 28,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.1307,), 'std': (0.3081,)}\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'num_channels': 3,\n",
    "        'px_h': 32,\n",
    "        'px_w': 32,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.4914, 0.4822, 0.4465), 'std': (0.2023, 0.1994, 0.2010)}\n",
    "    },\n",
    "    'CUSTOM': {\n",
    "        'num_channels': 1,  # Default, will be overridden\n",
    "        'px_h': 28,         # Default, will be overridden\n",
    "        'px_w': 28,         # Default, will be overridden\n",
    "        'num_classes': 10,  # Default, will be overridden\n",
    "        'normalization': {'mean': (0.5,), 'std': (0.5,)}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Auto-configure based on selected dataset\n",
    "def configure_dataset(config, dataset_name):\n",
    "    \"\"\"Auto-configures dataset parameters based on selected dataset.\"\"\"\n",
    "    if dataset_name in DATASET_CONFIGS:\n",
    "        dataset_config = DATASET_CONFIGS[dataset_name]\n",
    "        config['num_channels'] = dataset_config['num_channels']\n",
    "        config['px_h'] = dataset_config['px_h']\n",
    "        config['px_w'] = dataset_config['px_w']\n",
    "        config['num_classes'] = dataset_config['num_classes']\n",
    "        config['_normalization'] = dataset_config['normalization']\n",
    "    return config\n",
    "\n",
    "# Configure the selected dataset\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled):\")\n",
    "print(f\"   Selected dataset: {CONFIG['dataset']}\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.startswith('_'):  # Hide internal config\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"Available datasets: {list(DATASET_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### Información sobre Datasets Disponibles\n",
    "\n",
    "**MNIST**: \n",
    "- Dígitos escritos a mano (0-9)\n",
    "- Imágenes en escala de grises (1 canal)\n",
    "- Tamaño: 28x28 píxeles\n",
    "- Dificultad: **Fácil** - Ideal para pruebas rápidas\n",
    "- Fitness objetivo recomendado: >95%\n",
    "\n",
    "**CIFAR-10**: \n",
    "- Objetos del mundo real (aviones, coches, pájaros, etc.)\n",
    "- Imágenes en color (3 canales RGB)\n",
    "- Tamaño: 32x32 píxeles\n",
    "- Dificultad: **Media-Alta** - Más realista y desafiante\n",
    "- Fitness objetivo recomendado: >80%\n",
    "- Clases: plane, car, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "**CUSTOM**: \n",
    "- Tu propio dataset\n",
    "- Configuración manual requerida\n",
    "- Estructura de carpetas por clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_type = config['dataset']\n",
    "    \n",
    "    if dataset_type == 'CUSTOM' and config['dataset_path']:\n",
    "        print(f\"Loading custom dataset from: {config['dataset_path']}\")\n",
    "        \n",
    "        # Transformations for custom dataset\n",
    "        if config['num_channels'] == 1:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        else:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        \n",
    "        # Load dataset from folders organized by class\n",
    "        full_dataset = datasets.ImageFolder(root=config['dataset_path'], transform=transform)\n",
    "        \n",
    "        # Split into train and test\n",
    "        train_size = int((1 - config['test_split']) * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "        \n",
    "        print(f\"Custom dataset loaded:\")\n",
    "        print(f\"   Classes found: {len(full_dataset.classes)}\")\n",
    "        print(f\"   Total samples: {len(full_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'MNIST':\n",
    "        print(\"Loading MNIST dataset...\")\n",
    "        \n",
    "        # Transformations for MNIST\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load MNIST\n",
    "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        \n",
    "        print(f\"MNIST dataset loaded:\")\n",
    "        print(f\"   Classes: {len(train_dataset.classes)}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'CIFAR10':\n",
    "        print(\"Loading CIFAR-10 dataset...\")\n",
    "        \n",
    "        # Transformations for CIFAR-10\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),  # Data augmentation for training\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load CIFAR-10\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "        \n",
    "        print(f\"CIFAR-10 dataset loaded:\")\n",
    "        print(f\"   Classes: {train_dataset.classes}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_type}' not supported. Available: MNIST, CIFAR10, CUSTOM\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load the dataset\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Get a sample to verify dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_data, sample_labels = sample_batch\n",
    "print(f\"\\nDataset loaded successfully:\")\n",
    "print(f\"   Batch shape: {sample_data.shape}\")\n",
    "print(f\"   Data type: {sample_data.dtype}\")\n",
    "print(f\"   Device: {sample_data.device}\")\n",
    "print(f\"   Value range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\n",
    "\n",
    "# Show some class information for CIFAR-10\n",
    "if CONFIG['dataset'] == 'CIFAR10':\n",
    "    cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    print(f\"   CIFAR-10 classes: {cifar10_classes}\")\n",
    "    unique_labels = torch.unique(sample_labels)\n",
    "    print(f\"   Labels in this batch: {unique_labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable CNN class that can be dynamically configured\n",
    "    according to genome parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds convolutional layers according to genome.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = self.config['num_channels']\n",
    "        \n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            out_channels = self.genome['filters'][i]\n",
    "            kernel_size = self.genome['kernel_sizes'][i]\n",
    "            \n",
    "            # Convolutional layer\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Batch normalization\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            # Activation function\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # Max pooling (except in last layer)\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d(2, 1))  # Stride 1 in last layer\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"Calculates output size after convolutional layers.\"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['px_h'], self.config['px_w'])\n",
    "        \n",
    "        # Pass through convolutional layers\n",
    "        x = dummy_input\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten and get size\n",
    "        return x.view(-1).shape[0]\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        \n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation except on last layer\n",
    "            if i < len(self.fc_layers) - 1 and not isinstance(layer, nn.Dropout):\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be19766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"Creates a random genome within specified ranges.\"\"\"\n",
    "    # Number of layers\n",
    "    num_conv_layers = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "    num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "    # Filters for each convolutional layer\n",
    "    filters = [random.randint(config['min_filters'], config['max_filters']) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Kernel sizes\n",
    "    kernel_sizes = [random.choice([3, 5, 7]) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Nodes in fully connected layers\n",
    "    fc_nodes = [random.randint(config['min_fc_nodes'], config['max_fc_nodes']) for _ in range(num_fc_layers)]\n",
    "\n",
    "    # Activation functions for each layer\n",
    "    activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "    # Other parameters\n",
    "    dropout_rate = random.uniform(0.1, 0.5)\n",
    "    learning_rate = random.choice([0.001, 0.0001, 0.01, 0.005])\n",
    "    optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    genome = {\n",
    "        'num_conv_layers': num_conv_layers,\n",
    "        'num_fc_layers': num_fc_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'fc_nodes': fc_nodes,\n",
    "        'activations': activations,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "    return genome\n",
    "\n",
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"Applies mutation to a genome using adaptive mutation rate.\"\"\"\n",
    "    mutated_genome = copy.deepcopy(genome)\n",
    "    mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "    # Mutate number of convolutional layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "        num_conv = mutated_genome['num_conv_layers']\n",
    "        mutated_genome['filters'] = mutated_genome['filters'][:num_conv]\n",
    "        mutated_genome['kernel_sizes'] = mutated_genome['kernel_sizes'][:num_conv]\n",
    "        while len(mutated_genome['filters']) < num_conv:\n",
    "            mutated_genome['filters'].append(random.randint(config['min_filters'], config['max_filters']))\n",
    "        while len(mutated_genome['kernel_sizes']) < num_conv:\n",
    "            mutated_genome['kernel_sizes'].append(random.choice([1, 3, 5, 7]))\n",
    "\n",
    "    # Mutate filters\n",
    "    for i in range(len(mutated_genome['filters'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "    # Mutate kernel sizes\n",
    "    for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['kernel_sizes'][i] = random.choice([1, 3, 5, 7])\n",
    "\n",
    "    # Mutate number of FC layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "        num_fc = mutated_genome['num_fc_layers']\n",
    "        mutated_genome['fc_nodes'] = mutated_genome['fc_nodes'][:num_fc]\n",
    "        while len(mutated_genome['fc_nodes']) < num_fc:\n",
    "            mutated_genome['fc_nodes'].append(random.randint(config['min_fc_nodes'], config['max_fc_nodes']))\n",
    "\n",
    "    # Mutate FC nodes\n",
    "    for i in range(len(mutated_genome['fc_nodes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "    # Mutate activation functions\n",
    "    for i in range(len(mutated_genome['activations'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['dropout_rate'] = random.uniform(0.1, 0.8)\n",
    "\n",
    "    # Mutate learning rate\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['learning_rate'] = random.choice([0.001, 0.0001, 0.01, 0.005, 0.000001, 0.05, 0.00005, 0.0005])\n",
    "\n",
    "    # Mutate optimizer\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    mutated_genome['fitness'] = 0.0\n",
    "    return mutated_genome\n",
    "\n",
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Performs crossover between two genomes.\"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "\n",
    "    # Crossover scalar parameters\n",
    "    for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer']:\n",
    "        if random.random() < 0.5:\n",
    "            child1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "    # Crossover lists (random cut point)\n",
    "    for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "        if random.random() < 0.5:\n",
    "            list1 = child1[list_key]\n",
    "            list2 = child2[list_key]\n",
    "            if len(list1) > 1 and len(list2) > 1:\n",
    "                point1 = random.randint(1, len(list1) - 1)\n",
    "                point2 = random.randint(1, len(list2) - 1)\n",
    "                child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    return child1, child2\n",
    "\n",
    "print(\"Genetic functions updated for adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class HybridNeuroevolution:\n",
    "    \"\"\"Main class that implements hybrid neuroevolution with adaptive mutation & epoch interleaved eval + Threading.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict, train_loader: DataLoader, test_loader: DataLoader):\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_individual = None\n",
    "        self.fitness_history = []\n",
    "        self.generation_stats = []\n",
    "        # Threading configuration\n",
    "        self.max_concurrent_training = config.get('max_concurrent_training', 2)  # Configurable, default 2\n",
    "        self.training_semaphore = threading.Semaphore(self.max_concurrent_training)\n",
    "        self.fitness_lock = threading.Lock()  # Para acceso seguro a variables compartidas\n",
    "\n",
    "    def initialize_population(self):\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "\n",
    "    def _train_one_epoch(self, model, optimizer, criterion, genome_id: str, epoch: int, thread_name: str):\n",
    "        \"\"\"Train one epoch with error handling.\"\"\"\n",
    "        try:\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            max_batches = min(len(self.train_loader), self.config['early_stopping_patience'])\n",
    "            \n",
    "            for data, target in self.train_loader:\n",
    "                try:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    # Check for invalid loss\n",
    "                    if torch.isnan(loss) or torch.isinf(loss):\n",
    "                        print(f\"{thread_name}: WARNING - Invalid loss detected, skipping batch\")\n",
    "                        continue\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    if batch_count >= max_batches:\n",
    "                        break\n",
    "                        \n",
    "                except Exception as batch_error:\n",
    "                    print(f\"{thread_name}: ERROR in training batch: {batch_error}\")\n",
    "                    continue\n",
    "            \n",
    "            if batch_count == 0:\n",
    "                print(f\"{thread_name}: ERROR - No valid batches processed in training\")\n",
    "                return None\n",
    "            \n",
    "            avg_loss = running_loss / batch_count\n",
    "            print(f\"{thread_name}: Train Epoch {epoch}: loss={avg_loss:.4f} ({batch_count} batches)\")\n",
    "            return avg_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{thread_name}: ERROR in _train_one_epoch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _evaluate(self, model, criterion, genome_id: str, epoch: int, thread_name: str):\n",
    "        \"\"\"Evaluate model with error handling.\"\"\"\n",
    "        try:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            eval_batches = 0\n",
    "            max_eval_batches = min(len(self.test_loader), 20)\n",
    "            total_eval_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in self.test_loader:\n",
    "                    try:\n",
    "                        data, target = data.to(device), target.to(device)\n",
    "                        output = model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        \n",
    "                        # Check for invalid loss\n",
    "                        if torch.isnan(loss) or torch.isinf(loss):\n",
    "                            print(f\"{thread_name}: WARNING - Invalid eval loss detected, skipping batch\")\n",
    "                            continue\n",
    "                        \n",
    "                        total_eval_loss += loss.item()\n",
    "                        _, predicted = torch.max(output, 1)\n",
    "                        total += target.size(0)\n",
    "                        correct += (predicted == target).sum().item()\n",
    "                        eval_batches += 1\n",
    "                        \n",
    "                        if eval_batches >= max_eval_batches:\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as batch_error:\n",
    "                        print(f\"{thread_name}: ERROR in evaluation batch: {batch_error}\")\n",
    "                        continue\n",
    "            \n",
    "            if eval_batches == 0 or total == 0:\n",
    "                print(f\"{thread_name}: ERROR - No valid evaluation batches processed\")\n",
    "                return None, None\n",
    "            \n",
    "            accuracy = 100.0 * correct / total\n",
    "            avg_eval_loss = total_eval_loss / eval_batches\n",
    "            print(f\"{thread_name}: Eval  Epoch {epoch}: acc={accuracy:.2f}% loss={avg_eval_loss:.4f} ({eval_batches} batches)\")\n",
    "            return accuracy, avg_eval_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{thread_name}: ERROR in _evaluate: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def evaluate_fitness(self, genome: dict, thread_name: str = \"MainThread\") -> float:\n",
    "        \"\"\"Thread-safe fitness evaluation method with robust error handling.\"\"\"\n",
    "        evaluation_start = time.time()\n",
    "        max_evaluation_time = 180  # 3 minutes max per fitness evaluation\n",
    "        \n",
    "        try:\n",
    "            # Validate genome before creating model\n",
    "            if not self._validate_genome(genome):\n",
    "                print(f\"{thread_name}: ERROR - Invalid genome structure for {genome['id']}\")\n",
    "                return 0.0\n",
    "            \n",
    "            print(f\"{thread_name}: Creating model for genome {genome['id']}...\")\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            \n",
    "            # Validate model creation\n",
    "            if model is None:\n",
    "                print(f\"{thread_name}: ERROR - Failed to create model for {genome['id']}\")\n",
    "                return 0.0\n",
    "            \n",
    "            optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "            optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            best_acc = 0.0\n",
    "            best_epoch = -1\n",
    "            patience_left = self.config['epoch_patience']\n",
    "            last_improvement_acc = 0.0\n",
    "\n",
    "            max_epochs = self.config['num_epochs']\n",
    "            print(f\"{thread_name}: Training/Evaluating model {genome['id']} (max {max_epochs} epochs interleaved)\")\n",
    "\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                # Check time limit\n",
    "                elapsed_time = time.time() - evaluation_start\n",
    "                if elapsed_time > max_evaluation_time:\n",
    "                    print(f\"{thread_name}: TIMEOUT - Stopping evaluation after {elapsed_time:.1f}s (limit: {max_evaluation_time}s)\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    # 1) Train epoch with error handling\n",
    "                    train_loss = self._train_one_epoch(model, optimizer, criterion, genome['id'], epoch, thread_name)\n",
    "                    if train_loss is None or np.isnan(train_loss) or np.isinf(train_loss):\n",
    "                        print(f\"{thread_name}: ERROR - Invalid training loss, stopping evaluation\")\n",
    "                        break\n",
    "                    \n",
    "                    # 2) Evaluate right after training epoch\n",
    "                    acc, eval_loss = self._evaluate(model, criterion, genome['id'], epoch, thread_name)\n",
    "                    if acc is None or np.isnan(acc) or np.isinf(acc):\n",
    "                        print(f\"{thread_name}: ERROR - Invalid accuracy, stopping evaluation\")\n",
    "                        break\n",
    "\n",
    "                    # Early stopping logic based on accuracy improvement\n",
    "                    improvement = acc - last_improvement_acc\n",
    "                    if improvement >= self.config['improvement_threshold']:\n",
    "                        patience_left = self.config['epoch_patience']\n",
    "                        last_improvement_acc = acc\n",
    "                    else:\n",
    "                        patience_left -= 1\n",
    "\n",
    "                    if acc > best_acc:\n",
    "                        best_acc = acc\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    print(f\"{thread_name}: -> Acc={acc:.2f}% (best={best_acc:.2f}% at epoch {best_epoch}) patience_left={patience_left}\")\n",
    "\n",
    "                    if patience_left <= 0:\n",
    "                        print(f\"{thread_name}: Early stopping triggered (no significant improvement)\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as epoch_error:\n",
    "                    print(f\"{thread_name}: ERROR in epoch {epoch}: {epoch_error}\")\n",
    "                    print(f\"{thread_name}: Continuing to next epoch...\")\n",
    "                    continue\n",
    "\n",
    "            total_time = time.time() - evaluation_start\n",
    "            print(f\"{thread_name}: Final fitness for {genome['id']}: {best_acc:.2f}% (best epoch {best_epoch}) in {total_time:.1f}s\")\n",
    "            \n",
    "            # Clean up model to free memory\n",
    "            del model, optimizer\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            return best_acc\n",
    "            \n",
    "        except Exception as e:\n",
    "            total_time = time.time() - evaluation_start\n",
    "            print(f\"{thread_name}: CRITICAL ERROR evaluating genome {genome['id']} after {total_time:.1f}s: {e}\")\n",
    "            print(f\"{thread_name}: Exception type: {type(e).__name__}\")\n",
    "            logger.warning(f\"Error evaluating genome {genome['id']}: {e}\")\n",
    "            \n",
    "            # Clean up in case of error\n",
    "            try:\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return 0.0\n",
    "\n",
    "    def _validate_genome(self, genome: dict) -> bool:\n",
    "        \"\"\"Validates genome structure to prevent errors.\"\"\"\n",
    "        required_keys = ['num_conv_layers', 'num_fc_layers', 'filters', 'kernel_sizes', \n",
    "                        'fc_nodes', 'activations', 'dropout_rate', 'learning_rate', 'optimizer']\n",
    "        \n",
    "        for key in required_keys:\n",
    "            if key not in genome:\n",
    "                return False\n",
    "        \n",
    "        # Basic sanity checks\n",
    "        if genome['num_conv_layers'] <= 0 or genome['num_fc_layers'] <= 0:\n",
    "            return False\n",
    "        if len(genome['filters']) != genome['num_conv_layers']:\n",
    "            return False\n",
    "        if len(genome['fc_nodes']) != genome['num_fc_layers']:\n",
    "            return False\n",
    "        if genome['dropout_rate'] < 0 or genome['dropout_rate'] > 1:\n",
    "            return False\n",
    "        if genome['learning_rate'] <= 0:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def _worker_evaluate_genome(self, genome: dict, index: int, total: int, results: list):\n",
    "        \"\"\"Worker function that evaluates a single genome in a thread with semaphore control and robust error handling.\"\"\"\n",
    "        thread_name = f\"Thread-{index+1}\"\n",
    "        fitness = 0.0\n",
    "        \n",
    "        # Acquire semaphore to limit concurrent training\n",
    "        semaphore_acquired = False\n",
    "        try:\n",
    "            # Timeout for semaphore acquisition (avoid deadlock)\n",
    "            semaphore_acquired = self.training_semaphore.acquire(timeout=30)\n",
    "            if not semaphore_acquired:\n",
    "                print(f\"{thread_name}: ERROR - Could not acquire semaphore within 30s\")\n",
    "                with self.fitness_lock:\n",
    "                    genome['fitness'] = 0.0\n",
    "                    results[index] = 0.0\n",
    "                return\n",
    "            \n",
    "            print(f\"{thread_name}: iniciando evaluación (máx {self.max_concurrent_training} concurrentes)...\")\n",
    "            print(f\"{thread_name}: Evaluating individual {index+1}/{total} (ID: {genome['id']})\")\n",
    "            print(f\"{thread_name}: Architecture: {genome['num_conv_layers']} conv + {genome['num_fc_layers']} fc, opt={genome['optimizer']}, lr={genome['learning_rate']}\")\n",
    "            \n",
    "            # Set a timeout for the entire evaluation process\n",
    "            evaluation_start = time.time()\n",
    "            max_evaluation_time = 240  # 4 minutes max per model\n",
    "            \n",
    "            # Evaluate fitness with timeout protection\n",
    "            try:\n",
    "                fitness = self.evaluate_fitness(genome, thread_name)\n",
    "                evaluation_time = time.time() - evaluation_start\n",
    "                \n",
    "                if evaluation_time > max_evaluation_time:\n",
    "                    print(f\"{thread_name}: WARNING - Evaluation took {evaluation_time:.1f}s (exceeded {max_evaluation_time}s)\")\n",
    "                \n",
    "            except Exception as eval_error:\n",
    "                print(f\"{thread_name}: ERROR during fitness evaluation: {eval_error}\")\n",
    "                print(f\"{thread_name}: Setting fitness to 0.0 due to evaluation error\")\n",
    "                fitness = 0.0\n",
    "            \n",
    "            # Thread-safe update of results\n",
    "            with self.fitness_lock:\n",
    "                genome['fitness'] = fitness\n",
    "                results[index] = fitness\n",
    "                print(f\"{thread_name}: Fitness obtained: {fitness:.2f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{thread_name}: CRITICAL ERROR durante evaluación: {e}\")\n",
    "            print(f\"{thread_name}: Exception type: {type(e).__name__}\")\n",
    "            with self.fitness_lock:\n",
    "                genome['fitness'] = 0.0\n",
    "                results[index] = 0.0\n",
    "        finally:\n",
    "            if semaphore_acquired:\n",
    "                print(f\"{thread_name}: terminado, liberando slot de entrenamiento.\")\n",
    "                self.training_semaphore.release()\n",
    "            else:\n",
    "                print(f\"{thread_name}: terminado SIN liberar semáforo (no se pudo adquirir).\")\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        \"\"\"Thread-safe population evaluation with concurrent model training and timeout protection.\"\"\"\n",
    "        print(f\"\\nEvaluating population with threading (Generation {self.generation})...\")\n",
    "        print(f\"Processing {len(self.population)} individuals with max {self.max_concurrent_training} concurrent training...\")\n",
    "        \n",
    "        # Initialize results list\n",
    "        fitness_scores = [0.0] * len(self.population)\n",
    "        best_fitness_so_far = 0.0\n",
    "        \n",
    "        # Create and start threads\n",
    "        threads = []\n",
    "        for i, genome in enumerate(self.population):\n",
    "            thread = threading.Thread(\n",
    "                target=self._worker_evaluate_genome,\n",
    "                args=(genome, i, len(self.population), fitness_scores)\n",
    "            )\n",
    "            thread.daemon = True  # Make threads daemon so they don't block program exit\n",
    "            threads.append(thread)\n",
    "        \n",
    "        # Start all threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for all threads to complete with timeout\n",
    "        timeout_per_thread = 300  # 5 minutes per thread max\n",
    "        max_total_timeout = timeout_per_thread * 2  # Total timeout for all threads\n",
    "        start_time = time.time()\n",
    "        \n",
    "        completed_threads = 0\n",
    "        failed_threads = []\n",
    "        \n",
    "        for i, thread in enumerate(threads):\n",
    "            remaining_time = max_total_timeout - (time.time() - start_time)\n",
    "            if remaining_time <= 0:\n",
    "                print(f\"WARNING: Global timeout reached, remaining threads will be abandoned\")\n",
    "                break\n",
    "                \n",
    "            thread_timeout = min(timeout_per_thread, remaining_time)\n",
    "            thread.join(timeout=thread_timeout)\n",
    "            \n",
    "            if thread.is_alive():\n",
    "                print(f\"WARNING: Thread-{i+1} timed out after {thread_timeout:.1f}s - marking as failed\")\n",
    "                failed_threads.append(i)\n",
    "                # Set failed thread result to 0.0\n",
    "                with self.fitness_lock:\n",
    "                    if i < len(self.population):\n",
    "                        self.population[i]['fitness'] = 0.0\n",
    "                        fitness_scores[i] = 0.0\n",
    "            else:\n",
    "                completed_threads += 1\n",
    "                print(f\"Thread-{i+1} completed successfully ({completed_threads}/{len(threads)})\")\n",
    "        \n",
    "        # Check for any remaining alive threads\n",
    "        alive_threads = [i for i, thread in enumerate(threads) if thread.is_alive()]\n",
    "        if alive_threads:\n",
    "            print(f\"WARNING: {len(alive_threads)} threads still alive after timeout: {alive_threads}\")\n",
    "            print(\"These threads will be abandoned and their fitness set to 0.0\")\n",
    "            for i in alive_threads:\n",
    "                with self.fitness_lock:\n",
    "                    if i < len(self.population):\n",
    "                        self.population[i]['fitness'] = 0.0\n",
    "                        fitness_scores[i] = 0.0\n",
    "        \n",
    "        print(f\"Threading summary: {completed_threads}/{len(threads)} completed, {len(failed_threads)} failed/timed out\")\n",
    "        \n",
    "        # Calculate generation statistics\n",
    "        if fitness_scores:\n",
    "            avg_fitness = np.mean(fitness_scores)\n",
    "            max_fitness = np.max(fitness_scores)\n",
    "            min_fitness = np.min(fitness_scores)\n",
    "            std_fitness = np.std(fitness_scores)\n",
    "        else:\n",
    "            avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "        stats = {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': avg_fitness,\n",
    "            'max_fitness': max_fitness,\n",
    "            'min_fitness': min_fitness,\n",
    "            'std_fitness': std_fitness\n",
    "        }\n",
    "        self.generation_stats.append(stats)\n",
    "        self.fitness_history.append(max_fitness)\n",
    "\n",
    "        best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "        if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "            self.best_individual = copy.deepcopy(best_genome)\n",
    "            print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "        print(f\"\\nGENERATION {self.generation} STATISTICS (Threading completed):\")\n",
    "        print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "        print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "        print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "        print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "        print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "        print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Concurrent training slots used: {self.max_concurrent_training}\")\n",
    "\n",
    "    def selection_and_reproduction(self):\n",
    "        print(f\"\\nStarting selection and reproduction...\")\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "        elite = self.population[:elite_size]\n",
    "        print(f\"Selecting {elite_size} elite individuals:\")\n",
    "        for i, individual in enumerate(elite):\n",
    "            print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "        new_population = copy.deepcopy(elite)\n",
    "        offspring_needed = self.config['population_size'] - len(new_population)\n",
    "        print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "        offspring_created = 0\n",
    "        while len(new_population) < self.config['population_size']:\n",
    "            parent1 = self.tournament_selection()\n",
    "            parent2 = self.tournament_selection()\n",
    "            child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "            child1 = mutate_genome(child1, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child1)\n",
    "            child2 = mutate_genome(child2, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child2)\n",
    "            offspring_created += 2\n",
    "            if offspring_created % 4 == 0:\n",
    "                print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "        self.population = new_population[:self.config['population_size']]\n",
    "        print(f\"New generation created with {len(self.population)} individuals\")\n",
    "        print(f\"   Elite preserved: {elite_size}\")\n",
    "        print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "    def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "        tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "        return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "    def _update_adaptive_mutation(self):\n",
    "        # Diversity measured via std of fitness in last generation\n",
    "        if not self.generation_stats:\n",
    "            self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "            return\n",
    "        last_std = self.generation_stats[-1]['std_fitness']\n",
    "        # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "        # Normalize std roughly assuming fitness in [0,100]\n",
    "        diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "        # Invert: low diversity (small std) should raise mutation\n",
    "        inverted = 1 - diversity_factor\n",
    "        new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "        new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "        self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "        print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "    def check_convergence(self) -> bool:\n",
    "        if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "            print(f\"Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "            return True\n",
    "        if self.generation >= self.config['max_generations']:\n",
    "            print(f\"Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "            return True\n",
    "        if len(self.fitness_history) >= 3:\n",
    "            recent = self.fitness_history[-3:]\n",
    "            if max(recent) - min(recent) < 0.5:\n",
    "                print(\"Stagnation detected in last 3 generations\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def evolve(self) -> dict:\n",
    "        print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + threading)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "        print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "        print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "        print(f\"   Max concurrent training: {self.max_concurrent_training} models\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(\"=\"*60)\n",
    "        self.initialize_population()\n",
    "        while not self.check_convergence():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GENERATION {self.generation}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            self.evaluate_population()\n",
    "            if self.check_convergence():\n",
    "                break\n",
    "            self._update_adaptive_mutation()\n",
    "            self.selection_and_reproduction()\n",
    "            self.generation += 1\n",
    "            print(f\"\\nPreparing for next generation...\")\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVOLUTION COMPLETED!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best individual found:\")\n",
    "        print(f\"   ID: {self.best_individual['id']}\")\n",
    "        print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Origin generation: {self.generation}\")\n",
    "        print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "        print(f\"   Threading efficiency: {self.max_concurrent_training} concurrent training slots\")\n",
    "        return self.best_individual\n",
    "\n",
    "print(\"HybridNeuroevolution class updated with adaptive mutation, interleaved early stopping, and threading support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACIÓN DE DATASET - MODIFICAR AQUÍ\n",
    "# ==========================================\n",
    "\n",
    "# Para cambiar el dataset, modifica la línea correspondiente y ejecuta esta celda:\n",
    "\n",
    "# Opción 1: Usar MNIST (28x28, grayscale, 10 classes)\n",
    "# CONFIG['dataset'] = 'MNIST'\n",
    "\n",
    "# Opción 2: Usar CIFAR-10 (32x32, RGB, 10 classes) - RECOMENDADO para mayor challenge\n",
    "# CONFIG['dataset'] = 'CIFAR10'\n",
    "\n",
    "# Opción 3: Usar dataset personalizado\n",
    "# CONFIG['dataset'] = 'CUSTOM'\n",
    "# CONFIG['dataset_path'] = r'E:\\Neuroevolution\\data\\phd_data'  # Ajustar ruta según tu dataset\n",
    "\n",
    "# ==========================================\n",
    "# OTRAS CONFIGURACIONES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# Reconfigurar el dataset con los nuevos parámetros\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "print(\"Current configuration:\")\n",
    "print(f\"   Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"   Image size: {CONFIG['px_h']}x{CONFIG['px_w']}x{CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Recargar el dataset con la nueva configuración\n",
    "print(f\"\\nReloading dataset with new configuration...\")\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Create system instance\n",
    "neuroevolution = HybridNeuroevolution(CONFIG, train_loader, test_loader)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nProcess completed at {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   • Errors in model architecture\")\n",
    "        print(f\"   • Memory problems (GPU/RAM)\")\n",
    "        print(f\"   • Invalid hyperparameter configurations\")\n",
    "        print(f\"   • Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"        BEST EVOLVED ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Convolutional Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS:\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv{i+1}: {filters} filters, kernel {kernel}x{kernel}, activation {activation}\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (classes)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.4f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(best_genome['activations'])}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Convolutional layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   TARGET: OK Fitness objective REACHED ({best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   TARGET: ERROR Fitness objective NOT reached ({best_genome['fitness']:.2f}% < {config['fitness_threshold']}%)\")\n",
    "    \n",
    "    print(f\"   TIME: Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'config_used': config,\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\nHYBRID NEUROEVOLUTION COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2964f-0098-4c42-bf72-e66c4ca0ed5a",
   "metadata": {},
   "source": [
    "# Demostración de Threading en Hybrid Neuroevolution\n",
    "\n",
    "Este notebook ha sido modificado para implementar **entrenamiento concurrente** de modelos usando threading, similar al patrón mostrado en `threading_demo_notebook.ipynb`.\n",
    "\n",
    "## Principales modificaciones implementadas:\n",
    "\n",
    "### 1. **Threading Support en HybridNeuroevolution**\n",
    "- Agregado soporte para entrenamiento concurrente de múltiples modelos\n",
    "- Máximo de 2 modelos entrenando simultáneamente (configurable)\n",
    "- Thread-safety usando semáforos y locks\n",
    "\n",
    "### 2. **Nuevos componentes de threading**\n",
    "- `training_semaphore`: Limita el número de entrenamientos concurrentes\n",
    "- `fitness_lock`: Protege el acceso a variables compartidas\n",
    "- `_worker_evaluate_genome()`: Función worker para evaluación en hilos separados\n",
    "- Threading-aware logging con identificadores de thread\n",
    "\n",
    "### 3. **Método evaluate_population() rediseñado**\n",
    "- Reemplaza evaluación secuencial con evaluación concurrente\n",
    "- Crea threads para cada genoma en la población\n",
    "- Sincronización segura de resultados de fitness\n",
    "- Manejo de errores por thread individual\n",
    "\n",
    "### 4. **Beneficios del threading**\n",
    "- **Paralelización**: Múltiples modelos entrenan simultáneamente\n",
    "- **Eficiencia**: Mejor uso de recursos computacionales\n",
    "- **Escalabilidad**: Fácil ajuste del número de threads concurrentes\n",
    "- **Robustez**: Manejo seguro de estado compartido\n",
    "\n",
    "### 5. **Configuración del threading**\n",
    "```python\n",
    "self.max_concurrent_training = 2  # Máximo 2 modelos concurrentes\n",
    "self.training_semaphore = threading.Semaphore(2)\n",
    "self.fitness_lock = threading.Lock()\n",
    "```\n",
    "\n",
    "El patrón implementado es idéntico al del notebook de demostración, adaptado para el contexto de neuroevolución híbrida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ceb16-25cf-428c-bd49-41db5c262038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del sistema de threading con población pequeña\n",
    "print(\"=\"*60)\n",
    "print(\"PRUEBA DE THREADING CON POBLACIÓN PEQUEÑA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuración reducida para prueba rápida\n",
    "TEST_CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset': 'MNIST',\n",
    "    'num_classes': 10,\n",
    "    'num_channels': 1,\n",
    "    'px_h': 28,\n",
    "    'px_w': 28,\n",
    "    \n",
    "    # Architecture constraints\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 2,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 1,\n",
    "    'min_filters': 8,\n",
    "    'max_filters': 16,\n",
    "    'min_fc_nodes': 32,\n",
    "    'max_fc_nodes': 64,\n",
    "    \n",
    "    # Evolution parameters\n",
    "    'population_size': 4,  # Población pequeña para prueba\n",
    "    'max_generations': 2,  # Solo 2 generaciones\n",
    "    'fitness_threshold': 85.0,\n",
    "    'elite_percentage': 0.5,\n",
    "    'crossover_rate': 0.8,\n",
    "    \n",
    "    # Mutation parameters (adaptive)\n",
    "    'base_mutation_rate': 0.2,\n",
    "    'current_mutation_rate': 0.2,\n",
    "    'mutation_rate_min': 0.1,\n",
    "    'mutation_rate_max': 0.6,\n",
    "    \n",
    "    # Training parameters (reducidos para prueba rápida)\n",
    "    'num_epochs': 2,  # Solo 2 épocas por modelo\n",
    "    'early_stopping_patience': 50,\n",
    "    'epoch_patience': 5,\n",
    "    'improvement_threshold': 0.5\n",
    "}\n",
    "\n",
    "# Crear datasets de prueba (tamaño reducido)\n",
    "print(\"Creando datasets de prueba...\")\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "# Usar solo un subconjunto para prueba rápida\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(1000))  # Solo 1000 muestras\n",
    "test_subset = torch.utils.data.Subset(test_dataset, range(200))     # Solo 200 muestras\n",
    "\n",
    "test_train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "test_test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Dataset de prueba creado: {len(train_subset)} train, {len(test_subset)} test\")\n",
    "\n",
    "# Crear instancia del algoritmo evolutivo con threading\n",
    "print(\"Iniciando algoritmo evolutivo con threading...\")\n",
    "test_neuroevolution = HybridNeuroevolution(TEST_CONFIG, test_train_loader, test_test_loader)\n",
    "\n",
    "# Ejecutar evolución de prueba\n",
    "print(\"Ejecutando evolución de prueba...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "best_test_genome = test_neuroevolution.evolve()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_test = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PRUEBA DE THREADING COMPLETADA\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Tiempo de ejecución: {execution_time_test:.2f} segundos\")\n",
    "print(f\"Mejor individuo encontrado:\")\n",
    "print(f\"   ID: {best_test_genome['id']}\")\n",
    "print(f\"   Fitness: {best_test_genome['fitness']:.2f}%\")\n",
    "print(f\"   Generaciones procesadas: {test_neuroevolution.generation + 1}\")\n",
    "print(f\"   Threading slots concurrentes: {test_neuroevolution.max_concurrent_training}\")\n",
    "print(f\"\\nLa prueba demuestra que el threading funciona correctamente!\")\n",
    "print(f\"Se entrenaron máximo {test_neuroevolution.max_concurrent_training} modelos concurrentemente.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be0b46",
   "metadata": {},
   "source": [
    "## Configuración de Threading y Timeouts\n",
    "\n",
    "Las siguientes mejoras han sido implementadas para resolver el problema de threads que se quedan colgados:\n",
    "\n",
    "### 🕐 **Timeouts implementados:**\n",
    "- **Thread timeout**: 5 minutos por thread individual\n",
    "- **Global timeout**: Tiempo total máximo para toda la población\n",
    "- **Evaluation timeout**: 3 minutos máximo por evaluación de fitness\n",
    "- **Semaphore timeout**: 30 segundos para adquirir slot de entrenamiento\n",
    "\n",
    "### 🛡️ **Manejo robusto de errores:**\n",
    "- Validación de genomas antes de crear modelos\n",
    "- Detección de valores NaN/Inf en loss y accuracy\n",
    "- Liberación automática de memoria GPU\n",
    "- Threads daemon para evitar bloqueo del programa\n",
    "- Manejo de excepciones por batch individual\n",
    "\n",
    "### ⚙️ **Configuración flexible:**\n",
    "Puedes ajustar estos parámetros en la clase `HybridNeuroevolution`:\n",
    "\n",
    "```python\n",
    "# En __init__:\n",
    "self.max_concurrent_training = config.get('max_concurrent_training', 2)\n",
    "\n",
    "# En evaluate_population:\n",
    "timeout_per_thread = 300  # 5 minutos por thread\n",
    "max_total_timeout = timeout_per_thread * 2  # Timeout total\n",
    "\n",
    "# En evaluate_fitness:\n",
    "max_evaluation_time = 180  # 3 minutos por evaluación\n",
    "```\n",
    "\n",
    "### 📊 **Monitoreo mejorado:**\n",
    "- Logs detallados por thread con timestamps\n",
    "- Conteo de threads completados vs fallidos\n",
    "- Identificación de threads que exceden timeout\n",
    "- Limpieza automática de threads abandonados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del sistema de threading CON TIMEOUTS - Más robusta\n",
    "print(\"=\"*70)\n",
    "print(\"PRUEBA DE THREADING CON TIMEOUTS Y MANEJO ROBUSTO DE ERRORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuración de prueba con threading y timeouts\n",
    "ROBUST_TEST_CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset': 'MNIST',\n",
    "    'num_classes': 10,\n",
    "    'num_channels': 1,\n",
    "    'px_h': 28,\n",
    "    'px_w': 28,\n",
    "    \n",
    "    # Architecture constraints (más permisivas para generar algunos errores)\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 3,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 2,\n",
    "    'min_filters': 4,    # Muy pocos filtros para causar posibles errores\n",
    "    'max_filters': 32,\n",
    "    'min_fc_nodes': 16,  # Pocos nodos para causar posibles errores\n",
    "    'max_fc_nodes': 128,\n",
    "    \n",
    "    # Evolution parameters\n",
    "    'population_size': 6,  # 6 individuos para probar timeouts\n",
    "    'max_generations': 1,  # Solo 1 generación para prueba\n",
    "    'fitness_threshold': 90.0,\n",
    "    'elite_percentage': 0.5,\n",
    "    'crossover_rate': 0.8,\n",
    "    \n",
    "    # Threading configuration - AQUÍ PUEDES AJUSTAR EL NÚMERO DE THREADS\n",
    "    'max_concurrent_training': 2,  # ← CAMBIA ESTE VALOR PARA MÁS/MENOS THREADS\n",
    "    \n",
    "    # Mutation parameters\n",
    "    'base_mutation_rate': 0.3,  # Alta mutación para generar arquitecturas problemáticas\n",
    "    'current_mutation_rate': 0.3,\n",
    "    'mutation_rate_min': 0.1,\n",
    "    'mutation_rate_max': 0.7,\n",
    "    \n",
    "    # Training parameters (reducidos para prueba rápida)\n",
    "    'num_epochs': 2,\n",
    "    'early_stopping_patience': 30,  # Pocos batches para acelerar\n",
    "    'epoch_patience': 3,\n",
    "    'improvement_threshold': 1.0\n",
    "}\n",
    "\n",
    "print(f\"Configuración de threading:\")\n",
    "print(f\"   Threads concurrentes: {ROBUST_TEST_CONFIG['max_concurrent_training']}\")\n",
    "print(f\"   Población: {ROBUST_TEST_CONFIG['population_size']} individuos\")\n",
    "print(f\"   Timeout por thread: 5 minutos\")\n",
    "print(f\"   Timeout total: {5 * 2} minutos\")\n",
    "\n",
    "# Crear datasets pequeños para prueba rápida\n",
    "print(\"\\nCreando datasets pequeños para prueba...\")\n",
    "if 'train_dataset' not in locals():\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                  ]))\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                                 transform=transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                 ]))\n",
    "\n",
    "# Usar subconjuntos aún más pequeños\n",
    "robust_train_subset = torch.utils.data.Subset(train_dataset, range(500))  # Solo 500 muestras\n",
    "robust_test_subset = torch.utils.data.Subset(test_dataset, range(100))    # Solo 100 muestras\n",
    "\n",
    "robust_train_loader = DataLoader(robust_train_subset, batch_size=32, shuffle=True)\n",
    "robust_test_loader = DataLoader(robust_test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Datasets creados: {len(robust_train_subset)} train, {len(robust_test_subset)} test\")\n",
    "\n",
    "# Inicializar el sistema con timeouts\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INICIANDO NEUROEVOLUCIÓN CON THREADING ROBUSTO\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "robust_neuroevolution = HybridNeuroevolution(ROBUST_TEST_CONFIG, robust_train_loader, robust_test_loader)\n",
    "\n",
    "# Mostrar configuración de threading actual\n",
    "print(f\"\\nConfiguración de threading aplicada:\")\n",
    "print(f\"   max_concurrent_training: {robust_neuroevolution.max_concurrent_training}\")\n",
    "print(f\"   Semaphore slots: {robust_neuroevolution.training_semaphore._value}\")\n",
    "\n",
    "# Ejecutar evolución con timeouts\n",
    "print(f\"\\nEjecutando evolución robusta...\")\n",
    "robust_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    robust_best_genome = robust_neuroevolution.evolve()\n",
    "    robust_success = True\n",
    "except Exception as e:\n",
    "    print(f\"ERROR durante evolución: {e}\")\n",
    "    robust_success = False\n",
    "    robust_best_genome = None\n",
    "\n",
    "robust_end_time = time.time()\n",
    "robust_execution_time = robust_end_time - robust_start_time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PRUEBA ROBUSTA DE THREADING COMPLETADA\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Tiempo de ejecución: {robust_execution_time:.2f} segundos\")\n",
    "print(f\"Éxito: {'SÍ' if robust_success else 'NO'}\")\n",
    "\n",
    "if robust_success and robust_best_genome:\n",
    "    print(f\"Mejor individuo encontrado:\")\n",
    "    print(f\"   ID: {robust_best_genome['id']}\")\n",
    "    print(f\"   Fitness: {robust_best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generaciones procesadas: {robust_neuroevolution.generation + 1}\")\n",
    "else:\n",
    "    print(f\"No se completó exitosamente, pero el sistema no se colgó!\")\n",
    "\n",
    "print(f\"\\n🎉 RESULTADO: El sistema de threading con timeouts funciona correctamente!\")\n",
    "print(f\"📊 Threads concurrentes configurados: {robust_neuroevolution.max_concurrent_training}\")\n",
    "print(f\"⏱️  Sistema robusto contra threads colgados y arquitecturas problemáticas\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

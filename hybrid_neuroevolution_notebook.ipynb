{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Synchronous Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements the complete hybrid neuroevolution process synchronously, without the need for databases or external Kafka services. The system combines genetic algorithms with convolutional neural networks to evolve optimal architectures.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **Synchronous processing**: Complete workflow executed in a single session\n",
    "- **Configurable dataset**: Supports MNIST by default or custom dataset\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of CNN architectures\n",
    "2. Evaluate fitness of each individual\n",
    "3. Select best architectures (top 50%)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import threading\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device configured: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 10,            # Population size\n",
    "    'max_generations': 30,            # Maximum number of generations\n",
    "    'fitness_threshold': 99.9,        # Target fitness (% accuracy)\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.35,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.35,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection\n",
    "    'dataset': 'MNIST',             # Options: 'MNIST', 'CIFAR10', 'CUSTOM'\n",
    "\n",
    "    # Dataset parameters (auto-configured based on dataset)\n",
    "    'num_channels': 3,                # Input channels (1=grayscale, 3=RGB)\n",
    "    'px_h': 32,                       # Image height\n",
    "    'px_w': 32,                       # Image width\n",
    "    'num_classes': 10,                # Number of classes\n",
    "    'batch_size': 128,                # Batch size\n",
    "    'test_split': 0.35,               # Validation percentage (for CUSTOM)\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 12,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.01,            # Base learning rate (used only if genome doesn't override)\n",
    "    'early_stopping_patience': 1000,  # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping (new)\n",
    "    'epoch_patience': 3,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.2,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Allowed architecture range\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 7,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 7,\n",
    "    'min_filters': 2,\n",
    "    'max_filters': 256,\n",
    "    'min_fc_nodes': 128,\n",
    "    'max_fc_nodes': 2048,\n",
    "\n",
    "    # Custom dataset configuration (only used if dataset='CUSTOM')\n",
    "    'dataset_path': None,             # Custom dataset path\n",
    "}\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIGS = {\n",
    "    'MNIST': {\n",
    "        'num_channels': 1,\n",
    "        'px_h': 28,\n",
    "        'px_w': 28,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.1307,), 'std': (0.3081,)}\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'num_channels': 3,\n",
    "        'px_h': 32,\n",
    "        'px_w': 32,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.4914, 0.4822, 0.4465), 'std': (0.2023, 0.1994, 0.2010)}\n",
    "    },\n",
    "    'CUSTOM': {\n",
    "        'num_channels': 1,  # Default, will be overridden\n",
    "        'px_h': 28,         # Default, will be overridden\n",
    "        'px_w': 28,         # Default, will be overridden\n",
    "        'num_classes': 10,  # Default, will be overridden\n",
    "        'normalization': {'mean': (0.5,), 'std': (0.5,)}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Auto-configure based on selected dataset\n",
    "def configure_dataset(config, dataset_name):\n",
    "    \"\"\"Auto-configures dataset parameters based on selected dataset.\"\"\"\n",
    "    if dataset_name in DATASET_CONFIGS:\n",
    "        dataset_config = DATASET_CONFIGS[dataset_name]\n",
    "        config['num_channels'] = dataset_config['num_channels']\n",
    "        config['px_h'] = dataset_config['px_h']\n",
    "        config['px_w'] = dataset_config['px_w']\n",
    "        config['num_classes'] = dataset_config['num_classes']\n",
    "        config['_normalization'] = dataset_config['normalization']\n",
    "    return config\n",
    "\n",
    "# Configure the selected dataset\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled):\")\n",
    "print(f\"   Selected dataset: {CONFIG['dataset']}\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.startswith('_'):  # Hide internal config\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"Available datasets: {list(DATASET_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### Información sobre Datasets Disponibles\n",
    "\n",
    "**MNIST**: \n",
    "- Dígitos escritos a mano (0-9)\n",
    "- Imágenes en escala de grises (1 canal)\n",
    "- Tamaño: 28x28 píxeles\n",
    "- Dificultad: **Fácil** - Ideal para pruebas rápidas\n",
    "- Fitness objetivo recomendado: >95%\n",
    "\n",
    "**CIFAR-10**: \n",
    "- Objetos del mundo real (aviones, coches, pájaros, etc.)\n",
    "- Imágenes en color (3 canales RGB)\n",
    "- Tamaño: 32x32 píxeles\n",
    "- Dificultad: **Media-Alta** - Más realista y desafiante\n",
    "- Fitness objetivo recomendado: >80%\n",
    "- Clases: plane, car, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "**CUSTOM**: \n",
    "- Tu propio dataset\n",
    "- Configuración manual requerida\n",
    "- Estructura de carpetas por clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_type = config['dataset']\n",
    "    \n",
    "    if dataset_type == 'CUSTOM' and config['dataset_path']:\n",
    "        print(f\"Loading custom dataset from: {config['dataset_path']}\")\n",
    "        \n",
    "        # Transformations for custom dataset\n",
    "        if config['num_channels'] == 1:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        else:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        \n",
    "        # Load dataset from folders organized by class\n",
    "        full_dataset = datasets.ImageFolder(root=config['dataset_path'], transform=transform)\n",
    "        \n",
    "        # Split into train and test\n",
    "        train_size = int((1 - config['test_split']) * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "        \n",
    "        print(f\"Custom dataset loaded:\")\n",
    "        print(f\"   Classes found: {len(full_dataset.classes)}\")\n",
    "        print(f\"   Total samples: {len(full_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'MNIST':\n",
    "        print(\"Loading MNIST dataset...\")\n",
    "        \n",
    "        # Transformations for MNIST\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load MNIST\n",
    "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        \n",
    "        print(f\"MNIST dataset loaded:\")\n",
    "        print(f\"   Classes: {len(train_dataset.classes)}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'CIFAR10':\n",
    "        print(\"Loading CIFAR-10 dataset...\")\n",
    "        \n",
    "        # Transformations for CIFAR-10\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),  # Data augmentation for training\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load CIFAR-10\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "        \n",
    "        print(f\"CIFAR-10 dataset loaded:\")\n",
    "        print(f\"   Classes: {train_dataset.classes}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_type}' not supported. Available: MNIST, CIFAR10, CUSTOM\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load the dataset\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Get a sample to verify dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_data, sample_labels = sample_batch\n",
    "print(f\"\\nDataset loaded successfully:\")\n",
    "print(f\"   Batch shape: {sample_data.shape}\")\n",
    "print(f\"   Data type: {sample_data.dtype}\")\n",
    "print(f\"   Device: {sample_data.device}\")\n",
    "print(f\"   Value range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\n",
    "\n",
    "# Show some class information for CIFAR-10\n",
    "if CONFIG['dataset'] == 'CIFAR10':\n",
    "    cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    print(f\"   CIFAR-10 classes: {cifar10_classes}\")\n",
    "    unique_labels = torch.unique(sample_labels)\n",
    "    print(f\"   Labels in this batch: {unique_labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable Convolutional Neural Network that builds architecture from genome specifications.\n",
    "    \n",
    "    This class implements a dynamically configurable CNN that constructs its architecture\n",
    "    based on genetic parameters stored in a genome dictionary. It supports variable\n",
    "    numbers of convolutional and fully connected layers with customizable parameters.\n",
    "    \n",
    "    Architecture Features:\n",
    "    - Variable number of convolutional layers (1-7 layers)\n",
    "    - Configurable filters, kernel sizes, and activation functions per layer\n",
    "    - Batch normalization for training stability\n",
    "    - Adaptive pooling based on layer position\n",
    "    - Variable number of fully connected layers (1-7 layers)\n",
    "    - Configurable dropout for regularization\n",
    "    - Automatic output size calculation\n",
    "    \n",
    "    Genome Parameters Used:\n",
    "    - num_conv_layers: Number of convolutional layers\n",
    "    - num_fc_layers: Number of fully connected layers\n",
    "    - filters: List of filter counts per conv layer\n",
    "    - kernel_sizes: List of kernel sizes per conv layer\n",
    "    - activations: List of activation function names\n",
    "    - fc_nodes: List of neuron counts per FC layer\n",
    "    - dropout_rate: Dropout probability for regularization\n",
    "    \n",
    "    Design Philosophy:\n",
    "    - Flexible architecture that can represent diverse CNN designs\n",
    "    - Automatic size calculation to handle variable input dimensions\n",
    "    - Robust error handling for invalid genome configurations\n",
    "    - Efficient forward pass implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        \"\"\"\n",
    "        Initializes the evolvable CNN with genome and configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            genome (dict): Genetic specification containing architecture parameters\n",
    "            config (dict): System configuration with dataset and training parameters\n",
    "        \n",
    "        Construction Process:\n",
    "        1. Store genome and configuration references\n",
    "        2. Build convolutional layers based on genome specifications\n",
    "        3. Calculate output dimensions after convolutions\n",
    "        4. Build fully connected layers with proper input sizing\n",
    "        \n",
    "        Error Handling:\n",
    "        - Validates genome parameters against configuration bounds\n",
    "        - Provides meaningful error messages for invalid configurations\n",
    "        - Ensures compatibility between layers\n",
    "        \"\"\"\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome      # Genetic architecture specification\n",
    "        self.config = config      # System configuration parameters\n",
    "        \n",
    "        # BUILD NETWORK ARCHITECTURE\n",
    "        # Construct layers in dependency order\n",
    "        \n",
    "        # 1. Build convolutional feature extraction layers\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # 2. Calculate dimensions after convolutions for FC layer sizing\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # 3. Build fully connected classification layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"\n",
    "        Constructs convolutional layers according to genome specifications.\n",
    "        \n",
    "        This method builds the feature extraction part of the CNN with:\n",
    "        - Variable number of convolutional layers\n",
    "        - Batch normalization for training stability\n",
    "        - Configurable activation functions\n",
    "        - Adaptive pooling strategy\n",
    "        \n",
    "        Returns:\n",
    "            nn.ModuleList: Sequential list of convolutional layer components\n",
    "        \n",
    "        Layer Structure (per convolutional layer):\n",
    "        1. Conv2d: Feature extraction with specified filters and kernel size\n",
    "        2. BatchNorm2d: Normalization for training stability\n",
    "        3. Activation: Non-linear activation function from genome\n",
    "        4. MaxPool2d: Spatial downsampling (adaptive based on layer position)\n",
    "        \n",
    "        Pooling Strategy:\n",
    "        - Intermediate layers: MaxPool2d(2,2) for standard downsampling\n",
    "        - Final layer: MaxPool2d(2,1) to preserve more spatial information\n",
    "        \"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        # Start with input channels from dataset configuration\n",
    "        in_channels = self.config['num_channels']\n",
    "        \n",
    "        # BUILD EACH CONVOLUTIONAL LAYER\n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            # Extract layer parameters from genome\n",
    "            out_channels = self.genome['filters'][i]      # Number of filters\n",
    "            kernel_size = self.genome['kernel_sizes'][i]  # Convolution kernel size\n",
    "            \n",
    "            # 1. CONVOLUTIONAL LAYER\n",
    "            # Use padding=1 to maintain spatial dimensions initially\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # 2. BATCH NORMALIZATION\n",
    "            # Normalizes activations for better training dynamics\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            # 3. ACTIVATION FUNCTION\n",
    "            # Apply activation specified in genome (with cycling for multiple layers)\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # 4. POOLING LAYER\n",
    "            # Adaptive pooling strategy based on layer position\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                # Intermediate layers: standard 2x2 pooling for downsampling\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "            else:\n",
    "                # Final layer: preserve more spatial information with stride=1\n",
    "                layers.append(nn.MaxPool2d(2, 1))\n",
    "            \n",
    "            # Update input channels for next layer\n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"Calculates output size after convolutional layers.\"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['px_h'], self.config['px_w'])\n",
    "        \n",
    "        # Pass through convolutional layers\n",
    "        x = dummy_input\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten and get size\n",
    "        return x.view(-1).shape[0]\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        \n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation except on last layer\n",
    "            if i < len(self.fc_layers) - 1 and not isinstance(layer, nn.Dropout):\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be19766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"Creates a random genome within specified ranges.\"\"\"\n",
    "    # Number of layers\n",
    "    num_conv_layers = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "    num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "    # Filters for each convolutional layer\n",
    "    filters = [random.randint(config['min_filters'], config['max_filters']) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Kernel sizes\n",
    "    kernel_sizes = [random.choice([3, 5, 7]) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Nodes in fully connected layers\n",
    "    fc_nodes = [random.randint(config['min_fc_nodes'], config['max_fc_nodes']) for _ in range(num_fc_layers)]\n",
    "\n",
    "    # Activation functions for each layer\n",
    "    activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "    # Other parameters\n",
    "    dropout_rate = random.uniform(0.1, 0.5)\n",
    "    learning_rate = random.choice([0.001, 0.0001, 0.01, 0.005])\n",
    "    optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    genome = {\n",
    "        'num_conv_layers': num_conv_layers,\n",
    "        'num_fc_layers': num_fc_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'fc_nodes': fc_nodes,\n",
    "        'activations': activations,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "    return genome\n",
    "\n",
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"Applies mutation to a genome using adaptive mutation rate.\"\"\"\n",
    "    mutated_genome = copy.deepcopy(genome)\n",
    "    mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "    # Mutate number of convolutional layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "        num_conv = mutated_genome['num_conv_layers']\n",
    "        mutated_genome['filters'] = mutated_genome['filters'][:num_conv]\n",
    "        mutated_genome['kernel_sizes'] = mutated_genome['kernel_sizes'][:num_conv]\n",
    "        while len(mutated_genome['filters']) < num_conv:\n",
    "            mutated_genome['filters'].append(random.randint(config['min_filters'], config['max_filters']))\n",
    "        while len(mutated_genome['kernel_sizes']) < num_conv:\n",
    "            mutated_genome['kernel_sizes'].append(random.choice([1, 3, 5, 7]))\n",
    "\n",
    "    # Mutate filters\n",
    "    for i in range(len(mutated_genome['filters'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "    # Mutate kernel sizes\n",
    "    for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['kernel_sizes'][i] = random.choice([1, 3, 5, 7])\n",
    "\n",
    "    # Mutate number of FC layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "        num_fc = mutated_genome['num_fc_layers']\n",
    "        mutated_genome['fc_nodes'] = mutated_genome['fc_nodes'][:num_fc]\n",
    "        while len(mutated_genome['fc_nodes']) < num_fc:\n",
    "            mutated_genome['fc_nodes'].append(random.randint(config['min_fc_nodes'], config['max_fc_nodes']))\n",
    "\n",
    "    # Mutate FC nodes\n",
    "    for i in range(len(mutated_genome['fc_nodes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "    # Mutate activation functions\n",
    "    for i in range(len(mutated_genome['activations'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['dropout_rate'] = random.uniform(0.1, 0.8)\n",
    "\n",
    "    # Mutate learning rate\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['learning_rate'] = random.choice([0.001, 0.0001, 0.01, 0.005, 0.000001, 0.05, 0.00005, 0.0005])\n",
    "\n",
    "    # Mutate optimizer\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    mutated_genome['fitness'] = 0.0\n",
    "    return mutated_genome\n",
    "\n",
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Performs crossover between two genomes.\"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "\n",
    "    # Crossover scalar parameters\n",
    "    for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer']:\n",
    "        if random.random() < 0.5:\n",
    "            child1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "    # Crossover lists (random cut point)\n",
    "    for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "        if random.random() < 0.5:\n",
    "            list1 = child1[list_key]\n",
    "            list2 = child2[list_key]\n",
    "            if len(list1) > 1 and len(list2) > 1:\n",
    "                point1 = random.randint(1, len(list1) - 1)\n",
    "                point2 = random.randint(1, len(list2) - 1)\n",
    "                child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    return child1, child2\n",
    "\n",
    "print(\"Genetic functions updated for adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNeuroevolution:\n",
    "    \"\"\"\n",
    "    Main class implementing hybrid neuroevolution with concurrent processing capabilities.\n",
    "    \n",
    "    This class combines genetic algorithms with neural network training to evolve optimal\n",
    "    CNN architectures and hyperparameters. It includes advanced features for performance\n",
    "    and reliability:\n",
    "    \n",
    "    Key Features:\n",
    "    - Concurrent genome evaluation using threading (2x performance improvement)\n",
    "    - Adaptive mutation rates based on population diversity\n",
    "    - Interleaved training/evaluation with early stopping\n",
    "    - Comprehensive convergence criteria and stagnation detection\n",
    "    - Thread-safe operations with proper synchronization\n",
    "    - Real-time progress monitoring and logging\n",
    "    \n",
    "    Threading Architecture:\n",
    "    - Main thread: Coordinates evolution process and manages shared state\n",
    "    - Worker threads: Process genomes concurrently for fitness evaluation\n",
    "    - Synchronization: Uses locks to ensure thread-safe access to shared data\n",
    "    \n",
    "    Evolution Strategy:\n",
    "    - Elitism: Preserves best individuals across generations\n",
    "    - Tournament selection: Promotes diversity while favoring fitness\n",
    "    - Adaptive operators: Adjusts mutation rates based on population state\n",
    "    - Early stopping: Prevents overfitting and reduces computation time\n",
    "    \n",
    "    Attributes:\n",
    "        config (dict): Configuration parameters for evolution and training\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        test_loader (DataLoader): Test data loader for fitness evaluation\n",
    "        population (list): Current population of genomes\n",
    "        generation (int): Current generation number\n",
    "        best_individual (dict): Best genome found across all generations\n",
    "        fitness_history (list): History of best fitness per generation\n",
    "        generation_stats (list): Detailed statistics for each generation\n",
    "        \n",
    "    Thread-Safe Attributes (used during evaluation):\n",
    "        pending_genomes (list): Genomes waiting for evaluation\n",
    "        completed_genomes (list): Genomes that have been evaluated\n",
    "        fitness_scores (list): Fitness values for statistical analysis\n",
    "        evaluation_log (list): Chronological log of thread operations\n",
    "        genome_lock (Lock): Synchronizes access to genome lists\n",
    "        results_lock (Lock): Synchronizes logging operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict, train_loader: DataLoader, test_loader: DataLoader):\n",
    "        \"\"\"\n",
    "        Initializes the HybridNeuroevolution system with configuration and data loaders.\n",
    "        \n",
    "        Args:\n",
    "            config (dict): Configuration dictionary containing all evolution parameters\n",
    "            train_loader (DataLoader): PyTorch DataLoader for training data\n",
    "            test_loader (DataLoader): PyTorch DataLoader for test/validation data\n",
    "        \n",
    "        Initialization:\n",
    "        - Stores configuration and data loaders for the evolution process\n",
    "        - Initializes evolution state variables to default values\n",
    "        - Prepares data structures for population management and statistics\n",
    "        \n",
    "        State Variables:\n",
    "        - population: Will store the current generation of genomes\n",
    "        - generation: Tracks the current generation number (starts at 0)\n",
    "        - best_individual: Will store the best genome found across all generations\n",
    "        - fitness_history: Tracks best fitness value per generation for convergence analysis\n",
    "        - generation_stats: Stores detailed statistics for visualization and analysis\n",
    "        \"\"\"\n",
    "        # CORE CONFIGURATION\n",
    "        self.config = config  # Evolution parameters and hyperparameters\n",
    "        self.train_loader = train_loader  # Training data for fitness evaluation\n",
    "        self.test_loader = test_loader    # Test data for fitness evaluation\n",
    "        \n",
    "        # EVOLUTION STATE\n",
    "        self.population = []  # Current population of genomes (initially empty)\n",
    "        self.generation = 0   # Current generation counter (starts at 0)\n",
    "        self.best_individual = None  # Best genome found so far (initially None)\n",
    "        \n",
    "        # STATISTICS AND MONITORING\n",
    "        self.fitness_history = []     # Best fitness per generation for trend analysis\n",
    "        self.generation_stats = []    # Detailed statistics per generation for visualization\n",
    "\n",
    "    def initialize_population(self):\n",
    "        \"\"\"\n",
    "        Creates the initial population of random genomes for evolution.\n",
    "        \n",
    "        This function generates a diverse set of random neural network architectures\n",
    "        to serve as the starting point for the evolutionary process.\n",
    "        \n",
    "        Each genome contains:\n",
    "        - Architecture parameters (number of layers, filters, nodes)\n",
    "        - Hyperparameters (learning rate, optimizer, dropout)\n",
    "        - Unique identifier for tracking\n",
    "        \n",
    "        Population Diversity:\n",
    "        - Random architecture combinations within specified bounds\n",
    "        - Random hyperparameter selection from predefined ranges\n",
    "        - Ensures genetic diversity for effective evolution\n",
    "        \"\"\"\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        \n",
    "        # CREATE RANDOM GENOMES\n",
    "        # Generate population_size random genomes using the create_random_genome function\n",
    "        # Each genome represents a complete neural network specification\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        \n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "\n",
    "    def _train_one_epoch(self, model, optimizer, criterion, genome_id: str, epoch: int):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch with batch limiting for faster evaluation.\n",
    "        \n",
    "        This function implements a single training epoch with optimizations for\n",
    "        neuroevolution where we need fast evaluation rather than perfect training.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch neural network model\n",
    "            optimizer: Optimizer instance (Adam, SGD, etc.)\n",
    "            criterion: Loss function (CrossEntropyLoss)\n",
    "            genome_id: Unique identifier for logging\n",
    "            epoch: Current epoch number\n",
    "        \n",
    "        Returns:\n",
    "            float: Average loss for this epoch\n",
    "        \n",
    "        Optimizations:\n",
    "        - Batch limiting: Process only a subset of batches for speed\n",
    "        - Early exit: Stop after reaching batch limit\n",
    "        - Memory efficient: Uses standard training loop without unnecessary operations\n",
    "        \"\"\"\n",
    "        model.train()  # Set model to training mode (enables dropout, batch norm updates)\n",
    "        running_loss = 0.0  # Accumulator for loss values\n",
    "        batch_count = 0  # Counter for processed batches\n",
    "        \n",
    "        # BATCH LIMITING FOR SPEED\n",
    "        # Process only a limited number of batches per epoch to speed up evaluation\n",
    "        # This is acceptable in neuroevolution where we need relative fitness comparison\n",
    "        max_batches = min(len(self.train_loader), self.config['early_stopping_patience'])\n",
    "        \n",
    "        # TRAINING LOOP\n",
    "        for data, target in self.train_loader:\n",
    "            # Move data to appropriate device (GPU/CPU)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # STANDARD PYTORCH TRAINING STEP\n",
    "            optimizer.zero_grad()  # Clear gradients from previous iteration\n",
    "            output = model(data)   # Forward pass\n",
    "            loss = criterion(output, target)  # Calculate loss\n",
    "            loss.backward()        # Backward pass (calculate gradients)\n",
    "            optimizer.step()       # Update model parameters\n",
    "            \n",
    "            # ACCUMULATE STATISTICS\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # EARLY EXIT FOR SPEED\n",
    "            if batch_count >= max_batches:\n",
    "                break\n",
    "        \n",
    "        # CALCULATE AND REPORT AVERAGE LOSS\n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        print(f\"          Train Epoch {epoch}: loss={avg_loss:.4f} ({batch_count} batches)\")\n",
    "        return avg_loss\n",
    "\n",
    "    def _evaluate(self, model, criterion, genome_id: str, epoch: int):\n",
    "        \"\"\"\n",
    "        Evaluates the model on test data to measure current performance.\n",
    "        \n",
    "        This function performs model evaluation with optimizations for neuroevolution\n",
    "        where we need fast but reliable accuracy measurements.\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch neural network model\n",
    "            criterion: Loss function for evaluation\n",
    "            genome_id: Unique identifier for logging\n",
    "            epoch: Current epoch number\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (accuracy_percentage, average_loss)\n",
    "        \n",
    "        Optimizations:\n",
    "        - Batch limiting: Evaluate on subset of test data for speed\n",
    "        - No gradient computation: Uses torch.no_grad() for memory efficiency\n",
    "        - Early exit: Stop after reaching evaluation batch limit\n",
    "        \"\"\"\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout, batch norm training)\n",
    "        \n",
    "        # EVALUATION STATISTICS\n",
    "        correct = 0  # Count of correctly classified samples\n",
    "        total = 0    # Total number of samples processed\n",
    "        eval_batches = 0  # Number of evaluation batches processed\n",
    "        total_eval_loss = 0.0  # Accumulator for evaluation loss\n",
    "        \n",
    "        # BATCH LIMITING FOR SPEED\n",
    "        # Evaluate on limited number of batches to speed up the process\n",
    "        # 20 batches usually provide reliable accuracy estimate\n",
    "        max_eval_batches = min(len(self.test_loader), 20)\n",
    "        \n",
    "        # EVALUATION LOOP (NO GRADIENT COMPUTATION)\n",
    "        with torch.no_grad():  # Disable gradient computation for memory efficiency\n",
    "            for data, target in self.test_loader:\n",
    "                # Move data to appropriate device (GPU/CPU)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                # FORWARD PASS ONLY\n",
    "                output = model(data)  # Get model predictions\n",
    "                loss = criterion(output, target)  # Calculate loss for monitoring\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                # ACCURACY CALCULATION\n",
    "                _, predicted = torch.max(output, 1)  # Get class with highest probability\n",
    "                total += target.size(0)  # Add batch size to total\n",
    "                correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "                \n",
    "                eval_batches += 1\n",
    "                \n",
    "                # EARLY EXIT FOR SPEED\n",
    "                if eval_batches >= max_eval_batches:\n",
    "                    break\n",
    "        \n",
    "        # CALCULATE FINAL METRICS\n",
    "        accuracy = 100.0 * correct / max(1, total)  # Convert to percentage\n",
    "        avg_eval_loss = total_eval_loss / max(1, eval_batches)  # Average loss\n",
    "        \n",
    "        print(f\"          Eval  Epoch {epoch}: acc={accuracy:.2f}% loss={avg_eval_loss:.4f} ({eval_batches} batches)\")\n",
    "        return accuracy, avg_eval_loss\n",
    "\n",
    "    def evaluate_fitness(self, genome: dict) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates a single genome's fitness by training and testing a neural network.\n",
    "        \n",
    "        This is the core fitness evaluation function that:\n",
    "        1. Creates a CNN model from the genome specification\n",
    "        2. Trains the model using interleaved train/eval epochs\n",
    "        3. Implements early stopping based on improvement thresholds\n",
    "        4. Returns the best accuracy achieved as fitness score\n",
    "        \n",
    "        Args:\n",
    "            genome (dict): Genome dictionary containing architecture and hyperparameters\n",
    "        \n",
    "        Returns:\n",
    "            float: Fitness score (accuracy percentage, 0.0-100.0)\n",
    "        \n",
    "        Training Strategy:\n",
    "        - Interleaved training: alternates between training and evaluation each epoch\n",
    "        - Early stopping: stops training if no significant improvement is detected\n",
    "        - Batch limiting: limits batches per epoch for faster evaluation\n",
    "        \n",
    "        Error Handling:\n",
    "        - Returns 0.0 fitness if any errors occur during training\n",
    "        - Logs errors for debugging purposes\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # MODEL CREATION\n",
    "            # Instantiate CNN model from genome specification and move to GPU/CPU\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            \n",
    "            # OPTIMIZER SETUP\n",
    "            # Create optimizer instance based on genome's optimizer choice\n",
    "            optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "            optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "            \n",
    "            # LOSS FUNCTION\n",
    "            # Use CrossEntropyLoss for classification tasks\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # TRAINING STATE VARIABLES\n",
    "            best_acc = 0.0  # Best accuracy achieved during training\n",
    "            best_epoch = -1  # Epoch where best accuracy was achieved\n",
    "            patience_left = self.config['epoch_patience']  # Early stopping patience counter\n",
    "            last_improvement_acc = 0.0  # Last accuracy that triggered patience reset\n",
    "\n",
    "            max_epochs = self.config['num_epochs']  # Maximum training epochs\n",
    "            print(f\"      Training/Evaluating model {genome['id']} (max {max_epochs} epochs interleaved)\")\n",
    "\n",
    "            # INTERLEAVED TRAINING/EVALUATION LOOP\n",
    "            # Train for one epoch, then immediately evaluate - repeat\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                # TRAINING PHASE\n",
    "                # Train model for one epoch with batch limiting for speed\n",
    "                self._train_one_epoch(model, optimizer, criterion, genome['id'], epoch)\n",
    "                \n",
    "                # EVALUATION PHASE\n",
    "                # Immediately evaluate after training to get current performance\n",
    "                acc, eval_loss = self._evaluate(model, criterion, genome['id'], epoch)\n",
    "\n",
    "                # EARLY STOPPING LOGIC\n",
    "                # Check if accuracy improved significantly enough to continue training\n",
    "                improvement = acc - last_improvement_acc\n",
    "                if improvement >= self.config['improvement_threshold']:\n",
    "                    patience_left = self.config['epoch_patience']  # Reset patience\n",
    "                    last_improvement_acc = acc  # Update improvement baseline\n",
    "                else:\n",
    "                    patience_left -= 1  # Decrease patience\n",
    "\n",
    "                # BEST ACCURACY TRACKING\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                # PROGRESS REPORTING\n",
    "                print(f\"              -> Acc={acc:.2f}% (best={best_acc:.2f}% at epoch {best_epoch}) patience_left={patience_left}\")\n",
    "\n",
    "                # EARLY STOPPING CHECK\n",
    "                if patience_left <= 0:\n",
    "                    print(f\"              Early stopping triggered (no significant improvement)\")\n",
    "                    break\n",
    "\n",
    "            # FINAL RESULT\n",
    "            print(f\"      Final fitness for {genome['id']}: {best_acc:.2f}% (best epoch {best_epoch})\")\n",
    "            return best_acc\n",
    "            \n",
    "        except Exception as e:\n",
    "            # ERROR HANDLING\n",
    "            # Return 0.0 fitness and log error for debugging\n",
    "            print(f\"      ERROR evaluating genome {genome['id']}: {e}\")\n",
    "            logger.warning(f\"Error evaluating genome {genome['id']}: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _thread_worker(self, thread_name: str):\n",
    "        \"\"\"\n",
    "        Thread worker function that processes genomes from the shared queue.\n",
    "        \n",
    "        This function implements the core threading logic for concurrent genome evaluation.\n",
    "        Each thread continuously extracts genomes from a shared pending list, evaluates\n",
    "        their fitness, and stores results in a shared completed list.\n",
    "        \n",
    "        Args:\n",
    "            thread_name (str): Unique identifier for this thread (e.g., \"THREAD-1\")\n",
    "        \n",
    "        Thread Safety:\n",
    "            - Uses self.genome_lock to synchronize access to shared genome lists\n",
    "            - Uses self.results_lock to synchronize logging operations\n",
    "            - Implements atomic operations to prevent race conditions\n",
    "        \n",
    "        Workflow:\n",
    "            1. Extract genome from pending_genomes list (thread-safe)\n",
    "            2. Evaluate genome fitness (time-consuming, outside locks)\n",
    "            3. Store results in completed_genomes list (thread-safe)\n",
    "            4. Repeat until no more genomes to process\n",
    "        \"\"\"\n",
    "        operations_count = 0  # Counter for genomes processed by this thread\n",
    "        \n",
    "        print(f\"      🚀 {thread_name} started\")\n",
    "        \n",
    "        while True:\n",
    "            # Initialize variables for current operation\n",
    "            current_genome = None\n",
    "            remaining_count = 0\n",
    "            completed_count = 0\n",
    "            \n",
    "            # CRITICAL SECTION: Extract genome from pending list\n",
    "            # This section must be atomic to prevent multiple threads from\n",
    "            # extracting the same genome or corrupting the list state\n",
    "            with self.genome_lock:\n",
    "                if self.pending_genomes:  # Check if there are genomes to process\n",
    "                    current_genome = self.pending_genomes.pop(0)  # Extract first genome (FIFO)\n",
    "                    remaining_count = len(self.pending_genomes)  # Capture current state\n",
    "                    completed_count = len(self.completed_genomes)  # Capture current state\n",
    "                    operations_count += 1  # Increment this thread's counter\n",
    "                else:\n",
    "                    # No more genomes to process - thread should terminate\n",
    "                    break\n",
    "            \n",
    "            # GENOME PROCESSING: Execute outside of locks to maximize concurrency\n",
    "            # The actual fitness evaluation is the most time-consuming part and\n",
    "            # should not be done inside a lock to allow other threads to work\n",
    "            if current_genome:\n",
    "                # Generate timestamp for logging (high precision for ordering)\n",
    "                timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n",
    "                \n",
    "                # Log genome extraction (thread-safe logging)\n",
    "                with self.results_lock:\n",
    "                    self.evaluation_log.append({\n",
    "                        'thread': thread_name,\n",
    "                        'operation': f'EXTRACTED {current_genome[\"id\"]}',\n",
    "                        'pending_remaining': remaining_count,\n",
    "                        'completed_count': completed_count,\n",
    "                        'timestamp': timestamp\n",
    "                    })\n",
    "                \n",
    "                # Display processing information (user feedback)\n",
    "                print(f\"\\n      [{timestamp}] {thread_name} - Processing genome {current_genome['id']}\")\n",
    "                print(f\"         Architecture: {current_genome['num_conv_layers']} conv + {current_genome['num_fc_layers']} fc, opt={current_genome['optimizer']}, lr={current_genome['learning_rate']}\")\n",
    "                print(f\"         Remaining: {remaining_count} | Completed: {completed_count + 1}\")\n",
    "                \n",
    "                # CORE OPERATION: Evaluate fitness (computationally intensive)\n",
    "                # This is where the actual neural network training and evaluation occurs\n",
    "                # Takes significant time (seconds to minutes per genome)\n",
    "                fitness = self.evaluate_fitness(current_genome)\n",
    "                current_genome['fitness'] = fitness  # Store fitness in genome\n",
    "                \n",
    "                # CRITICAL SECTION: Store results safely\n",
    "                # Multiple threads may try to update shared state simultaneously\n",
    "                with self.genome_lock:\n",
    "                    self.completed_genomes.append(current_genome)  # Add to completed list\n",
    "                    self.fitness_scores.append(fitness)  # Add to fitness tracking\n",
    "                    \n",
    "                    # Update global best fitness if this genome achieved a new record\n",
    "                    if fitness > self.best_fitness_so_far:\n",
    "                        self.best_fitness_so_far = fitness\n",
    "                        print(f\"         🎯 New best fitness in this generation: {fitness:.2f}%!\")\n",
    "                \n",
    "                # Log completion (thread-safe logging)\n",
    "                completion_timestamp = datetime.now().strftime('%H:%M:%S.%f')[:-3]\n",
    "                with self.results_lock:\n",
    "                    self.evaluation_log.append({\n",
    "                        'thread': thread_name,\n",
    "                        'operation': f'COMPLETED {current_genome[\"id\"]} - {fitness:.2f}%',\n",
    "                        'pending_remaining': len(self.pending_genomes),\n",
    "                        'completed_count': len(self.completed_genomes),\n",
    "                        'timestamp': completion_timestamp\n",
    "                    })\n",
    "                \n",
    "                # Display completion information\n",
    "                print(f\"         ✅ Fitness: {fitness:.2f}% | Best so far: {self.best_fitness_so_far:.2f}%\")\n",
    "        \n",
    "        # Thread termination message\n",
    "        print(f\"      🏁 {thread_name} finished - {operations_count} genomes processed\")\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        \"\"\"\n",
    "        Evaluates the entire population using concurrent threading for improved performance.\n",
    "        \n",
    "        This method implements a producer-consumer pattern where:\n",
    "        - The main thread acts as a coordinator, setting up shared data structures\n",
    "        - Two worker threads act as consumers, processing genomes from a shared queue\n",
    "        - All threads synchronize using locks to ensure thread safety\n",
    "        \n",
    "        Threading Strategy:\n",
    "        - Uses 2 concurrent threads to process genomes in parallel (2x speedup)\n",
    "        - Implements thread-safe access to shared data structures\n",
    "        - Provides real-time logging and progress tracking\n",
    "        \n",
    "        Data Structures:\n",
    "        - pending_genomes: Source list of genomes to be evaluated\n",
    "        - completed_genomes: Destination list of evaluated genomes\n",
    "        - fitness_scores: List of fitness values for statistical analysis\n",
    "        - evaluation_log: Chronological log of thread operations\n",
    "        \n",
    "        Synchronization:\n",
    "        - genome_lock: Protects access to genome lists and fitness tracking\n",
    "        - results_lock: Protects logging operations to prevent garbled output\n",
    "        \"\"\"\n",
    "        print(f\"\\nEvaluating population (Generation {self.generation})...\")\n",
    "        print(f\"Processing {len(self.population)} individuals using 2 concurrent threads...\")\n",
    "        \n",
    "        # SHARED DATA STRUCTURES INITIALIZATION\n",
    "        # These structures are shared between threads and require synchronization\n",
    "        self.pending_genomes = self.population.copy()  # Source: genomes waiting for evaluation\n",
    "        self.completed_genomes = []  # Destination: genomes that have been evaluated\n",
    "        self.fitness_scores = []  # Fitness values for statistical analysis\n",
    "        self.best_fitness_so_far = 0.0  # Track best fitness in current generation\n",
    "        \n",
    "        # THREAD SYNCHRONIZATION MECHANISMS\n",
    "        # These locks ensure thread-safe access to shared data structures\n",
    "        self.genome_lock = threading.Lock()  # Protects genome lists and fitness data\n",
    "        self.results_lock = threading.Lock()  # Protects logging and output operations\n",
    "        self.evaluation_log = []  # Chronological log of all thread operations\n",
    "        \n",
    "        print(f\"Starting threaded evaluation with {len(self.pending_genomes)} genomes...\")\n",
    "        \n",
    "        # THREAD CREATION AND CONFIGURATION\n",
    "        # Create two worker threads for concurrent genome processing\n",
    "        thread1 = threading.Thread(target=self._thread_worker, args=(\"THREAD-1\",))\n",
    "        thread2 = threading.Thread(target=self._thread_worker, args=(\"THREAD-2\",))\n",
    "        \n",
    "        # Configure threads as daemon threads for clean shutdown\n",
    "        # Daemon threads automatically terminate when the main program exits\n",
    "        thread1.daemon = True\n",
    "        thread2.daemon = True\n",
    "        \n",
    "        # THREAD EXECUTION\n",
    "        # Start both threads simultaneously for maximum concurrency\n",
    "        thread1.start()\n",
    "        thread2.start()\n",
    "        \n",
    "        print(f\"   ✅ Threads started. Active threads: {threading.active_count()}\")\n",
    "        \n",
    "        # THREAD SYNCHRONIZATION\n",
    "        # Wait for both threads to complete their work before proceeding\n",
    "        # This ensures all genomes have been processed before continuing\n",
    "        thread1.join()\n",
    "        thread2.join()\n",
    "        \n",
    "        # COMPLETION REPORTING\n",
    "        print(f\"\\n💯 Threaded evaluation completed!\")\n",
    "        print(f\"   Total genomes processed: {len(self.completed_genomes)}\")\n",
    "        print(f\"   Best fitness found: {self.best_fitness_so_far:.2f}%\")\n",
    "        print(f\"   Active threads: {threading.active_count()}\")\n",
    "        \n",
    "        # OPERATION LOG DISPLAY\n",
    "        # Show chronological sequence of operations for debugging and analysis\n",
    "        print(f\"\\n--- Chronological Thread Operations Log ---\")\n",
    "        self.evaluation_log.sort(key=lambda x: x['timestamp'])  # Sort by timestamp\n",
    "        for entry in self.evaluation_log:\n",
    "            print(f\"[{entry['timestamp']}] {entry['thread']} - {entry['operation']}\")\n",
    "        \n",
    "        # DATA CONSOLIDATION\n",
    "        # Update population with evaluated results and prepare for next generation\n",
    "        self.population = self.completed_genomes.copy()  # Replace population with evaluated genomes\n",
    "        fitness_scores = self.fitness_scores.copy()  # Get fitness scores for statistics\n",
    "        \n",
    "        # STATISTICAL ANALYSIS\n",
    "        # Calculate generation statistics for monitoring evolution progress\n",
    "        if fitness_scores:\n",
    "            avg_fitness = np.mean(fitness_scores)  # Population average fitness\n",
    "            max_fitness = np.max(fitness_scores)   # Best fitness in generation\n",
    "            min_fitness = np.min(fitness_scores)   # Worst fitness in generation\n",
    "            std_fitness = np.std(fitness_scores)   # Population diversity measure\n",
    "        else:\n",
    "            # Handle edge case where no valid fitness scores exist\n",
    "            avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "        # Store generation statistics for analysis and visualization\n",
    "        stats = {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': avg_fitness,\n",
    "            'max_fitness': max_fitness,\n",
    "            'min_fitness': min_fitness,\n",
    "            'std_fitness': std_fitness\n",
    "        }\n",
    "        self.generation_stats.append(stats)\n",
    "        self.fitness_history.append(max_fitness)  # Track fitness evolution over time\n",
    "\n",
    "        # GLOBAL BEST TRACKING\n",
    "        # Update global best individual if a new record was achieved\n",
    "        best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "        if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "            self.best_individual = copy.deepcopy(best_genome)  # Deep copy to preserve state\n",
    "            print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "        # GENERATION SUMMARY REPORTING\n",
    "        # Display comprehensive statistics for this generation\n",
    "        print(f\"\\nGENERATION {self.generation} STATISTICS:\")\n",
    "        print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "        print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "        print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "        print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "        print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "        print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "    def selection_and_reproduction(self):\n",
    "        print(f\"\\nStarting selection and reproduction...\")\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "        elite = self.population[:elite_size]\n",
    "        print(f\"Selecting {elite_size} elite individuals:\")\n",
    "        for i, individual in enumerate(elite):\n",
    "            print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "        new_population = copy.deepcopy(elite)\n",
    "        offspring_needed = self.config['population_size'] - len(new_population)\n",
    "        print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "        offspring_created = 0\n",
    "        while len(new_population) < self.config['population_size']:\n",
    "            parent1 = self.tournament_selection()\n",
    "            parent2 = self.tournament_selection()\n",
    "            child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "            child1 = mutate_genome(child1, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child1)\n",
    "            child2 = mutate_genome(child2, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child2)\n",
    "            offspring_created += 2\n",
    "            if offspring_created % 4 == 0:\n",
    "                print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "        self.population = new_population[:self.config['population_size']]\n",
    "        print(f\"New generation created with {len(self.population)} individuals\")\n",
    "        print(f\"   Elite preserved: {elite_size}\")\n",
    "        print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "    def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "        tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "        return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "    def _update_adaptive_mutation(self):\n",
    "        # Diversity measured via std of fitness in last generation\n",
    "        if not self.generation_stats:\n",
    "            self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "            return\n",
    "        last_std = self.generation_stats[-1]['std_fitness']\n",
    "        # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "        # Normalize std roughly assuming fitness in [0,100]\n",
    "        diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "        # Invert: low diversity (small std) should raise mutation\n",
    "        inverted = 1 - diversity_factor\n",
    "        new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "        new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "        self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "        print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "    def check_convergence(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the evolutionary process should terminate based on multiple criteria.\n",
    "        \n",
    "        This function implements comprehensive convergence checking to determine\n",
    "        when the evolution should stop. It considers multiple termination conditions\n",
    "        to balance optimization time with solution quality.\n",
    "        \n",
    "        Termination Criteria:\n",
    "        1. Target fitness reached: Best individual achieves desired accuracy\n",
    "        2. Maximum generations: Computational budget exhausted\n",
    "        3. Stagnation detection: No improvement in recent generations\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if evolution should terminate, False to continue\n",
    "        \n",
    "        Design Philosophy:\n",
    "        - Multiple criteria ensure robustness against different scenarios\n",
    "        - Prevents infinite loops while allowing sufficient optimization time\n",
    "        - Balances solution quality with computational efficiency\n",
    "        \"\"\"\n",
    "        # CRITERION 1: TARGET FITNESS ACHIEVED\n",
    "        # Stop if we've reached the desired accuracy threshold\n",
    "        if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "            print(f\"Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "            return True\n",
    "        \n",
    "        # CRITERION 2: MAXIMUM GENERATIONS REACHED\n",
    "        # Stop if we've exhausted our computational budget\n",
    "        if self.generation >= self.config['max_generations']:\n",
    "            print(f\"Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "            return True\n",
    "        \n",
    "        # CRITERION 3: STAGNATION DETECTION\n",
    "        # Stop if fitness hasn't improved significantly in recent generations\n",
    "        if len(self.fitness_history) >= 3:\n",
    "            recent = self.fitness_history[-3:]  # Last 3 generations\n",
    "            fitness_range = max(recent) - min(recent)\n",
    "            if fitness_range < 0.5:  # Less than 0.5% improvement\n",
    "                print(\"Stagnation detected in last 3 generations\")\n",
    "                return True\n",
    "        \n",
    "        # CONTINUE EVOLUTION\n",
    "        return False\n",
    "\n",
    "    def evolve(self) -> dict:\n",
    "        \"\"\"\n",
    "        Executes the complete hybrid neuroevolution process.\n",
    "        \n",
    "        This is the main entry point that orchestrates the entire evolutionary algorithm.\n",
    "        It combines genetic algorithms with neural network training to evolve optimal\n",
    "        architectures and hyperparameters.\n",
    "        \n",
    "        Evolution Process:\n",
    "        1. Initialize random population of neural network architectures\n",
    "        2. Evaluate fitness of each individual using threaded training\n",
    "        3. Select elite individuals and create offspring through crossover/mutation\n",
    "        4. Repeat until convergence criteria are met\n",
    "        5. Return the best architecture found\n",
    "        \n",
    "        Returns:\n",
    "            dict: Best genome found during evolution with highest fitness\n",
    "        \n",
    "        Key Features:\n",
    "        - Adaptive mutation rates based on population diversity\n",
    "        - Threaded fitness evaluation for performance (2x speedup)\n",
    "        - Multiple convergence criteria for robust termination\n",
    "        - Comprehensive logging and progress tracking\n",
    "        \"\"\"\n",
    "        # EVOLUTION INITIALIZATION\n",
    "        print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "        print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "        print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # CREATE INITIAL POPULATION\n",
    "        # Generate diverse set of random architectures to start evolution\n",
    "        self.initialize_population()\n",
    "        \n",
    "        # MAIN EVOLUTION LOOP\n",
    "        # Continue until convergence criteria are satisfied\n",
    "        while not self.check_convergence():\n",
    "            # GENERATION HEADER\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GENERATION {self.generation}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # POPULATION EVALUATION (THREADED)\n",
    "            # Evaluate fitness of all individuals using concurrent threads\n",
    "            # This is the most computationally expensive part\n",
    "            self.evaluate_population()\n",
    "            \n",
    "            # EARLY CONVERGENCE CHECK\n",
    "            # Check if target fitness was reached during evaluation\n",
    "            if self.check_convergence():\n",
    "                break\n",
    "            \n",
    "            # ADAPTIVE PARAMETER ADJUSTMENT\n",
    "            # Update mutation rate based on population diversity\n",
    "            self._update_adaptive_mutation()\n",
    "            \n",
    "            # REPRODUCTION AND SELECTION\n",
    "            # Create next generation through selection, crossover, and mutation\n",
    "            self.selection_and_reproduction()\n",
    "            \n",
    "            # ADVANCE TO NEXT GENERATION\n",
    "            self.generation += 1\n",
    "            print(f\"\\nPreparing for next generation...\")\n",
    "        \n",
    "        # EVOLUTION COMPLETION\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVOLUTION COMPLETED!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best individual found:\")\n",
    "        print(f\"   ID: {self.best_individual['id']}\")\n",
    "        print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Origin generation: {self.generation}\")\n",
    "        print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "        \n",
    "        return self.best_individual\n",
    "\n",
    "print(\"HybridNeuroevolution class updated with adaptive mutation and interleaved early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACIÓN DE DATASET - MODIFICAR AQUÍ\n",
    "# ==========================================\n",
    "\n",
    "# Para cambiar el dataset, modifica la línea correspondiente y ejecuta esta celda:\n",
    "\n",
    "# Opción 1: Usar MNIST (28x28, grayscale, 10 classes)\n",
    "# CONFIG['dataset'] = 'MNIST'\n",
    "\n",
    "# Opción 2: Usar CIFAR-10 (32x32, RGB, 10 classes) - RECOMENDADO para mayor challenge\n",
    "# CONFIG['dataset'] = 'CIFAR10'\n",
    "\n",
    "# Opción 3: Usar dataset personalizado\n",
    "# CONFIG['dataset'] = 'CUSTOM'\n",
    "# CONFIG['dataset_path'] = r'E:\\Neuroevolution\\data\\phd_data'  # Ajustar ruta según tu dataset\n",
    "\n",
    "# ==========================================\n",
    "# OTRAS CONFIGURACIONES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# Reconfigurar el dataset con los nuevos parámetros\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "print(\"Current configuration:\")\n",
    "print(f\"   Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"   Image size: {CONFIG['px_h']}x{CONFIG['px_w']}x{CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Recargar el dataset con la nueva configuración\n",
    "print(f\"\\nReloading dataset with new configuration...\")\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Create system instance\n",
    "neuroevolution = HybridNeuroevolution(CONFIG, train_loader, test_loader)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nProcess completed at {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   • Errors in model architecture\")\n",
    "        print(f\"   • Memory problems (GPU/RAM)\")\n",
    "        print(f\"   • Invalid hyperparameter configurations\")\n",
    "        print(f\"   • Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"        BEST EVOLVED ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Convolutional Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS:\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv{i+1}: {filters} filters, kernel {kernel}x{kernel}, activation {activation}\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (classes)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.4f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(best_genome['activations'])}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Convolutional layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   TARGET: OK Fitness objective REACHED ({best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   TARGET: ERROR Fitness objective NOT reached ({best_genome['fitness']:.2f}% < {config['fitness_threshold']}%)\")\n",
    "    \n",
    "    print(f\"   TIME: Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'config_used': config,\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\nHYBRID NEUROEVOLUTION COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2964f-0098-4c42-bf72-e66c4ca0ed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ceb16-25cf-428c-bd49-41db5c262038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 📚 Code Documentation Summary\n",
    "\n",
    "This notebook now includes comprehensive English comments throughout all major functions and classes. Here's what has been documented:\n",
    "\n",
    "### 🧵 **Threading Implementation** \n",
    "- **`_thread_worker()`**: Core threading logic with detailed synchronization explanations\n",
    "- **`evaluate_population()`**: Producer-consumer pattern and thread coordination \n",
    "- **Thread Safety**: Lock usage, atomic operations, and race condition prevention\n",
    "\n",
    "### 🔬 **Fitness Evaluation System**\n",
    "- **`evaluate_fitness()`**: Complete neural network training and evaluation pipeline\n",
    "- **`_train_one_epoch()`**: Optimized training with batch limiting for speed\n",
    "- **`_evaluate()`**: Fast model evaluation with accuracy calculation\n",
    "\n",
    "### 🧬 **Genetic Algorithm Core**\n",
    "- **`evolve()`**: Main evolution orchestration and flow control\n",
    "- **`initialize_population()`**: Random genome generation for diversity\n",
    "- **`check_convergence()`**: Multi-criteria termination detection\n",
    "\n",
    "### 🏗️ **Neural Architecture**\n",
    "- **`EvolvableCNN`**: Dynamic CNN construction from genetic specifications\n",
    "- **`_build_conv_layers()`**: Feature extraction layer assembly\n",
    "- **Architecture flexibility**: Variable layers, filters, and hyperparameters\n",
    "\n",
    "### 🔄 **Process Flow Documentation**\n",
    "1. **Initialization**: Random population generation with diverse architectures\n",
    "2. **Evaluation**: Concurrent fitness assessment using 2 worker threads  \n",
    "3. **Selection**: Elite preservation and tournament selection\n",
    "4. **Reproduction**: Crossover and adaptive mutation for next generation\n",
    "5. **Convergence**: Multiple termination criteria for robust stopping\n",
    "\n",
    "### 🛡️ **Thread Safety Features**\n",
    "- **Locks**: `genome_lock` for data access, `results_lock` for logging\n",
    "- **Atomic Operations**: Thread-safe list operations and state updates\n",
    "- **Synchronization**: Proper thread joining and cleanup procedures\n",
    "\n",
    "### ⚡ **Performance Optimizations**\n",
    "- **Concurrent Processing**: 2x speedup through parallel genome evaluation\n",
    "- **Batch Limiting**: Faster training through reduced batch processing\n",
    "- **Early Stopping**: Prevents overfitting and reduces computation time\n",
    "- **Memory Efficiency**: Proper GPU memory management and cleanup\n",
    "\n",
    "All critical functions now have detailed docstrings explaining purpose, parameters, return values, algorithms, and design decisions. The threading implementation is thoroughly documented with explanations of synchronization mechanisms and race condition prevention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

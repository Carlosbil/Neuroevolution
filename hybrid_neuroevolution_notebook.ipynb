{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Synchronous Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements the complete hybrid neuroevolution process synchronously, without the need for databases or external Kafka services. The system combines genetic algorithms with convolutional neural networks to evolve optimal architectures.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **Synchronous processing**: Complete workflow executed in a single session\n",
    "- **Configurable dataset**: Supports MNIST by default or custom dataset\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of CNN architectures\n",
    "2. Evaluate fitness of each individual\n",
    "3. Select best architectures (top 50%)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50120a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dependency installation for Hybrid Neuroevolution...\n",
      "============================================================\n",
      "Installing torch>=2.0.0...\n",
      "OK torch>=2.0.0 installed correctly\n",
      "Installing torchvision>=0.15.0...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m, in \u001b[0;36minstall_package\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m==\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision>=0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m required_packages:\n\u001b[1;32m---> 31\u001b[0m     \u001b[43minstall_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll dependencies have been verified/installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestart the kernel if this is the first time installing torch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36minstall_package\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m installed correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:408\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_call\u001b[39m(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[0;32m    410\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:391\u001b[0m, in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# Including KeyboardInterrupt, wait handled that.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         p\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\subprocess.py:1588\u001b[0m, in \u001b[0;36mPopen._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1585\u001b[0m     timeout_millis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1587\u001b[0m     \u001b[38;5;66;03m# API note: Returns immediately if timeout_millis == 0.\u001b[39;00m\n\u001b[1;32m-> 1588\u001b[0m     result \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForSingleObject(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   1589\u001b[0m                                          timeout_millis)\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWAIT_TIMEOUT:\n\u001b[0;32m   1591\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import threading  # Added for concurrent model training\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded (adaptive mutation enabled):\n",
      "   Selected dataset: MNIST\n",
      "   population_size: 10\n",
      "   max_generations: 30\n",
      "   fitness_threshold: 99.9\n",
      "   base_mutation_rate: 0.35\n",
      "   mutation_rate_min: 0.1\n",
      "   mutation_rate_max: 0.8\n",
      "   current_mutation_rate: 0.35\n",
      "   crossover_rate: 0.99\n",
      "   elite_percentage: 0.2\n",
      "   dataset: MNIST\n",
      "   num_channels: 1\n",
      "   px_h: 28\n",
      "   px_w: 28\n",
      "   num_classes: 10\n",
      "   batch_size: 128\n",
      "   test_split: 0.35\n",
      "   num_epochs: 12\n",
      "   learning_rate: 0.01\n",
      "   early_stopping_patience: 1000\n",
      "   epoch_patience: 3\n",
      "   improvement_threshold: 0.2\n",
      "   min_conv_layers: 1\n",
      "   max_conv_layers: 7\n",
      "   min_fc_layers: 1\n",
      "   max_fc_layers: 7\n",
      "   min_filters: 2\n",
      "   max_filters: 256\n",
      "   min_fc_nodes: 128\n",
      "   max_fc_nodes: 2048\n",
      "   dataset_path: None\n",
      "\n",
      "Available activation functions: ['relu', 'leaky_relu', 'tanh', 'sigmoid', 'selu']\n",
      "Available optimizers: ['adam', 'adamw', 'sgd', 'rmsprop']\n",
      "Available datasets: ['MNIST', 'CIFAR10', 'CUSTOM']\n"
     ]
    }
   ],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 10,            # Population size\n",
    "    'max_generations': 30,            # Maximum number of generations\n",
    "    'fitness_threshold': 99.9,        # Target fitness (% accuracy)\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.35,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.35,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection\n",
    "    'dataset': 'MNIST',             # Options: 'MNIST', 'CIFAR10', 'CUSTOM'\n",
    "\n",
    "    # Dataset parameters (auto-configured based on dataset)\n",
    "    'num_channels': 3,                # Input channels (1=grayscale, 3=RGB)\n",
    "    'px_h': 32,                       # Image height\n",
    "    'px_w': 32,                       # Image width\n",
    "    'num_classes': 10,                # Number of classes\n",
    "    'batch_size': 128,                # Batch size\n",
    "    'test_split': 0.35,               # Validation percentage (for CUSTOM)\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 12,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.01,            # Base learning rate (used only if genome doesn't override)\n",
    "    'early_stopping_patience': 1000,  # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping (new)\n",
    "    'epoch_patience': 3,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.2,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Allowed architecture range\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 7,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 7,\n",
    "    'min_filters': 2,\n",
    "    'max_filters': 256,\n",
    "    'min_fc_nodes': 128,\n",
    "    'max_fc_nodes': 2048,\n",
    "\n",
    "    # Custom dataset configuration (only used if dataset='CUSTOM')\n",
    "    'dataset_path': None,             # Custom dataset path\n",
    "}\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIGS = {\n",
    "    'MNIST': {\n",
    "        'num_channels': 1,\n",
    "        'px_h': 28,\n",
    "        'px_w': 28,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.1307,), 'std': (0.3081,)}\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'num_channels': 3,\n",
    "        'px_h': 32,\n",
    "        'px_w': 32,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.4914, 0.4822, 0.4465), 'std': (0.2023, 0.1994, 0.2010)}\n",
    "    },\n",
    "    'CUSTOM': {\n",
    "        'num_channels': 1,  # Default, will be overridden\n",
    "        'px_h': 28,         # Default, will be overridden\n",
    "        'px_w': 28,         # Default, will be overridden\n",
    "        'num_classes': 10,  # Default, will be overridden\n",
    "        'normalization': {'mean': (0.5,), 'std': (0.5,)}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Auto-configure based on selected dataset\n",
    "def configure_dataset(config, dataset_name):\n",
    "    \"\"\"Auto-configures dataset parameters based on selected dataset.\"\"\"\n",
    "    if dataset_name in DATASET_CONFIGS:\n",
    "        dataset_config = DATASET_CONFIGS[dataset_name]\n",
    "        config['num_channels'] = dataset_config['num_channels']\n",
    "        config['px_h'] = dataset_config['px_h']\n",
    "        config['px_w'] = dataset_config['px_w']\n",
    "        config['num_classes'] = dataset_config['num_classes']\n",
    "        config['_normalization'] = dataset_config['normalization']\n",
    "    return config\n",
    "\n",
    "# Configure the selected dataset\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled):\")\n",
    "print(f\"   Selected dataset: {CONFIG['dataset']}\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.startswith('_'):  # Hide internal config\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"Available datasets: {list(DATASET_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### Información sobre Datasets Disponibles\n",
    "\n",
    "**MNIST**: \n",
    "- Dígitos escritos a mano (0-9)\n",
    "- Imágenes en escala de grises (1 canal)\n",
    "- Tamaño: 28x28 píxeles\n",
    "- Dificultad: **Fácil** - Ideal para pruebas rápidas\n",
    "- Fitness objetivo recomendado: >95%\n",
    "\n",
    "**CIFAR-10**: \n",
    "- Objetos del mundo real (aviones, coches, pájaros, etc.)\n",
    "- Imágenes en color (3 canales RGB)\n",
    "- Tamaño: 32x32 píxeles\n",
    "- Dificultad: **Media-Alta** - Más realista y desafiante\n",
    "- Fitness objetivo recomendado: >80%\n",
    "- Clases: plane, car, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "**CUSTOM**: \n",
    "- Tu propio dataset\n",
    "- Configuración manual requerida\n",
    "- Estructura de carpetas por clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "MNIST dataset loaded:\n",
      "   Classes: 10\n",
      "   Training samples: 60000\n",
      "   Test samples: 10000\n",
      "\n",
      "Dataset loaded successfully:\n",
      "   Batch shape: torch.Size([128, 1, 28, 28])\n",
      "   Data type: torch.float32\n",
      "   Device: cpu\n",
      "   Value range: [-0.424, 2.821]\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_type = config['dataset']\n",
    "    \n",
    "    if dataset_type == 'CUSTOM' and config['dataset_path']:\n",
    "        print(f\"Loading custom dataset from: {config['dataset_path']}\")\n",
    "        \n",
    "        # Transformations for custom dataset\n",
    "        if config['num_channels'] == 1:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        else:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        \n",
    "        # Load dataset from folders organized by class\n",
    "        full_dataset = datasets.ImageFolder(root=config['dataset_path'], transform=transform)\n",
    "        \n",
    "        # Split into train and test\n",
    "        train_size = int((1 - config['test_split']) * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "        \n",
    "        print(f\"Custom dataset loaded:\")\n",
    "        print(f\"   Classes found: {len(full_dataset.classes)}\")\n",
    "        print(f\"   Total samples: {len(full_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'MNIST':\n",
    "        print(\"Loading MNIST dataset...\")\n",
    "        \n",
    "        # Transformations for MNIST\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load MNIST\n",
    "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        \n",
    "        print(f\"MNIST dataset loaded:\")\n",
    "        print(f\"   Classes: {len(train_dataset.classes)}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'CIFAR10':\n",
    "        print(\"Loading CIFAR-10 dataset...\")\n",
    "        \n",
    "        # Transformations for CIFAR-10\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),  # Data augmentation for training\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load CIFAR-10\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "        \n",
    "        print(f\"CIFAR-10 dataset loaded:\")\n",
    "        print(f\"   Classes: {train_dataset.classes}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_type}' not supported. Available: MNIST, CIFAR10, CUSTOM\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load the dataset\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Get a sample to verify dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_data, sample_labels = sample_batch\n",
    "print(f\"\\nDataset loaded successfully:\")\n",
    "print(f\"   Batch shape: {sample_data.shape}\")\n",
    "print(f\"   Data type: {sample_data.dtype}\")\n",
    "print(f\"   Device: {sample_data.device}\")\n",
    "print(f\"   Value range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\n",
    "\n",
    "# Show some class information for CIFAR-10\n",
    "if CONFIG['dataset'] == 'CIFAR10':\n",
    "    cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    print(f\"   CIFAR-10 classes: {cifar10_classes}\")\n",
    "    unique_labels = torch.unique(sample_labels)\n",
    "    print(f\"   Labels in this batch: {unique_labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvolvableCNN class defined correctly\n"
     ]
    }
   ],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable CNN class that can be dynamically configured\n",
    "    according to genome parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds convolutional layers according to genome.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = self.config['num_channels']\n",
    "        \n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            out_channels = self.genome['filters'][i]\n",
    "            kernel_size = self.genome['kernel_sizes'][i]\n",
    "            \n",
    "            # Convolutional layer\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Batch normalization\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            # Activation function\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # Max pooling (except in last layer)\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d(2, 1))  # Stride 1 in last layer\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"Calculates output size after convolutional layers.\"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['px_h'], self.config['px_w'])\n",
    "        \n",
    "        # Pass through convolutional layers\n",
    "        x = dummy_input\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten and get size\n",
    "        return x.view(-1).shape[0]\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        \n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation except on last layer\n",
    "            if i < len(self.fc_layers) - 1 and not isinstance(layer, nn.Dropout):\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be19766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic functions updated for adaptive mutation\n"
     ]
    }
   ],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"Creates a random genome within specified ranges.\"\"\"\n",
    "    # Number of layers\n",
    "    num_conv_layers = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "    num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "    # Filters for each convolutional layer\n",
    "    filters = [random.randint(config['min_filters'], config['max_filters']) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Kernel sizes\n",
    "    kernel_sizes = [random.choice([3, 5, 7]) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Nodes in fully connected layers\n",
    "    fc_nodes = [random.randint(config['min_fc_nodes'], config['max_fc_nodes']) for _ in range(num_fc_layers)]\n",
    "\n",
    "    # Activation functions for each layer\n",
    "    activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "    # Other parameters\n",
    "    dropout_rate = random.uniform(0.1, 0.5)\n",
    "    learning_rate = random.choice([0.001, 0.0001, 0.01, 0.005])\n",
    "    optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    genome = {\n",
    "        'num_conv_layers': num_conv_layers,\n",
    "        'num_fc_layers': num_fc_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'fc_nodes': fc_nodes,\n",
    "        'activations': activations,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "    return genome\n",
    "\n",
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"Applies mutation to a genome using adaptive mutation rate.\"\"\"\n",
    "    mutated_genome = copy.deepcopy(genome)\n",
    "    mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "    # Mutate number of convolutional layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "        num_conv = mutated_genome['num_conv_layers']\n",
    "        mutated_genome['filters'] = mutated_genome['filters'][:num_conv]\n",
    "        mutated_genome['kernel_sizes'] = mutated_genome['kernel_sizes'][:num_conv]\n",
    "        while len(mutated_genome['filters']) < num_conv:\n",
    "            mutated_genome['filters'].append(random.randint(config['min_filters'], config['max_filters']))\n",
    "        while len(mutated_genome['kernel_sizes']) < num_conv:\n",
    "            mutated_genome['kernel_sizes'].append(random.choice([1, 3, 5, 7]))\n",
    "\n",
    "    # Mutate filters\n",
    "    for i in range(len(mutated_genome['filters'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "    # Mutate kernel sizes\n",
    "    for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['kernel_sizes'][i] = random.choice([1, 3, 5, 7])\n",
    "\n",
    "    # Mutate number of FC layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "        num_fc = mutated_genome['num_fc_layers']\n",
    "        mutated_genome['fc_nodes'] = mutated_genome['fc_nodes'][:num_fc]\n",
    "        while len(mutated_genome['fc_nodes']) < num_fc:\n",
    "            mutated_genome['fc_nodes'].append(random.randint(config['min_fc_nodes'], config['max_fc_nodes']))\n",
    "\n",
    "    # Mutate FC nodes\n",
    "    for i in range(len(mutated_genome['fc_nodes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "    # Mutate activation functions\n",
    "    for i in range(len(mutated_genome['activations'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['dropout_rate'] = random.uniform(0.1, 0.8)\n",
    "\n",
    "    # Mutate learning rate\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['learning_rate'] = random.choice([0.001, 0.0001, 0.01, 0.005, 0.000001, 0.05, 0.00005, 0.0005])\n",
    "\n",
    "    # Mutate optimizer\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    mutated_genome['fitness'] = 0.0\n",
    "    return mutated_genome\n",
    "\n",
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Performs crossover between two genomes.\"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "\n",
    "    # Crossover scalar parameters\n",
    "    for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer']:\n",
    "        if random.random() < 0.5:\n",
    "            child1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "    # Crossover lists (random cut point)\n",
    "    for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "        if random.random() < 0.5:\n",
    "            list1 = child1[list_key]\n",
    "            list2 = child2[list_key]\n",
    "            if len(list1) > 1 and len(list2) > 1:\n",
    "                point1 = random.randint(1, len(list1) - 1)\n",
    "                point2 = random.randint(1, len(list2) - 1)\n",
    "                child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    return child1, child2\n",
    "\n",
    "print(\"Genetic functions updated for adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridNeuroevolution class updated with adaptive mutation, interleaved early stopping, and threading support\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class HybridNeuroevolution:\n",
    "    \"\"\"Main class that implements hybrid neuroevolution with adaptive mutation & epoch interleaved eval + Threading.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict, train_loader: DataLoader, test_loader: DataLoader):\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_individual = None\n",
    "        self.fitness_history = []\n",
    "        self.generation_stats = []\n",
    "        # Threading configuration\n",
    "        self.max_concurrent_training = 2  # Máximo 2 modelos entrenando simultáneamente\n",
    "        self.training_semaphore = threading.Semaphore(self.max_concurrent_training)\n",
    "        self.fitness_lock = threading.Lock()  # Para acceso seguro a variables compartidas\n",
    "\n",
    "    def initialize_population(self):\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "\n",
    "    def _train_one_epoch(self, model, optimizer, criterion, genome_id: str, epoch: int, thread_name: str):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        max_batches = min(len(self.train_loader), self.config['early_stopping_patience'])\n",
    "        for data, target in self.train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if batch_count >= max_batches:\n",
    "                break\n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        print(f\"{thread_name}: Train Epoch {epoch}: loss={avg_loss:.4f} ({batch_count} batches)\")\n",
    "        return avg_loss\n",
    "\n",
    "    def _evaluate(self, model, criterion, genome_id: str, epoch: int, thread_name: str):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        eval_batches = 0\n",
    "        max_eval_batches = min(len(self.test_loader), 20)\n",
    "        total_eval_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_eval_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                eval_batches += 1\n",
    "                if eval_batches >= max_eval_batches:\n",
    "                    break\n",
    "        accuracy = 100.0 * correct / max(1, total)\n",
    "        avg_eval_loss = total_eval_loss / max(1, eval_batches)\n",
    "        print(f\"{thread_name}: Eval  Epoch {epoch}: acc={accuracy:.2f}% loss={avg_eval_loss:.4f} ({eval_batches} batches)\")\n",
    "        return accuracy, avg_eval_loss\n",
    "\n",
    "    def evaluate_fitness(self, genome: dict, thread_name: str = \"MainThread\") -> float:\n",
    "        \"\"\"Thread-safe fitness evaluation method.\"\"\"\n",
    "        try:\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "            optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            best_acc = 0.0\n",
    "            best_epoch = -1\n",
    "            patience_left = self.config['epoch_patience']\n",
    "            last_improvement_acc = 0.0\n",
    "\n",
    "            max_epochs = self.config['num_epochs']\n",
    "            print(f\"{thread_name}: Training/Evaluating model {genome['id']} (max {max_epochs} epochs interleaved)\")\n",
    "\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                # 1) Train epoch\n",
    "                self._train_one_epoch(model, optimizer, criterion, genome['id'], epoch, thread_name)\n",
    "                # 2) Evaluate right after training epoch\n",
    "                acc, eval_loss = self._evaluate(model, criterion, genome['id'], epoch, thread_name)\n",
    "\n",
    "                # Early stopping logic based on accuracy improvement\n",
    "                improvement = acc - last_improvement_acc\n",
    "                if improvement >= self.config['improvement_threshold']:\n",
    "                    patience_left = self.config['epoch_patience']\n",
    "                    last_improvement_acc = acc\n",
    "                else:\n",
    "                    patience_left -= 1\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                print(f\"{thread_name}: -> Acc={acc:.2f}% (best={best_acc:.2f}% at epoch {best_epoch}) patience_left={patience_left}\")\n",
    "\n",
    "                if patience_left <= 0:\n",
    "                    print(f\"{thread_name}: Early stopping triggered (no significant improvement)\")\n",
    "                    break\n",
    "\n",
    "            print(f\"{thread_name}: Final fitness for {genome['id']}: {best_acc:.2f}% (best epoch {best_epoch})\")\n",
    "            return best_acc\n",
    "        except Exception as e:\n",
    "            print(f\"{thread_name}: ERROR evaluating genome {genome['id']}: {e}\")\n",
    "            logger.warning(f\"Error evaluating genome {genome['id']}: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _worker_evaluate_genome(self, genome: dict, index: int, total: int, results: list):\n",
    "        \"\"\"Worker function that evaluates a single genome in a thread with semaphore control.\"\"\"\n",
    "        thread_name = f\"Thread-{index+1}\"\n",
    "        \n",
    "        # Acquire semaphore to limit concurrent training\n",
    "        self.training_semaphore.acquire()\n",
    "        try:\n",
    "            print(f\"{thread_name}: iniciando evaluación (máx {self.max_concurrent_training} concurrentes)...\")\n",
    "            print(f\"{thread_name}: Evaluating individual {index+1}/{total} (ID: {genome['id']})\")\n",
    "            print(f\"{thread_name}: Architecture: {genome['num_conv_layers']} conv + {genome['num_fc_layers']} fc, opt={genome['optimizer']}, lr={genome['learning_rate']}\")\n",
    "            \n",
    "            # Evaluate fitness\n",
    "            fitness = self.evaluate_fitness(genome, thread_name)\n",
    "            \n",
    "            # Thread-safe update of results\n",
    "            with self.fitness_lock:\n",
    "                genome['fitness'] = fitness\n",
    "                results[index] = fitness\n",
    "                print(f\"{thread_name}: Fitness obtained: {fitness:.2f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"{thread_name}: Error durante evaluación: {e}\")\n",
    "            with self.fitness_lock:\n",
    "                genome['fitness'] = 0.0\n",
    "                results[index] = 0.0\n",
    "        finally:\n",
    "            print(f\"{thread_name}: terminado, liberando slot de entrenamiento.\")\n",
    "            self.training_semaphore.release()\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        \"\"\"Thread-safe population evaluation with concurrent model training.\"\"\"\n",
    "        print(f\"\\nEvaluating population with threading (Generation {self.generation})...\")\n",
    "        print(f\"Processing {len(self.population)} individuals with max {self.max_concurrent_training} concurrent training...\")\n",
    "        \n",
    "        # Initialize results list\n",
    "        fitness_scores = [0.0] * len(self.population)\n",
    "        best_fitness_so_far = 0.0\n",
    "        \n",
    "        # Create and start threads\n",
    "        threads = []\n",
    "        for i, genome in enumerate(self.population):\n",
    "            thread = threading.Thread(\n",
    "                target=self._worker_evaluate_genome,\n",
    "                args=(genome, i, len(self.population), fitness_scores)\n",
    "            )\n",
    "            threads.append(thread)\n",
    "        \n",
    "        # Start all threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "        # Calculate generation statistics\n",
    "        if fitness_scores:\n",
    "            avg_fitness = np.mean(fitness_scores)\n",
    "            max_fitness = np.max(fitness_scores)\n",
    "            min_fitness = np.min(fitness_scores)\n",
    "            std_fitness = np.std(fitness_scores)\n",
    "        else:\n",
    "            avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "        stats = {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': avg_fitness,\n",
    "            'max_fitness': max_fitness,\n",
    "            'min_fitness': min_fitness,\n",
    "            'std_fitness': std_fitness\n",
    "        }\n",
    "        self.generation_stats.append(stats)\n",
    "        self.fitness_history.append(max_fitness)\n",
    "\n",
    "        best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "        if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "            self.best_individual = copy.deepcopy(best_genome)\n",
    "            print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "        print(f\"\\nGENERATION {self.generation} STATISTICS (Threading completed):\")\n",
    "        print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "        print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "        print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "        print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "        print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "        print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Concurrent training slots used: {self.max_concurrent_training}\")\n",
    "\n",
    "    def selection_and_reproduction(self):\n",
    "        print(f\"\\nStarting selection and reproduction...\")\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "        elite = self.population[:elite_size]\n",
    "        print(f\"Selecting {elite_size} elite individuals:\")\n",
    "        for i, individual in enumerate(elite):\n",
    "            print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "        new_population = copy.deepcopy(elite)\n",
    "        offspring_needed = self.config['population_size'] - len(new_population)\n",
    "        print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "        offspring_created = 0\n",
    "        while len(new_population) < self.config['population_size']:\n",
    "            parent1 = self.tournament_selection()\n",
    "            parent2 = self.tournament_selection()\n",
    "            child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "            child1 = mutate_genome(child1, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child1)\n",
    "            child2 = mutate_genome(child2, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child2)\n",
    "            offspring_created += 2\n",
    "            if offspring_created % 4 == 0:\n",
    "                print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "        self.population = new_population[:self.config['population_size']]\n",
    "        print(f\"New generation created with {len(self.population)} individuals\")\n",
    "        print(f\"   Elite preserved: {elite_size}\")\n",
    "        print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "    def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "        tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "        return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "    def _update_adaptive_mutation(self):\n",
    "        # Diversity measured via std of fitness in last generation\n",
    "        if not self.generation_stats:\n",
    "            self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "            return\n",
    "        last_std = self.generation_stats[-1]['std_fitness']\n",
    "        # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "        # Normalize std roughly assuming fitness in [0,100]\n",
    "        diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "        # Invert: low diversity (small std) should raise mutation\n",
    "        inverted = 1 - diversity_factor\n",
    "        new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "        new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "        self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "        print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "    def check_convergence(self) -> bool:\n",
    "        if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "            print(f\"Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "            return True\n",
    "        if self.generation >= self.config['max_generations']:\n",
    "            print(f\"Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "            return True\n",
    "        if len(self.fitness_history) >= 3:\n",
    "            recent = self.fitness_history[-3:]\n",
    "            if max(recent) - min(recent) < 0.5:\n",
    "                print(\"Stagnation detected in last 3 generations\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def evolve(self) -> dict:\n",
    "        print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + threading)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "        print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "        print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "        print(f\"   Max concurrent training: {self.max_concurrent_training} models\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(\"=\"*60)\n",
    "        self.initialize_population()\n",
    "        while not self.check_convergence():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GENERATION {self.generation}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            self.evaluate_population()\n",
    "            if self.check_convergence():\n",
    "                break\n",
    "            self._update_adaptive_mutation()\n",
    "            self.selection_and_reproduction()\n",
    "            self.generation += 1\n",
    "            print(f\"\\nPreparing for next generation...\")\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVOLUTION COMPLETED!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best individual found:\")\n",
    "        print(f\"   ID: {self.best_individual['id']}\")\n",
    "        print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Origin generation: {self.generation}\")\n",
    "        print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "        print(f\"   Threading efficiency: {self.max_concurrent_training} concurrent training slots\")\n",
    "        return self.best_individual\n",
    "\n",
    "print(\"HybridNeuroevolution class updated with adaptive mutation, interleaved early stopping, and threading support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be51ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration:\n",
      "   Dataset: MNIST\n",
      "   Image size: 28x28x1\n",
      "   Number of classes: 10\n",
      "   Population: 10 individuals\n",
      "   Maximum generations: 30\n",
      "   Target fitness: 99.9%\n",
      "   Device: cuda\n",
      "\n",
      "Reloading dataset with new configuration...\n",
      "Loading MNIST dataset...\n",
      "MNIST dataset loaded:\n",
      "   Classes: 10\n",
      "   Training samples: 60000\n",
      "   Test samples: 10000\n",
      "\n",
      "Starting neuroevolution at 21:11:30\n",
      "STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + threading)\n",
      "============================================================\n",
      "Configuration:\n",
      "   Population: 10 individuals\n",
      "   Maximum generations: 30\n",
      "   Target fitness: 99.9%\n",
      "   Max concurrent training: 2 models\n",
      "   Device: cuda\n",
      "============================================================\n",
      "Initializing population of 10 individuals...\n",
      "Population initialized with 10 individuals\n",
      "\n",
      "================================================================================\n",
      "GENERATION 0\n",
      "================================================================================\n",
      "\n",
      "Evaluating population with threading (Generation 0)...\n",
      "Processing 10 individuals with max 2 concurrent training...\n",
      "Thread-1: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-1: Evaluating individual 1/10 (ID: 4831919a)\n",
      "Thread-1: Architecture: 6 conv + 5 fc, opt=adam, lr=0.001\n",
      "Thread-2: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-2: Evaluating individual 2/10 (ID: 72b5ed20)\n",
      "Thread-2: Architecture: 7 conv + 3 fc, opt=adam, lr=0.005\n",
      "Thread-1: ERROR evaluating genome 4831919a: Expected more than 1 value per channel when training, got input size torch.Size([1, 63, 1, 1])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:11:30,238 - WARNING - Error evaluating genome 4831919a: Expected more than 1 value per channel when training, got input size torch.Size([1, 63, 1, 1])\n",
      "2025-09-17 21:11:30,261 - WARNING - Error evaluating genome 72b5ed20: Calculated padded input size per channel: (6 x 6). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thread-1: Fitness obtained: 0.00%\n",
      "Thread-1: terminado, liberando slot de entrenamiento.\n",
      "Thread-3: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-3: Evaluating individual 3/10 (ID: b9eab84f)\n",
      "Thread-3: Architecture: 5 conv + 7 fc, opt=adam, lr=0.0001\n",
      "Thread-2: ERROR evaluating genome 72b5ed20: Calculated padded input size per channel: (6 x 6). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "Thread-2: Fitness obtained: 0.00%\n",
      "Thread-2: terminado, liberando slot de entrenamiento.\n",
      "Thread-4: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-4: Evaluating individual 4/10 (ID: cc1e6edb)\n",
      "Thread-4: Architecture: 5 conv + 5 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-3: ERROR evaluating genome b9eab84f: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:11:30,279 - WARNING - Error evaluating genome b9eab84f: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "2025-09-17 21:11:30,289 - WARNING - Error evaluating genome cc1e6edb: Calculated padded input size per channel: (4 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "2025-09-17 21:11:30,300 - WARNING - Error evaluating genome 2490715d: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-3: Fitness obtained: 0.00%\n",
      "Thread-3: terminado, liberando slot de entrenamiento.\n",
      "Thread-5: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-5: Evaluating individual 5/10 (ID: 2490715d)\n",
      "Thread-5: Architecture: 5 conv + 7 fc, opt=sgd, lr=0.0001\n",
      "Thread-4: ERROR evaluating genome cc1e6edb: Calculated padded input size per channel: (4 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "Thread-4: Fitness obtained: 0.00%\n",
      "Thread-4: terminado, liberando slot de entrenamiento.\n",
      "Thread-6: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-6: Evaluating individual 6/10 (ID: 23b40aea)\n",
      "Thread-6: Architecture: 1 conv + 7 fc, opt=adamw, lr=0.0001\n",
      "Thread-5: ERROR evaluating genome 2490715d: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "Thread-5: Fitness obtained: 0.00%\n",
      "Thread-5: terminado, liberando slot de entrenamiento.\n",
      "Thread-7: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-7: Evaluating individual 7/10 (ID: bfcdfda6)\n",
      "Thread-7: Architecture: 5 conv + 5 fc, opt=adam, lr=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:11:30,931 - WARNING - Error evaluating genome bfcdfda6: Expected more than 1 value per channel when training, got input size torch.Size([1, 228, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-7: ERROR evaluating genome bfcdfda6: Expected more than 1 value per channel when training, got input size torch.Size([1, 228, 1, 1])\n",
      "Thread-7: Fitness obtained: 0.00%\n",
      "Thread-7: terminado, liberando slot de entrenamiento.\n",
      "Thread-8: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-8: Evaluating individual 8/10 (ID: 681214d0)\n",
      "Thread-8: Architecture: 3 conv + 1 fc, opt=rmsprop, lr=0.001\n",
      "Thread-6: Training/Evaluating model 23b40aea (max 12 epochs interleaved)\n",
      "Thread-8: Training/Evaluating model 681214d0 (max 12 epochs interleaved)\n",
      "Thread-6: Train Epoch 1: loss=0.3837 (469 batches)Thread-8: Train Epoch 1: loss=1.6109 (469 batches)\n",
      "\n",
      "Thread-6: Eval  Epoch 1: acc=94.77% loss=0.1703 (20 batches)\n",
      "Thread-6: -> Acc=94.77% (best=94.77% at epoch 1) patience_left=3\n",
      "Thread-8: Eval  Epoch 1: acc=96.21% loss=0.1183 (20 batches)\n",
      "Thread-8: -> Acc=96.21% (best=96.21% at epoch 1) patience_left=3\n",
      "Thread-6: Train Epoch 2: loss=0.1103 (469 batches)\n",
      "Thread-8: Train Epoch 2: loss=0.0962 (469 batches)\n",
      "Thread-8: Eval  Epoch 2: acc=97.50% loss=0.0816 (20 batches)\n",
      "Thread-8: -> Acc=97.50% (best=97.50% at epoch 2) patience_left=3\n",
      "Thread-6: Eval  Epoch 2: acc=96.80% loss=0.1081 (20 batches)\n",
      "Thread-6: -> Acc=96.80% (best=96.80% at epoch 2) patience_left=3\n",
      "Thread-6: Train Epoch 3: loss=0.0753 (469 batches)\n",
      "Thread-8: Train Epoch 3: loss=0.0571 (469 batches)\n",
      "Thread-8: Eval  Epoch 3: acc=98.40% loss=0.0498 (20 batches)\n",
      "Thread-6: Eval  Epoch 3: acc=97.07% loss=0.1040 (20 batches)\n",
      "Thread-8: -> Acc=98.40% (best=98.40% at epoch 3) patience_left=3\n",
      "Thread-6: -> Acc=97.07% (best=97.07% at epoch 3) patience_left=3\n",
      "Thread-6: Train Epoch 4: loss=0.0582 (469 batches)\n",
      "Thread-8: Train Epoch 4: loss=0.0350 (469 batches)\n",
      "Thread-6: Eval  Epoch 4: acc=97.34% loss=0.0824 (20 batches)\n",
      "Thread-6: -> Acc=97.34% (best=97.34% at epoch 4) patience_left=3\n",
      "Thread-8: Eval  Epoch 4: acc=98.55% loss=0.0439 (20 batches)\n",
      "Thread-8: -> Acc=98.55% (best=98.55% at epoch 4) patience_left=2\n",
      "Thread-6: Train Epoch 5: loss=0.0486 (469 batches)\n",
      "Thread-8: Train Epoch 5: loss=0.0295 (469 batches)\n",
      "Thread-8: Eval  Epoch 5: acc=89.92% loss=0.4439 (20 batches)\n",
      "Thread-8: -> Acc=89.92% (best=98.55% at epoch 4) patience_left=1\n",
      "Thread-6: Eval  Epoch 5: acc=97.77% loss=0.0831 (20 batches)\n",
      "Thread-6: -> Acc=97.77% (best=97.77% at epoch 5) patience_left=3\n",
      "Thread-6: Train Epoch 6: loss=0.0418 (469 batches)\n",
      "Thread-8: Train Epoch 6: loss=0.0236 (469 batches)\n",
      "Thread-8: Eval  Epoch 6: acc=98.63% loss=0.0426 (20 batches)\n",
      "Thread-8: -> Acc=98.63% (best=98.63% at epoch 6) patience_left=3\n",
      "Thread-6: Eval  Epoch 6: acc=97.62% loss=0.0808 (20 batches)\n",
      "Thread-6: -> Acc=97.62% (best=97.77% at epoch 5) patience_left=2\n",
      "Thread-6: Train Epoch 7: loss=0.0363 (469 batches)\n",
      "Thread-8: Train Epoch 7: loss=0.0173 (469 batches)\n",
      "Thread-6: Eval  Epoch 7: acc=97.81% loss=0.0812 (20 batches)\n",
      "Thread-6: -> Acc=97.81% (best=97.81% at epoch 7) patience_left=1\n",
      "Thread-8: Eval  Epoch 7: acc=98.12% loss=0.0660 (20 batches)\n",
      "Thread-8: -> Acc=98.12% (best=98.63% at epoch 6) patience_left=2\n",
      "Thread-6: Train Epoch 8: loss=0.0284 (469 batches)\n",
      "Thread-8: Train Epoch 8: loss=0.0178 (469 batches)\n",
      "Thread-6: Eval  Epoch 8: acc=97.58% loss=0.1080 (20 batches)\n",
      "Thread-6: -> Acc=97.58% (best=97.81% at epoch 7) patience_left=0\n",
      "Thread-6: Early stopping triggered (no significant improvement)\n",
      "Thread-6: Final fitness for 23b40aea: 97.81% (best epoch 7)\n",
      "Thread-6: Fitness obtained: 97.81%\n",
      "Thread-6: terminado, liberando slot de entrenamiento.\n",
      "Thread-9: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-9: Evaluating individual 9/10 (ID: 49270d26)\n",
      "Thread-9: Architecture: 2 conv + 6 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-8: Eval  Epoch 8: acc=98.79% loss=0.0378 (20 batches)\n",
      "Thread-8: -> Acc=98.79% (best=98.79% at epoch 8) patience_left=1\n",
      "Thread-9: Training/Evaluating model 49270d26 (max 12 epochs interleaved)\n",
      "Thread-9: Train Epoch 1: loss=0.4617 (469 batches)\n",
      "Thread-8: Train Epoch 9: loss=0.0129 (469 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:18:42,231 - WARNING - Error evaluating genome 850a8693: Expected more than 1 value per channel when training, got input size torch.Size([1, 163, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-9: Eval  Epoch 1: acc=96.29% loss=0.1163 (20 batches)Thread-8: Eval  Epoch 9: acc=97.77% loss=0.1045 (20 batches)\n",
      "Thread-8: -> Acc=97.77% (best=98.79% at epoch 8) patience_left=0\n",
      "Thread-8: Early stopping triggered (no significant improvement)\n",
      "Thread-8: Final fitness for 681214d0: 98.79% (best epoch 8)\n",
      "\n",
      "Thread-9: -> Acc=96.29% (best=96.29% at epoch 1) patience_left=3\n",
      "Thread-8: Fitness obtained: 98.79%\n",
      "Thread-8: terminado, liberando slot de entrenamiento.\n",
      "Thread-10: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-10: Evaluating individual 10/10 (ID: 850a8693)\n",
      "Thread-10: Architecture: 5 conv + 1 fc, opt=sgd, lr=0.0001\n",
      "Thread-10: ERROR evaluating genome 850a8693: Expected more than 1 value per channel when training, got input size torch.Size([1, 163, 1, 1])\n",
      "Thread-10: Fitness obtained: 0.00%\n",
      "Thread-10: terminado, liberando slot de entrenamiento.\n",
      "Thread-9: Train Epoch 2: loss=0.0906 (469 batches)\n",
      "Thread-9: Eval  Epoch 2: acc=96.80% loss=0.1170 (20 batches)\n",
      "Thread-9: -> Acc=96.80% (best=96.80% at epoch 2) patience_left=3\n",
      "Thread-9: Train Epoch 3: loss=0.0612 (469 batches)\n",
      "Thread-9: Eval  Epoch 3: acc=97.93% loss=0.0738 (20 batches)\n",
      "Thread-9: -> Acc=97.93% (best=97.93% at epoch 3) patience_left=3\n",
      "Thread-9: Train Epoch 4: loss=0.0483 (469 batches)\n",
      "Thread-9: Eval  Epoch 4: acc=95.55% loss=0.1704 (20 batches)\n",
      "Thread-9: -> Acc=95.55% (best=97.93% at epoch 3) patience_left=2\n",
      "Thread-9: Train Epoch 5: loss=0.0409 (469 batches)\n",
      "Thread-9: Eval  Epoch 5: acc=98.05% loss=0.0634 (20 batches)\n",
      "Thread-9: -> Acc=98.05% (best=98.05% at epoch 5) patience_left=1\n",
      "Thread-9: Train Epoch 6: loss=0.0350 (469 batches)\n",
      "Thread-9: Eval  Epoch 6: acc=98.40% loss=0.0572 (20 batches)\n",
      "Thread-9: -> Acc=98.40% (best=98.40% at epoch 6) patience_left=3\n",
      "Thread-9: Train Epoch 7: loss=0.0302 (469 batches)\n",
      "Thread-9: Eval  Epoch 7: acc=98.12% loss=0.0709 (20 batches)\n",
      "Thread-9: -> Acc=98.12% (best=98.40% at epoch 6) patience_left=2\n",
      "Thread-9: Train Epoch 8: loss=0.0264 (469 batches)\n",
      "Thread-9: Eval  Epoch 8: acc=77.58% loss=1.3168 (20 batches)\n",
      "Thread-9: -> Acc=77.58% (best=98.40% at epoch 6) patience_left=1\n",
      "Thread-9: Train Epoch 9: loss=0.0235 (469 batches)\n",
      "Thread-9: Eval  Epoch 9: acc=95.66% loss=0.1785 (20 batches)\n",
      "Thread-9: -> Acc=95.66% (best=98.40% at epoch 6) patience_left=0\n",
      "Thread-9: Early stopping triggered (no significant improvement)\n",
      "Thread-9: Final fitness for 49270d26: 98.40% (best epoch 6)\n",
      "Thread-9: Fitness obtained: 98.40%\n",
      "Thread-9: terminado, liberando slot de entrenamiento.\n",
      "\n",
      "New global best individual found!\n",
      "\n",
      "GENERATION 0 STATISTICS (Threading completed):\n",
      "   Maximum fitness: 98.79%\n",
      "   Average fitness: 29.50%\n",
      "   Minimum fitness: 0.00%\n",
      "   Standard deviation: 45.06%\n",
      "   Best individual: 681214d0 with 98.79%\n",
      "   Global best individual: 681214d0 with 98.79%\n",
      "   Concurrent training slots used: 2\n",
      "Adaptive mutation rate updated to 0.15 (std_fitness=45.06)\n",
      "\n",
      "Starting selection and reproduction...\n",
      "Selecting 2 elite individuals:\n",
      "   Elite 1: 681214d0 (fitness: 98.79%)\n",
      "   Elite 2: 49270d26 (fitness: 98.40%)\n",
      "Creating 8 new individuals through crossover and mutation...\n",
      "   Created 4 of 8 new individuals...\n",
      "   Created 8 of 8 new individuals...\n",
      "New generation created with 10 individuals\n",
      "   Elite preserved: 2\n",
      "   New individuals: 8\n",
      "\n",
      "Preparing for next generation...\n",
      "\n",
      "================================================================================\n",
      "GENERATION 1\n",
      "================================================================================\n",
      "\n",
      "Evaluating population with threading (Generation 1)...\n",
      "Processing 10 individuals with max 2 concurrent training...\n",
      "Thread-1: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-1: Evaluating individual 1/10 (ID: 681214d0)\n",
      "Thread-1: Architecture: 3 conv + 1 fc, opt=rmsprop, lr=0.001\n",
      "Thread-2: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-2: Evaluating individual 2/10 (ID: 49270d26)\n",
      "Thread-2: Architecture: 2 conv + 6 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-1: Training/Evaluating model 681214d0 (max 12 epochs interleaved)\n",
      "Thread-2: Training/Evaluating model 49270d26 (max 12 epochs interleaved)\n",
      "Thread-2: Train Epoch 1: loss=0.4220 (469 batches)\n",
      "Thread-1: Train Epoch 1: loss=1.5330 (469 batches)\n",
      "Thread-1: Eval  Epoch 1: acc=93.75% loss=0.1841 (20 batches)\n",
      "Thread-1: -> Acc=93.75% (best=93.75% at epoch 1) patience_left=3\n",
      "Thread-2: Eval  Epoch 1: acc=93.79% loss=0.2178 (20 batches)\n",
      "Thread-2: -> Acc=93.79% (best=93.79% at epoch 1) patience_left=3\n",
      "Thread-2: Train Epoch 2: loss=0.0883 (469 batches)\n",
      "Thread-1: Train Epoch 2: loss=0.1060 (469 batches)\n",
      "Thread-1: Eval  Epoch 2: acc=97.19% loss=0.0922 (20 batches)Thread-2: Eval  Epoch 2: acc=90.08% loss=0.3307 (20 batches)\n",
      "\n",
      "Thread-2: -> Acc=90.08% (best=93.79% at epoch 1) patience_left=2\n",
      "Thread-1: -> Acc=97.19% (best=97.19% at epoch 2) patience_left=3\n",
      "Thread-2: Train Epoch 3: loss=0.0604 (469 batches)\n",
      "Thread-1: Train Epoch 3: loss=0.0536 (469 batches)\n",
      "Thread-1: Eval  Epoch 3: acc=98.28% loss=0.0488 (20 batches)\n",
      "Thread-1: -> Acc=98.28% (best=98.28% at epoch 3) patience_left=3\n",
      "Thread-2: Eval  Epoch 3: acc=97.62% loss=0.0924 (20 batches)\n",
      "Thread-2: -> Acc=97.62% (best=97.62% at epoch 3) patience_left=3\n",
      "Thread-2: Train Epoch 4: loss=0.0493 (469 batches)\n",
      "Thread-1: Train Epoch 4: loss=0.0367 (469 batches)\n",
      "Thread-1: Eval  Epoch 4: acc=98.32% loss=0.0521 (20 batches)\n",
      "Thread-1: -> Acc=98.32% (best=98.32% at epoch 4) patience_left=2\n",
      "Thread-2: Eval  Epoch 4: acc=98.20% loss=0.0549 (20 batches)\n",
      "Thread-2: -> Acc=98.20% (best=98.20% at epoch 4) patience_left=3\n",
      "Thread-2: Train Epoch 5: loss=0.0401 (469 batches)Thread-1: Train Epoch 5: loss=0.0275 (469 batches)\n",
      "\n",
      "Thread-1: Eval  Epoch 5: acc=97.50% loss=0.1063 (20 batches)\n",
      "Thread-1: -> Acc=97.50% (best=98.32% at epoch 4) patience_left=1\n",
      "Thread-2: Eval  Epoch 5: acc=98.28% loss=0.0545 (20 batches)\n",
      "Thread-2: -> Acc=98.28% (best=98.28% at epoch 5) patience_left=2\n",
      "Thread-2: Train Epoch 6: loss=0.0330 (469 batches)\n",
      "Thread-1: Train Epoch 6: loss=0.0237 (469 batches)\n",
      "Thread-1: Eval  Epoch 6: acc=97.30% loss=0.1081 (20 batches)\n",
      "Thread-1: -> Acc=97.30% (best=98.32% at epoch 4) patience_left=0\n",
      "Thread-1: Early stopping triggered (no significant improvement)\n",
      "Thread-1: Final fitness for 681214d0: 98.32% (best epoch 4)\n",
      "Thread-1: Fitness obtained: 98.32%\n",
      "Thread-1: terminado, liberando slot de entrenamiento.\n",
      "Thread-3: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-3: Evaluating individual 3/10 (ID: a2f17087)\n",
      "Thread-3: Architecture: 1 conv + 6 fc, opt=adamw, lr=1e-06\n",
      "Thread-2: Eval  Epoch 6: acc=98.05% loss=0.0726 (20 batches)\n",
      "Thread-2: -> Acc=98.05% (best=98.28% at epoch 5) patience_left=1\n",
      "Thread-3: Training/Evaluating model a2f17087 (max 12 epochs interleaved)\n",
      "Thread-2: Train Epoch 7: loss=0.0298 (469 batches)\n",
      "Thread-3: Train Epoch 1: loss=2.2972 (469 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:25:12,555 - WARNING - Error evaluating genome ddec4ad5: list index out of range\n",
      "2025-09-17 21:25:12,585 - WARNING - Error evaluating genome 2b3a0715: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-2: Eval  Epoch 7: acc=97.66% loss=0.0800 (20 batches)\n",
      "Thread-2: -> Acc=97.66% (best=98.28% at epoch 5) patience_left=0\n",
      "Thread-2: Early stopping triggered (no significant improvement)\n",
      "Thread-2: Final fitness for 49270d26: 98.28% (best epoch 5)\n",
      "Thread-2: Fitness obtained: 98.28%\n",
      "Thread-2: terminado, liberando slot de entrenamiento.\n",
      "Thread-4: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-4: Evaluating individual 4/10 (ID: ddec4ad5)\n",
      "Thread-4: Architecture: 2 conv + 7 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-4: ERROR evaluating genome ddec4ad5: list index out of range\n",
      "Thread-3: Eval  Epoch 1: acc=44.69% loss=2.2786 (20 batches)\n",
      "Thread-3: -> Acc=44.69% (best=44.69% at epoch 1) patience_left=3\n",
      "Thread-4: Fitness obtained: 0.00%\n",
      "Thread-4: terminado, liberando slot de entrenamiento.\n",
      "Thread-5: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-5: Evaluating individual 5/10 (ID: 2b3a0715)\n",
      "Thread-5: Architecture: 6 conv + 7 fc, opt=adam, lr=0.001\n",
      "Thread-5: ERROR evaluating genome 2b3a0715: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "Thread-5: Fitness obtained: 0.00%\n",
      "Thread-5: terminado, liberando slot de entrenamiento.\n",
      "Thread-6: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-6: Evaluating individual 6/10 (ID: 5081142d)\n",
      "Thread-6: Architecture: 1 conv + 7 fc, opt=adamw, lr=0.0001\n",
      "Thread-6: Training/Evaluating model 5081142d (max 12 epochs interleaved)\n",
      "Thread-3: Train Epoch 2: loss=2.1924 (469 batches)\n",
      "Thread-6: Train Epoch 1: loss=2.1091 (469 batches)\n",
      "Thread-3: Eval  Epoch 2: acc=38.32% loss=1.9193 (20 batches)\n",
      "Thread-3: -> Acc=38.32% (best=44.69% at epoch 1) patience_left=2\n",
      "Thread-6: Eval  Epoch 1: acc=33.48% loss=1.6134 (20 batches)\n",
      "Thread-6: -> Acc=33.48% (best=33.48% at epoch 1) patience_left=3\n",
      "Thread-3: Train Epoch 3: loss=1.7438 (469 batches)\n",
      "Thread-6: Train Epoch 2: loss=1.6732 (469 batches)\n",
      "Thread-3: Eval  Epoch 3: acc=46.76% loss=1.4890 (20 batches)\n",
      "Thread-3: -> Acc=46.76% (best=46.76% at epoch 3) patience_left=3\n",
      "Thread-6: Eval  Epoch 2: acc=39.18% loss=1.5505 (20 batches)\n",
      "Thread-6: -> Acc=39.18% (best=39.18% at epoch 2) patience_left=3\n",
      "Thread-3: Train Epoch 4: loss=1.4633 (469 batches)\n",
      "Thread-3: Eval  Epoch 4: acc=53.01% loss=1.3262 (20 batches)\n",
      "Thread-3: -> Acc=53.01% (best=53.01% at epoch 4) patience_left=3\n",
      "Thread-6: Train Epoch 3: loss=1.5952 (469 batches)\n",
      "Thread-6: Eval  Epoch 3: acc=39.22% loss=1.4630 (20 batches)\n",
      "Thread-6: -> Acc=39.22% (best=39.22% at epoch 3) patience_left=2\n",
      "Thread-3: Train Epoch 5: loss=1.3105 (469 batches)\n",
      "Thread-3: Eval  Epoch 5: acc=56.64% loss=1.2085 (20 batches)\n",
      "Thread-3: -> Acc=56.64% (best=56.64% at epoch 5) patience_left=3\n",
      "Thread-6: Train Epoch 4: loss=1.5045 (469 batches)\n",
      "Thread-6: Eval  Epoch 4: acc=46.05% loss=1.3516 (20 batches)\n",
      "Thread-6: -> Acc=46.05% (best=46.05% at epoch 4) patience_left=3\n",
      "Thread-3: Train Epoch 6: loss=1.1935 (469 batches)\n",
      "Thread-3: Eval  Epoch 6: acc=60.62% loss=1.1131 (20 batches)\n",
      "Thread-3: -> Acc=60.62% (best=60.62% at epoch 6) patience_left=3\n",
      "Thread-6: Train Epoch 5: loss=1.4224 (469 batches)\n",
      "Thread-3: Train Epoch 7: loss=1.0816 (469 batches)\n",
      "Thread-6: Eval  Epoch 5: acc=49.61% loss=1.4362 (20 batches)\n",
      "Thread-6: -> Acc=49.61% (best=49.61% at epoch 5) patience_left=3\n",
      "Thread-3: Eval  Epoch 7: acc=63.59% loss=1.0102 (20 batches)\n",
      "Thread-3: -> Acc=63.59% (best=63.59% at epoch 7) patience_left=3\n",
      "Thread-6: Train Epoch 6: loss=1.2824 (469 batches)\n",
      "Thread-3: Train Epoch 8: loss=0.9865 (469 batches)\n",
      "Thread-6: Eval  Epoch 6: acc=47.54% loss=1.4559 (20 batches)\n",
      "Thread-6: -> Acc=47.54% (best=49.61% at epoch 5) patience_left=2\n",
      "Thread-3: Eval  Epoch 8: acc=67.19% loss=0.9339 (20 batches)\n",
      "Thread-3: -> Acc=67.19% (best=67.19% at epoch 8) patience_left=3\n",
      "Thread-6: Train Epoch 7: loss=1.1784 (469 batches)\n",
      "Thread-3: Train Epoch 9: loss=0.9115 (469 batches)\n",
      "Thread-6: Eval  Epoch 7: acc=50.12% loss=1.3675 (20 batches)\n",
      "Thread-6: -> Acc=50.12% (best=50.12% at epoch 7) patience_left=3\n",
      "Thread-3: Eval  Epoch 9: acc=68.79% loss=0.8837 (20 batches)\n",
      "Thread-3: -> Acc=68.79% (best=68.79% at epoch 9) patience_left=3\n",
      "Thread-6: Train Epoch 8: loss=1.0745 (469 batches)\n",
      "Thread-3: Train Epoch 10: loss=0.8575 (469 batches)\n",
      "Thread-3: Eval  Epoch 10: acc=71.21% loss=0.8373 (20 batches)\n",
      "Thread-3: -> Acc=71.21% (best=71.21% at epoch 10) patience_left=3\n",
      "Thread-6: Eval  Epoch 8: acc=41.48% loss=1.7205 (20 batches)\n",
      "Thread-6: -> Acc=41.48% (best=50.12% at epoch 7) patience_left=2\n",
      "Thread-6: Train Epoch 9: loss=0.9900 (469 batches)\n",
      "Thread-3: Train Epoch 11: loss=0.8176 (469 batches)\n",
      "Thread-6: Eval  Epoch 9: acc=41.29% loss=1.7444 (20 batches)\n",
      "Thread-6: -> Acc=41.29% (best=50.12% at epoch 7) patience_left=1\n",
      "Thread-3: Eval  Epoch 11: acc=72.97% loss=0.8067 (20 batches)\n",
      "Thread-3: -> Acc=72.97% (best=72.97% at epoch 11) patience_left=3\n",
      "Thread-6: Train Epoch 10: loss=0.9481 (469 batches)\n",
      "Thread-3: Train Epoch 12: loss=0.7780 (469 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:33:20,790 - WARNING - Error evaluating genome 1e523b29: list index out of range\n",
      "2025-09-17 21:33:20,817 - WARNING - Error evaluating genome a31c0810: Expected more than 1 value per channel when training, got input size torch.Size([1, 21, 1, 1])\n",
      "2025-09-17 21:33:20,829 - WARNING - Error evaluating genome de13fbc9: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-6: Eval  Epoch 10: acc=37.19% loss=1.8929 (20 batches)\n",
      "Thread-6: -> Acc=37.19% (best=50.12% at epoch 7) patience_left=0\n",
      "Thread-6: Early stopping triggered (no significant improvement)\n",
      "Thread-6: Final fitness for 5081142d: 50.12% (best epoch 7)\n",
      "Thread-6: Fitness obtained: 50.12%\n",
      "Thread-6: terminado, liberando slot de entrenamiento.\n",
      "Thread-7: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-7: Evaluating individual 7/10 (ID: 1e523b29)\n",
      "Thread-7: Architecture: 3 conv + 7 fc, opt=rmsprop, lr=0.001\n",
      "Thread-3: Eval  Epoch 12: acc=74.73% loss=0.7723 (20 batches)\n",
      "Thread-3: -> Acc=74.73% (best=74.73% at epoch 12) patience_left=3\n",
      "Thread-3: Final fitness for a2f17087: 74.73% (best epoch 12)\n",
      "Thread-3: Fitness obtained: 74.73%\n",
      "Thread-3: terminado, liberando slot de entrenamiento.\n",
      "Thread-8: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-8: Evaluating individual 8/10 (ID: f56de5ad)\n",
      "Thread-8: Architecture: 1 conv + 1 fc, opt=adamw, lr=0.0001\n",
      "Thread-8: Training/Evaluating model f56de5ad (max 12 epochs interleaved)\n",
      "Thread-7: ERROR evaluating genome 1e523b29: list index out of range\n",
      "Thread-7: Fitness obtained: 0.00%\n",
      "Thread-7: terminado, liberando slot de entrenamiento.\n",
      "Thread-9: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-9: Evaluating individual 9/10 (ID: a31c0810)\n",
      "Thread-9: Architecture: 3 conv + 7 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-9: ERROR evaluating genome a31c0810: Expected more than 1 value per channel when training, got input size torch.Size([1, 21, 1, 1])\n",
      "Thread-9: Fitness obtained: 0.00%\n",
      "Thread-9: terminado, liberando slot de entrenamiento.\n",
      "Thread-10: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-10: Evaluating individual 10/10 (ID: de13fbc9)\n",
      "Thread-10: Architecture: 5 conv + 1 fc, opt=sgd, lr=0.01\n",
      "Thread-10: ERROR evaluating genome de13fbc9: list index out of range\n",
      "Thread-10: Fitness obtained: 0.00%\n",
      "Thread-10: terminado, liberando slot de entrenamiento.\n",
      "Thread-8: Train Epoch 1: loss=0.3193 (469 batches)\n",
      "Thread-8: Eval  Epoch 1: acc=93.79% loss=0.2037 (20 batches)\n",
      "Thread-8: -> Acc=93.79% (best=93.79% at epoch 1) patience_left=3\n",
      "Thread-8: Train Epoch 2: loss=0.1266 (469 batches)\n",
      "Thread-8: Eval  Epoch 2: acc=95.74% loss=0.1433 (20 batches)\n",
      "Thread-8: -> Acc=95.74% (best=95.74% at epoch 2) patience_left=3\n",
      "Thread-8: Train Epoch 3: loss=0.0870 (469 batches)\n",
      "Thread-8: Eval  Epoch 3: acc=96.56% loss=0.1106 (20 batches)\n",
      "Thread-8: -> Acc=96.56% (best=96.56% at epoch 3) patience_left=3\n",
      "Thread-8: Train Epoch 4: loss=0.0648 (469 batches)\n",
      "Thread-8: Eval  Epoch 4: acc=96.80% loss=0.0993 (20 batches)\n",
      "Thread-8: -> Acc=96.80% (best=96.80% at epoch 4) patience_left=3\n",
      "Thread-8: Train Epoch 5: loss=0.0511 (469 batches)\n",
      "Thread-8: Eval  Epoch 5: acc=97.07% loss=0.0852 (20 batches)\n",
      "Thread-8: -> Acc=97.07% (best=97.07% at epoch 5) patience_left=3\n",
      "Thread-8: Train Epoch 6: loss=0.0397 (469 batches)\n",
      "Thread-8: Eval  Epoch 6: acc=97.58% loss=0.0802 (20 batches)\n",
      "Thread-8: -> Acc=97.58% (best=97.58% at epoch 6) patience_left=3\n",
      "Thread-8: Train Epoch 7: loss=0.0321 (469 batches)\n",
      "Thread-8: Eval  Epoch 7: acc=97.50% loss=0.0774 (20 batches)\n",
      "Thread-8: -> Acc=97.50% (best=97.58% at epoch 6) patience_left=2\n",
      "Thread-8: Train Epoch 8: loss=0.0255 (469 batches)\n",
      "Thread-8: Eval  Epoch 8: acc=97.66% loss=0.0730 (20 batches)\n",
      "Thread-8: -> Acc=97.66% (best=97.66% at epoch 8) patience_left=1\n",
      "Thread-8: Train Epoch 9: loss=0.0204 (469 batches)\n",
      "Thread-8: Eval  Epoch 9: acc=97.46% loss=0.0735 (20 batches)\n",
      "Thread-8: -> Acc=97.46% (best=97.66% at epoch 8) patience_left=0\n",
      "Thread-8: Early stopping triggered (no significant improvement)\n",
      "Thread-8: Final fitness for f56de5ad: 97.66% (best epoch 8)\n",
      "Thread-8: Fitness obtained: 97.66%\n",
      "Thread-8: terminado, liberando slot de entrenamiento.\n",
      "\n",
      "GENERATION 1 STATISTICS (Threading completed):\n",
      "   Maximum fitness: 98.32%\n",
      "   Average fitness: 41.91%\n",
      "   Minimum fitness: 0.00%\n",
      "   Standard deviation: 44.04%\n",
      "   Best individual: 681214d0 with 98.32%\n",
      "   Global best individual: 681214d0 with 98.79%\n",
      "   Concurrent training slots used: 2\n",
      "Adaptive mutation rate updated to 0.15 (std_fitness=44.04)\n",
      "\n",
      "Starting selection and reproduction...\n",
      "Selecting 2 elite individuals:\n",
      "   Elite 1: 681214d0 (fitness: 98.32%)\n",
      "   Elite 2: 49270d26 (fitness: 98.28%)\n",
      "Creating 8 new individuals through crossover and mutation...\n",
      "   Created 4 of 8 new individuals...\n",
      "   Created 8 of 8 new individuals...\n",
      "New generation created with 10 individuals\n",
      "   Elite preserved: 2\n",
      "   New individuals: 8\n",
      "\n",
      "Preparing for next generation...\n",
      "\n",
      "================================================================================\n",
      "GENERATION 2\n",
      "================================================================================\n",
      "\n",
      "Evaluating population with threading (Generation 2)...\n",
      "Processing 10 individuals with max 2 concurrent training...\n",
      "Thread-1: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-1: Evaluating individual 1/10 (ID: 681214d0)\n",
      "Thread-1: Architecture: 3 conv + 1 fc, opt=rmsprop, lr=0.001\n",
      "Thread-2: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-2: Evaluating individual 2/10 (ID: 49270d26)\n",
      "Thread-2: Architecture: 2 conv + 6 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-1: Training/Evaluating model 681214d0 (max 12 epochs interleaved)\n",
      "Thread-2: Training/Evaluating model 49270d26 (max 12 epochs interleaved)\n",
      "Thread-1: Train Epoch 1: loss=1.6920 (469 batches)\n",
      "Thread-2: Train Epoch 1: loss=0.5000 (469 batches)\n",
      "Thread-2: Eval  Epoch 1: acc=96.91% loss=0.0990 (20 batches)\n",
      "Thread-1: Eval  Epoch 1: acc=95.78% loss=0.1218 (20 batches)\n",
      "Thread-2: -> Acc=96.91% (best=96.91% at epoch 1) patience_left=3\n",
      "Thread-1: -> Acc=95.78% (best=95.78% at epoch 1) patience_left=3\n",
      "Thread-2: Train Epoch 2: loss=0.0911 (469 batches)\n",
      "Thread-1: Train Epoch 2: loss=0.0803 (469 batches)\n",
      "Thread-2: Eval  Epoch 2: acc=97.27% loss=0.0889 (20 batches)\n",
      "Thread-2: -> Acc=97.27% (best=97.27% at epoch 2) patience_left=3\n",
      "Thread-1: Eval  Epoch 2: acc=96.84% loss=0.0890 (20 batches)\n",
      "Thread-1: -> Acc=96.84% (best=96.84% at epoch 2) patience_left=3\n",
      "Thread-2: Train Epoch 3: loss=0.0640 (469 batches)\n",
      "Thread-1: Train Epoch 3: loss=0.0500 (469 batches)\n",
      "Thread-2: Eval  Epoch 3: acc=97.85% loss=0.0717 (20 batches)\n",
      "Thread-2: -> Acc=97.85% (best=97.85% at epoch 3) patience_left=3\n",
      "Thread-1: Eval  Epoch 3: acc=98.67% loss=0.0400 (20 batches)\n",
      "Thread-1: -> Acc=98.67% (best=98.67% at epoch 3) patience_left=3\n",
      "Thread-1: Train Epoch 4: loss=0.0379 (469 batches)\n",
      "Thread-2: Train Epoch 4: loss=0.0488 (469 batches)\n",
      "Thread-1: Eval  Epoch 4: acc=98.87% loss=0.0351 (20 batches)\n",
      "Thread-1: -> Acc=98.87% (best=98.87% at epoch 4) patience_left=2\n",
      "Thread-2: Eval  Epoch 4: acc=96.99% loss=0.1093 (20 batches)\n",
      "Thread-2: -> Acc=96.99% (best=97.85% at epoch 3) patience_left=2\n",
      "Thread-1: Train Epoch 5: loss=0.0285 (469 batches)\n",
      "Thread-2: Train Epoch 5: loss=0.0406 (469 batches)\n",
      "Thread-2: Eval  Epoch 5: acc=96.21% loss=0.1325 (20 batches)\n",
      "Thread-2: -> Acc=96.21% (best=97.85% at epoch 3) patience_left=1\n",
      "Thread-1: Eval  Epoch 5: acc=98.36% loss=0.0511 (20 batches)\n",
      "Thread-1: -> Acc=98.36% (best=98.87% at epoch 4) patience_left=1\n",
      "Thread-2: Train Epoch 6: loss=0.0339 (469 batches)\n",
      "Thread-1: Train Epoch 6: loss=0.0223 (469 batches)\n",
      "Thread-2: Eval  Epoch 6: acc=97.93% loss=0.0772 (20 batches)\n",
      "Thread-2: -> Acc=97.93% (best=97.93% at epoch 6) patience_left=0\n",
      "Thread-2: Early stopping triggered (no significant improvement)\n",
      "Thread-2: Final fitness for 49270d26: 97.93% (best epoch 6)\n",
      "Thread-2: Fitness obtained: 97.93%\n",
      "Thread-2: terminado, liberando slot de entrenamiento.\n",
      "Thread-3: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-3: Evaluating individual 3/10 (ID: 743fd39a)\n",
      "Thread-3: Architecture: 2 conv + 7 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-1: Eval  Epoch 6: acc=98.67% loss=0.0409 (20 batches)\n",
      "Thread-1: -> Acc=98.67% (best=98.87% at epoch 4) patience_left=0\n",
      "Thread-1: Early stopping triggered (no significant improvement)\n",
      "Thread-1: Final fitness for 681214d0: 98.87% (best epoch 4)\n",
      "Thread-1: Fitness obtained: 98.87%\n",
      "Thread-1: terminado, liberando slot de entrenamiento.\n",
      "Thread-4: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-4: Evaluating individual 4/10 (ID: c972676a)\n",
      "Thread-4: Architecture: 3 conv + 6 fc, opt=rmsprop, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:38:58,614 - WARNING - Error evaluating genome 743fd39a: list index out of range\n",
      "2025-09-17 21:38:58,631 - WARNING - Error evaluating genome c972676a: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-3: ERROR evaluating genome 743fd39a: list index out of range\n",
      "Thread-3: Fitness obtained: 0.00%\n",
      "Thread-3: terminado, liberando slot de entrenamiento.\n",
      "Thread-5: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-5: Evaluating individual 5/10 (ID: 58ca620b)\n",
      "Thread-5: Architecture: 3 conv + 5 fc, opt=rmsprop, lr=0.001\n",
      "Thread-4: ERROR evaluating genome c972676a: list index out of range\n",
      "Thread-4: Fitness obtained: 0.00%\n",
      "Thread-4: terminado, liberando slot de entrenamiento.\n",
      "Thread-6: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-6: Evaluating individual 6/10 (ID: 1366d4f3)\n",
      "Thread-6: Architecture: 1 conv + 7 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-5: Training/Evaluating model 58ca620b (max 12 epochs interleaved)\n",
      "Thread-6: Training/Evaluating model 1366d4f3 (max 12 epochs interleaved)\n",
      "Thread-6: Train Epoch 1: loss=0.5466 (469 batches)\n",
      "Thread-5: Train Epoch 1: loss=5.0111 (469 batches)\n",
      "Thread-6: Eval  Epoch 1: acc=92.50% loss=0.2478 (20 batches)\n",
      "Thread-6: -> Acc=92.50% (best=92.50% at epoch 1) patience_left=3\n",
      "Thread-5: Eval  Epoch 1: acc=91.13% loss=0.3180 (20 batches)\n",
      "Thread-5: -> Acc=91.13% (best=91.13% at epoch 1) patience_left=3\n",
      "Thread-6: Train Epoch 2: loss=0.1575 (469 batches)\n",
      "Thread-5: Train Epoch 2: loss=0.2187 (469 batches)\n",
      "Thread-6: Eval  Epoch 2: acc=94.96% loss=0.1813 (20 batches)\n",
      "Thread-6: -> Acc=94.96% (best=94.96% at epoch 2) patience_left=3\n",
      "Thread-5: Eval  Epoch 2: acc=97.50% loss=0.0976 (20 batches)\n",
      "Thread-5: -> Acc=97.50% (best=97.50% at epoch 2) patience_left=3\n",
      "Thread-6: Train Epoch 3: loss=0.1051 (469 batches)\n",
      "Thread-5: Train Epoch 3: loss=0.1197 (469 batches)\n",
      "Thread-6: Eval  Epoch 3: acc=96.37% loss=0.1263 (20 batches)\n",
      "Thread-6: -> Acc=96.37% (best=96.37% at epoch 3) patience_left=3\n",
      "Thread-5: Eval  Epoch 3: acc=97.07% loss=0.1108 (20 batches)\n",
      "Thread-5: -> Acc=97.07% (best=97.50% at epoch 2) patience_left=2\n",
      "Thread-6: Train Epoch 4: loss=0.0820 (469 batches)\n",
      "Thread-6: Eval  Epoch 4: acc=96.84% loss=0.1193 (20 batches)\n",
      "Thread-6: -> Acc=96.84% (best=96.84% at epoch 4) patience_left=3\n",
      "Thread-5: Train Epoch 4: loss=0.0911 (469 batches)\n",
      "Thread-5: Eval  Epoch 4: acc=97.38% loss=0.1009 (20 batches)\n",
      "Thread-5: -> Acc=97.38% (best=97.50% at epoch 2) patience_left=1\n",
      "Thread-6: Train Epoch 5: loss=0.0648 (469 batches)\n",
      "Thread-6: Eval  Epoch 5: acc=96.56% loss=0.1230 (20 batches)\n",
      "Thread-6: -> Acc=96.56% (best=96.84% at epoch 4) patience_left=2\n",
      "Thread-5: Train Epoch 5: loss=0.0707 (469 batches)\n",
      "Thread-5: Eval  Epoch 5: acc=98.63% loss=0.0572 (20 batches)\n",
      "Thread-5: -> Acc=98.63% (best=98.63% at epoch 5) patience_left=3\n",
      "Thread-6: Train Epoch 6: loss=0.0507 (469 batches)\n",
      "Thread-6: Eval  Epoch 6: acc=97.38% loss=0.1051 (20 batches)\n",
      "Thread-6: -> Acc=97.38% (best=97.38% at epoch 6) patience_left=3\n",
      "Thread-5: Train Epoch 6: loss=0.0646 (469 batches)\n",
      "Thread-6: Train Epoch 7: loss=0.0430 (469 batches)\n",
      "Thread-5: Eval  Epoch 6: acc=98.75% loss=0.0556 (20 batches)\n",
      "Thread-5: -> Acc=98.75% (best=98.75% at epoch 6) patience_left=2\n",
      "Thread-6: Eval  Epoch 7: acc=96.99% loss=0.1246 (20 batches)\n",
      "Thread-6: -> Acc=96.99% (best=97.38% at epoch 6) patience_left=2\n",
      "Thread-5: Train Epoch 7: loss=0.0558 (469 batches)\n",
      "Thread-6: Train Epoch 8: loss=0.0376 (469 batches)\n",
      "Thread-5: Eval  Epoch 7: acc=97.77% loss=0.1086 (20 batches)\n",
      "Thread-5: -> Acc=97.77% (best=98.75% at epoch 6) patience_left=1\n",
      "Thread-6: Eval  Epoch 8: acc=96.84% loss=0.1371 (20 batches)\n",
      "Thread-6: -> Acc=96.84% (best=97.38% at epoch 6) patience_left=1\n",
      "Thread-5: Train Epoch 8: loss=0.0515 (469 batches)\n",
      "Thread-6: Train Epoch 9: loss=0.0306 (469 batches)\n",
      "Thread-5: Eval  Epoch 8: acc=98.87% loss=0.0493 (20 batches)\n",
      "Thread-5: -> Acc=98.87% (best=98.87% at epoch 8) patience_left=3\n",
      "Thread-6: Eval  Epoch 9: acc=93.16% loss=0.2575 (20 batches)\n",
      "Thread-6: -> Acc=93.16% (best=97.38% at epoch 6) patience_left=0\n",
      "Thread-6: Early stopping triggered (no significant improvement)\n",
      "Thread-6: Final fitness for 1366d4f3: 97.38% (best epoch 6)\n",
      "Thread-6: Fitness obtained: 97.38%\n",
      "Thread-6: terminado, liberando slot de entrenamiento.\n",
      "Thread-7: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-7: Evaluating individual 7/10 (ID: 9328bd9f)\n",
      "Thread-7: Architecture: 1 conv + 1 fc, opt=adamw, lr=0.001\n",
      "Thread-7: Training/Evaluating model 9328bd9f (max 12 epochs interleaved)\n",
      "Thread-5: Train Epoch 9: loss=0.0501 (469 batches)\n",
      "Thread-7: Train Epoch 1: loss=1.4561 (469 batches)\n",
      "Thread-5: Eval  Epoch 9: acc=98.52% loss=0.0663 (20 batches)\n",
      "Thread-5: -> Acc=98.52% (best=98.87% at epoch 8) patience_left=2\n",
      "Thread-7: Eval  Epoch 1: acc=86.88% loss=0.4108 (20 batches)\n",
      "Thread-7: -> Acc=86.88% (best=86.88% at epoch 1) patience_left=3\n",
      "Thread-5: Train Epoch 10: loss=0.0421 (469 batches)\n",
      "Thread-7: Train Epoch 2: loss=0.2855 (469 batches)\n",
      "Thread-5: Eval  Epoch 10: acc=98.28% loss=0.0816 (20 batches)\n",
      "Thread-5: -> Acc=98.28% (best=98.87% at epoch 8) patience_left=1\n",
      "Thread-7: Eval  Epoch 2: acc=91.45% loss=0.2835 (20 batches)\n",
      "Thread-7: -> Acc=91.45% (best=91.45% at epoch 2) patience_left=3\n",
      "Thread-5: Train Epoch 11: loss=0.0442 (469 batches)\n",
      "Thread-7: Train Epoch 3: loss=0.2052 (469 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 21:43:30,365 - WARNING - Error evaluating genome 4b144164: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-5: Eval  Epoch 11: acc=97.19% loss=0.1601 (20 batches)\n",
      "Thread-5: -> Acc=97.19% (best=98.87% at epoch 8) patience_left=0\n",
      "Thread-5: Early stopping triggered (no significant improvement)\n",
      "Thread-5: Final fitness for 58ca620b: 98.87% (best epoch 8)\n",
      "Thread-5: Fitness obtained: 98.87%\n",
      "Thread-5: terminado, liberando slot de entrenamiento.\n",
      "Thread-8: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-8: Evaluating individual 8/10 (ID: 4b144164)\n",
      "Thread-8: Architecture: 3 conv + 6 fc, opt=rmsprop, lr=1e-06\n",
      "Thread-8: ERROR evaluating genome 4b144164: list index out of range\n",
      "Thread-8: Fitness obtained: 0.00%\n",
      "Thread-8: terminado, liberando slot de entrenamiento.\n",
      "Thread-9: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-9: Evaluating individual 9/10 (ID: a916dfa8)\n",
      "Thread-9: Architecture: 2 conv + 6 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-9: Training/Evaluating model a916dfa8 (max 12 epochs interleaved)\n",
      "Thread-7: Eval  Epoch 3: acc=93.71% loss=0.2020 (20 batches)\n",
      "Thread-7: -> Acc=93.71% (best=93.71% at epoch 3) patience_left=3\n",
      "Thread-9: Train Epoch 1: loss=1.4343 (469 batches)\n",
      "Thread-7: Train Epoch 4: loss=0.1580 (469 batches)\n",
      "Thread-9: Eval  Epoch 1: acc=51.84% loss=1.3948 (20 batches)\n",
      "Thread-9: -> Acc=51.84% (best=51.84% at epoch 1) patience_left=3\n",
      "Thread-7: Eval  Epoch 4: acc=95.62% loss=0.1385 (20 batches)\n",
      "Thread-7: -> Acc=95.62% (best=95.62% at epoch 4) patience_left=3\n",
      "Thread-9: Train Epoch 2: loss=0.5774 (469 batches)\n",
      "Thread-7: Train Epoch 5: loss=0.1281 (469 batches)\n",
      "Thread-9: Eval  Epoch 2: acc=82.07% loss=0.5628 (20 batches)\n",
      "Thread-9: -> Acc=82.07% (best=82.07% at epoch 2) patience_left=3\n",
      "Thread-7: Eval  Epoch 5: acc=95.31% loss=0.1454 (20 batches)\n",
      "Thread-7: -> Acc=95.31% (best=95.62% at epoch 4) patience_left=2\n",
      "Thread-9: Train Epoch 3: loss=0.2841 (469 batches)\n",
      "Thread-7: Train Epoch 6: loss=0.1102 (469 batches)\n",
      "Thread-9: Eval  Epoch 3: acc=93.01% loss=0.2339 (20 batches)\n",
      "Thread-9: -> Acc=93.01% (best=93.01% at epoch 3) patience_left=3\n",
      "Thread-7: Eval  Epoch 6: acc=96.45% loss=0.1138 (20 batches)\n",
      "Thread-7: -> Acc=96.45% (best=96.45% at epoch 6) patience_left=3\n",
      "Thread-9: Train Epoch 4: loss=0.2150 (469 batches)\n",
      "Thread-7: Train Epoch 7: loss=0.0941 (469 batches)\n",
      "Thread-9: Eval  Epoch 4: acc=93.12% loss=0.2169 (20 batches)\n",
      "Thread-9: -> Acc=93.12% (best=93.12% at epoch 4) patience_left=2\n",
      "Thread-7: Eval  Epoch 7: acc=94.38% loss=0.1775 (20 batches)\n",
      "Thread-7: -> Acc=94.38% (best=96.45% at epoch 6) patience_left=2\n",
      "Thread-9: Train Epoch 5: loss=0.1678 (469 batches)\n",
      "Thread-7: Train Epoch 8: loss=0.0856 (469 batches)\n",
      "Thread-9: Eval  Epoch 5: acc=95.94% loss=0.1175 (20 batches)\n",
      "Thread-9: -> Acc=95.94% (best=95.94% at epoch 5) patience_left=3\n",
      "Thread-7: Eval  Epoch 8: acc=96.52% loss=0.1233 (20 batches)\n",
      "Thread-7: -> Acc=96.52% (best=96.52% at epoch 8) patience_left=1\n",
      "Thread-9: Train Epoch 6: loss=0.1330 (469 batches)\n",
      "Thread-7: Train Epoch 9: loss=0.0813 (469 batches)\n",
      "Thread-9: Eval  Epoch 6: acc=71.76% loss=1.1247 (20 batches)\n",
      "Thread-9: -> Acc=71.76% (best=95.94% at epoch 5) patience_left=2\n",
      "Thread-7: Eval  Epoch 9: acc=96.02% loss=0.1202 (20 batches)\n",
      "Thread-7: -> Acc=96.02% (best=96.52% at epoch 8) patience_left=0\n",
      "Thread-7: Early stopping triggered (no significant improvement)\n",
      "Thread-7: Final fitness for 9328bd9f: 96.52% (best epoch 8)\n",
      "Thread-7: Fitness obtained: 96.52%\n",
      "Thread-7: terminado, liberando slot de entrenamiento.\n",
      "Thread-10: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-10: Evaluating individual 10/10 (ID: 5ee71e70)\n",
      "Thread-10: Architecture: 2 conv + 1 fc, opt=rmsprop, lr=0.0001\n",
      "Thread-10: Training/Evaluating model 5ee71e70 (max 12 epochs interleaved)\n",
      "Thread-9: Train Epoch 7: loss=0.1212 (469 batches)\n",
      "Thread-10: Train Epoch 1: loss=0.3429 (469 batches)\n",
      "Thread-9: Eval  Epoch 7: acc=95.78% loss=0.1557 (20 batches)\n",
      "Thread-9: -> Acc=95.78% (best=95.94% at epoch 5) patience_left=1\n",
      "Thread-10: Eval  Epoch 1: acc=93.63% loss=0.1771 (20 batches)\n",
      "Thread-10: -> Acc=93.63% (best=93.63% at epoch 1) patience_left=3\n",
      "Thread-9: Train Epoch 8: loss=0.1056 (469 batches)\n",
      "Thread-10: Train Epoch 2: loss=0.0641 (469 batches)\n",
      "Thread-9: Eval  Epoch 8: acc=97.70% loss=0.0767 (20 batches)\n",
      "Thread-9: -> Acc=97.70% (best=97.70% at epoch 8) patience_left=3\n",
      "Thread-10: Eval  Epoch 2: acc=97.70% loss=0.0731 (20 batches)\n",
      "Thread-10: -> Acc=97.70% (best=97.70% at epoch 2) patience_left=3\n",
      "Thread-9: Train Epoch 9: loss=0.1000 (469 batches)\n",
      "Thread-9: Eval  Epoch 9: acc=71.64% loss=0.9888 (20 batches)\n",
      "Thread-9: -> Acc=71.64% (best=97.70% at epoch 8) patience_left=2\n",
      "Thread-10: Train Epoch 3: loss=0.0415 (469 batches)\n",
      "Thread-10: Eval  Epoch 3: acc=95.90% loss=0.1247 (20 batches)\n",
      "Thread-10: -> Acc=95.90% (best=97.70% at epoch 2) patience_left=2\n",
      "Thread-9: Train Epoch 10: loss=0.0964 (469 batches)\n",
      "Thread-9: Eval  Epoch 10: acc=96.60% loss=0.1161 (20 batches)\n",
      "Thread-9: -> Acc=96.60% (best=97.70% at epoch 8) patience_left=1\n",
      "Thread-10: Train Epoch 4: loss=0.0302 (469 batches)\n",
      "Thread-10: Eval  Epoch 4: acc=86.80% loss=0.5074 (20 batches)\n",
      "Thread-10: -> Acc=86.80% (best=97.70% at epoch 2) patience_left=1\n",
      "Thread-9: Train Epoch 11: loss=0.0840 (469 batches)\n",
      "Thread-9: Eval  Epoch 11: acc=96.56% loss=0.1059 (20 batches)\n",
      "Thread-9: -> Acc=96.56% (best=97.70% at epoch 8) patience_left=0\n",
      "Thread-9: Early stopping triggered (no significant improvement)\n",
      "Thread-9: Final fitness for a916dfa8: 97.70% (best epoch 8)\n",
      "Thread-9: Fitness obtained: 97.70%\n",
      "Thread-9: terminado, liberando slot de entrenamiento.\n",
      "Thread-10: Train Epoch 5: loss=0.0234 (469 batches)\n",
      "Thread-10: Eval  Epoch 5: acc=98.71% loss=0.0410 (20 batches)\n",
      "Thread-10: -> Acc=98.71% (best=98.71% at epoch 5) patience_left=3\n",
      "Thread-10: Train Epoch 6: loss=0.0177 (469 batches)\n",
      "Thread-10: Eval  Epoch 6: acc=97.58% loss=0.0836 (20 batches)\n",
      "Thread-10: -> Acc=97.58% (best=98.71% at epoch 5) patience_left=2\n",
      "Thread-10: Train Epoch 7: loss=0.0134 (469 batches)\n",
      "Thread-10: Eval  Epoch 7: acc=93.48% loss=0.2228 (20 batches)\n",
      "Thread-10: -> Acc=93.48% (best=98.71% at epoch 5) patience_left=1\n",
      "Thread-10: Train Epoch 8: loss=0.0110 (469 batches)\n",
      "Thread-10: Eval  Epoch 8: acc=95.31% loss=0.1630 (20 batches)\n",
      "Thread-10: -> Acc=95.31% (best=98.71% at epoch 5) patience_left=0\n",
      "Thread-10: Early stopping triggered (no significant improvement)\n",
      "Thread-10: Final fitness for 5ee71e70: 98.71% (best epoch 5)\n",
      "Thread-10: Fitness obtained: 98.71%\n",
      "Thread-10: terminado, liberando slot de entrenamiento.\n",
      "\n",
      "New global best individual found!\n",
      "\n",
      "GENERATION 2 STATISTICS (Threading completed):\n",
      "   Maximum fitness: 98.87%\n",
      "   Average fitness: 68.60%\n",
      "   Minimum fitness: 0.00%\n",
      "   Standard deviation: 44.91%\n",
      "   Best individual: 681214d0 with 98.87%\n",
      "   Global best individual: 681214d0 with 98.87%\n",
      "   Concurrent training slots used: 2\n",
      "Adaptive mutation rate updated to 0.15 (std_fitness=44.91)\n",
      "\n",
      "Starting selection and reproduction...\n",
      "Selecting 2 elite individuals:\n",
      "   Elite 1: 681214d0 (fitness: 98.87%)\n",
      "   Elite 2: 58ca620b (fitness: 98.87%)\n",
      "Creating 8 new individuals through crossover and mutation...\n",
      "   Created 4 of 8 new individuals...\n",
      "   Created 8 of 8 new individuals...\n",
      "New generation created with 10 individuals\n",
      "   Elite preserved: 2\n",
      "   New individuals: 8\n",
      "\n",
      "Preparing for next generation...\n",
      "\n",
      "================================================================================\n",
      "GENERATION 3\n",
      "================================================================================\n",
      "\n",
      "Evaluating population with threading (Generation 3)...\n",
      "Processing 10 individuals with max 2 concurrent training...\n",
      "Thread-1: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-1: Evaluating individual 1/10 (ID: 681214d0)\n",
      "Thread-1: Architecture: 3 conv + 1 fc, opt=rmsprop, lr=0.001\n",
      "Thread-2: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-2: Evaluating individual 2/10 (ID: 58ca620b)\n",
      "Thread-2: Architecture: 3 conv + 5 fc, opt=rmsprop, lr=0.001\n",
      "Thread-1: Training/Evaluating model 681214d0 (max 12 epochs interleaved)\n",
      "Thread-2: Training/Evaluating model 58ca620b (max 12 epochs interleaved)\n",
      "Thread-2: Train Epoch 1: loss=2.4023 (469 batches)\n",
      "Thread-1: Train Epoch 1: loss=1.5502 (469 batches)\n",
      "Thread-2: Eval  Epoch 1: acc=96.29% loss=0.1558 (20 batches)\n",
      "Thread-2: -> Acc=96.29% (best=96.29% at epoch 1) patience_left=3\n",
      "Thread-1: Eval  Epoch 1: acc=95.23% loss=0.1434 (20 batches)\n",
      "Thread-1: -> Acc=95.23% (best=95.23% at epoch 1) patience_left=3\n",
      "Thread-1: Train Epoch 2: loss=0.0807 (469 batches)\n",
      "Thread-2: Train Epoch 2: loss=0.1326 (469 batches)\n",
      "Thread-2: Eval  Epoch 2: acc=96.21% loss=0.1674 (20 batches)\n",
      "Thread-2: -> Acc=96.21% (best=96.29% at epoch 1) patience_left=2\n",
      "Thread-1: Eval  Epoch 2: acc=98.01% loss=0.0540 (20 batches)\n",
      "Thread-1: -> Acc=98.01% (best=98.01% at epoch 2) patience_left=3\n",
      "Thread-1: Train Epoch 3: loss=0.0532 (469 batches)\n",
      "Thread-2: Train Epoch 3: loss=0.1160 (469 batches)\n",
      "Thread-1: Eval  Epoch 3: acc=98.48% loss=0.0418 (20 batches)Thread-2: Eval  Epoch 3: acc=98.28% loss=0.0854 (20 batches)\n",
      "\n",
      "Thread-2: -> Acc=98.28% (best=98.28% at epoch 3) patience_left=3\n",
      "Thread-1: -> Acc=98.48% (best=98.48% at epoch 3) patience_left=3\n",
      "Thread-1: Train Epoch 4: loss=0.0356 (469 batches)\n",
      "Thread-2: Train Epoch 4: loss=0.0803 (469 batches)\n",
      "Thread-2: Eval  Epoch 4: acc=98.12% loss=0.0854 (20 batches)\n",
      "Thread-2: -> Acc=98.12% (best=98.28% at epoch 3) patience_left=2\n",
      "Thread-1: Eval  Epoch 4: acc=98.40% loss=0.0474 (20 batches)\n",
      "Thread-1: -> Acc=98.40% (best=98.48% at epoch 3) patience_left=2\n",
      "Thread-2: Train Epoch 5: loss=0.0684 (469 batches)\n",
      "Thread-1: Train Epoch 5: loss=0.0296 (469 batches)\n",
      "Thread-2: Eval  Epoch 5: acc=97.27% loss=0.1193 (20 batches)\n",
      "Thread-2: -> Acc=97.27% (best=98.28% at epoch 3) patience_left=1\n",
      "Thread-1: Eval  Epoch 5: acc=97.73% loss=0.0764 (20 batches)\n",
      "Thread-1: -> Acc=97.73% (best=98.48% at epoch 3) patience_left=1\n",
      "Thread-2: Train Epoch 6: loss=0.0631 (469 batches)\n",
      "Thread-1: Train Epoch 6: loss=0.0232 (469 batches)\n",
      "Thread-2: Eval  Epoch 6: acc=98.01% loss=0.0948 (20 batches)\n",
      "Thread-2: -> Acc=98.01% (best=98.28% at epoch 3) patience_left=0\n",
      "Thread-2: Early stopping triggered (no significant improvement)\n",
      "Thread-2: Final fitness for 58ca620b: 98.28% (best epoch 3)\n",
      "Thread-2: Fitness obtained: 98.28%\n",
      "Thread-2: terminado, liberando slot de entrenamiento.\n",
      "Thread-3: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-3: Evaluating individual 3/10 (ID: b5d42175)\n",
      "Thread-3: Architecture: 3 conv + 1 fc, opt=rmsprop, lr=0.001\n",
      "Thread-1: Eval  Epoch 6: acc=98.36% loss=0.0517 (20 batches)\n",
      "Thread-1: -> Acc=98.36% (best=98.48% at epoch 3) patience_left=0\n",
      "Thread-1: Early stopping triggered (no significant improvement)\n",
      "Thread-1: Final fitness for 681214d0: 98.48% (best epoch 3)\n",
      "Thread-1: Fitness obtained: 98.48%\n",
      "Thread-1: terminado, liberando slot de entrenamiento.\n",
      "Thread-4: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-4: Evaluating individual 4/10 (ID: f5e53aa7)\n",
      "Thread-4: Architecture: 3 conv + 5 fc, opt=rmsprop, lr=5e-05\n",
      "Thread-3: Training/Evaluating model b5d42175 (max 12 epochs interleaved)\n",
      "Thread-4: Training/Evaluating model f5e53aa7 (max 12 epochs interleaved)\n",
      "Thread-4: Train Epoch 1: loss=0.2155 (469 batches)\n",
      "Thread-3: Train Epoch 1: loss=0.1695 (469 batches)\n",
      "Thread-4: Eval  Epoch 1: acc=95.35% loss=0.1530 (20 batches)\n",
      "Thread-4: -> Acc=95.35% (best=95.35% at epoch 1) patience_left=3\n",
      "Thread-3: Eval  Epoch 1: acc=98.40% loss=0.0515 (20 batches)\n",
      "Thread-3: -> Acc=98.40% (best=98.40% at epoch 1) patience_left=3\n",
      "Thread-4: Train Epoch 2: loss=0.0545 (469 batches)\n",
      "Thread-3: Train Epoch 2: loss=0.0458 (469 batches)\n",
      "Thread-4: Eval  Epoch 2: acc=94.22% loss=0.1918 (20 batches)\n",
      "Thread-4: -> Acc=94.22% (best=95.35% at epoch 1) patience_left=2\n",
      "Thread-3: Eval  Epoch 2: acc=98.44% loss=0.0525 (20 batches)\n",
      "Thread-3: -> Acc=98.44% (best=98.44% at epoch 2) patience_left=2\n",
      "Thread-4: Train Epoch 3: loss=0.0401 (469 batches)\n",
      "Thread-3: Train Epoch 3: loss=0.0333 (469 batches)\n",
      "Thread-3: Eval  Epoch 3: acc=98.55% loss=0.0391 (20 batches)\n",
      "Thread-3: -> Acc=98.55% (best=98.55% at epoch 3) patience_left=1\n",
      "Thread-4: Eval  Epoch 3: acc=98.32% loss=0.0508 (20 batches)\n",
      "Thread-4: -> Acc=98.32% (best=98.32% at epoch 3) patience_left=3\n",
      "Thread-3: Train Epoch 4: loss=0.0260 (469 batches)\n",
      "Thread-4: Train Epoch 4: loss=0.0302 (469 batches)\n",
      "Thread-4: Eval  Epoch 4: acc=98.36% loss=0.0487 (20 batches)\n",
      "Thread-4: -> Acc=98.36% (best=98.36% at epoch 4) patience_left=2\n",
      "Thread-3: Eval  Epoch 4: acc=96.88% loss=0.0909 (20 batches)\n",
      "Thread-3: -> Acc=96.88% (best=98.55% at epoch 3) patience_left=0\n",
      "Thread-3: Early stopping triggered (no significant improvement)\n",
      "Thread-3: Final fitness for b5d42175: 98.55% (best epoch 3)\n",
      "Thread-3: Fitness obtained: 98.55%\n",
      "Thread-3: terminado, liberando slot de entrenamiento.\n",
      "Thread-5: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-5: Evaluating individual 5/10 (ID: 55b421f0)\n",
      "Thread-5: Architecture: 3 conv + 5 fc, opt=rmsprop, lr=0.001\n",
      "Thread-5: Training/Evaluating model 55b421f0 (max 12 epochs interleaved)\n",
      "Thread-4: Train Epoch 5: loss=0.0229 (469 batches)\n",
      "Thread-5: Train Epoch 1: loss=1.7526 (469 batches)\n",
      "Thread-4: Eval  Epoch 5: acc=98.55% loss=0.0498 (20 batches)\n",
      "Thread-4: -> Acc=98.55% (best=98.55% at epoch 5) patience_left=3\n",
      "Thread-5: Eval  Epoch 1: acc=86.09% loss=0.7766 (20 batches)\n",
      "Thread-5: -> Acc=86.09% (best=86.09% at epoch 1) patience_left=3\n",
      "Thread-4: Train Epoch 6: loss=0.0185 (469 batches)\n",
      "Thread-5: Train Epoch 2: loss=0.1720 (469 batches)\n",
      "Thread-4: Eval  Epoch 6: acc=98.55% loss=0.0482 (20 batches)\n",
      "Thread-4: -> Acc=98.55% (best=98.55% at epoch 5) patience_left=2\n",
      "Thread-5: Eval  Epoch 2: acc=97.30% loss=0.0994 (20 batches)\n",
      "Thread-5: -> Acc=97.30% (best=97.30% at epoch 2) patience_left=3\n",
      "Thread-4: Train Epoch 7: loss=0.0157 (469 batches)\n",
      "Thread-5: Train Epoch 3: loss=0.1016 (469 batches)\n",
      "Thread-4: Eval  Epoch 7: acc=98.36% loss=0.0641 (20 batches)\n",
      "Thread-4: -> Acc=98.36% (best=98.55% at epoch 5) patience_left=1\n",
      "Thread-5: Eval  Epoch 3: acc=97.38% loss=0.1161 (20 batches)\n",
      "Thread-5: -> Acc=97.38% (best=97.38% at epoch 3) patience_left=2\n",
      "Thread-5: Train Epoch 4: loss=0.0959 (469 batches)\n",
      "Thread-4: Train Epoch 8: loss=0.0142 (469 batches)\n",
      "Thread-5: Eval  Epoch 4: acc=97.46% loss=0.1090 (20 batches)Thread-4: Eval  Epoch 8: acc=98.52% loss=0.0620 (20 batches)\n",
      "Thread-4: -> Acc=98.52% (best=98.55% at epoch 5) patience_left=0\n",
      "Thread-4: Early stopping triggered (no significant improvement)\n",
      "Thread-4: Final fitness for f5e53aa7: 98.55% (best epoch 5)\n",
      "\n",
      "Thread-5: -> Acc=97.46% (best=97.46% at epoch 4) patience_left=1\n",
      "Thread-4: Fitness obtained: 98.55%\n",
      "Thread-4: terminado, liberando slot de entrenamiento.\n",
      "Thread-6: iniciando evaluación (máx 2 concurrentes)...\n",
      "Thread-6: Evaluating individual 6/10 (ID: aa43ea2a)\n",
      "Thread-6: Architecture: 2 conv + 5 fc, opt=rmsprop, lr=0.001\n",
      "Thread-6: Training/Evaluating model aa43ea2a (max 12 epochs interleaved)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m neuroevolution \u001b[38;5;241m=\u001b[39m HybridNeuroevolution(CONFIG, train_loader, test_loader)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Execute evolution process\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m best_genome \u001b[38;5;241m=\u001b[39m \u001b[43mneuroevolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     49\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[7], line 284\u001b[0m, in \u001b[0;36mHybridNeuroevolution.evolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGENERATION \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_convergence():\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 169\u001b[0m, in \u001b[0;36mHybridNeuroevolution.evaluate_population\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Wait for all threads to complete\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[1;32m--> 169\u001b[0m     \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Calculate generation statistics\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fitness_scores:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACIÓN DE DATASET - MODIFICAR AQUÍ\n",
    "# ==========================================\n",
    "\n",
    "# Para cambiar el dataset, modifica la línea correspondiente y ejecuta esta celda:\n",
    "\n",
    "# Opción 1: Usar MNIST (28x28, grayscale, 10 classes)\n",
    "# CONFIG['dataset'] = 'MNIST'\n",
    "\n",
    "# Opción 2: Usar CIFAR-10 (32x32, RGB, 10 classes) - RECOMENDADO para mayor challenge\n",
    "# CONFIG['dataset'] = 'CIFAR10'\n",
    "\n",
    "# Opción 3: Usar dataset personalizado\n",
    "# CONFIG['dataset'] = 'CUSTOM'\n",
    "# CONFIG['dataset_path'] = r'E:\\Neuroevolution\\data\\phd_data'  # Ajustar ruta según tu dataset\n",
    "\n",
    "# ==========================================\n",
    "# OTRAS CONFIGURACIONES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# Reconfigurar el dataset con los nuevos parámetros\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "print(\"Current configuration:\")\n",
    "print(f\"   Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"   Image size: {CONFIG['px_h']}x{CONFIG['px_w']}x{CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Recargar el dataset con la nueva configuración\n",
    "print(f\"\\nReloading dataset with new configuration...\")\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Create system instance\n",
    "neuroevolution = HybridNeuroevolution(CONFIG, train_loader, test_loader)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nProcess completed at {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   • Errors in model architecture\")\n",
    "        print(f\"   • Memory problems (GPU/RAM)\")\n",
    "        print(f\"   • Invalid hyperparameter configurations\")\n",
    "        print(f\"   • Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"        BEST EVOLVED ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Convolutional Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS:\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv{i+1}: {filters} filters, kernel {kernel}x{kernel}, activation {activation}\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (classes)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.4f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(best_genome['activations'])}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Convolutional layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   TARGET: OK Fitness objective REACHED ({best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   TARGET: ERROR Fitness objective NOT reached ({best_genome['fitness']:.2f}% < {config['fitness_threshold']}%)\")\n",
    "    \n",
    "    print(f\"   TIME: Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'config_used': config,\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\nHYBRID NEUROEVOLUTION COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2964f-0098-4c42-bf72-e66c4ca0ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración de Threading en Hybrid Neuroevolution\n",
    "\n",
    "Este notebook ha sido modificado para implementar **entrenamiento concurrente** de modelos usando threading, similar al patrón mostrado en `threading_demo_notebook.ipynb`.\n",
    "\n",
    "## Principales modificaciones implementadas:\n",
    "\n",
    "### 1. **Threading Support en HybridNeuroevolution**\n",
    "- Agregado soporte para entrenamiento concurrente de múltiples modelos\n",
    "- Máximo de 2 modelos entrenando simultáneamente (configurable)\n",
    "- Thread-safety usando semáforos y locks\n",
    "\n",
    "### 2. **Nuevos componentes de threading**\n",
    "- `training_semaphore`: Limita el número de entrenamientos concurrentes\n",
    "- `fitness_lock`: Protege el acceso a variables compartidas\n",
    "- `_worker_evaluate_genome()`: Función worker para evaluación en hilos separados\n",
    "- Threading-aware logging con identificadores de thread\n",
    "\n",
    "### 3. **Método evaluate_population() rediseñado**\n",
    "- Reemplaza evaluación secuencial con evaluación concurrente\n",
    "- Crea threads para cada genoma en la población\n",
    "- Sincronización segura de resultados de fitness\n",
    "- Manejo de errores por thread individual\n",
    "\n",
    "### 4. **Beneficios del threading**\n",
    "- **Paralelización**: Múltiples modelos entrenan simultáneamente\n",
    "- **Eficiencia**: Mejor uso de recursos computacionales\n",
    "- **Escalabilidad**: Fácil ajuste del número de threads concurrentes\n",
    "- **Robustez**: Manejo seguro de estado compartido\n",
    "\n",
    "### 5. **Configuración del threading**\n",
    "```python\n",
    "self.max_concurrent_training = 2  # Máximo 2 modelos concurrentes\n",
    "self.training_semaphore = threading.Semaphore(2)\n",
    "self.fitness_lock = threading.Lock()\n",
    "```\n",
    "\n",
    "El patrón implementado es idéntico al del notebook de demostración, adaptado para el contexto de neuroevolución híbrida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ceb16-25cf-428c-bd49-41db5c262038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del sistema de threading con población pequeña\n",
    "print(\"=\"*60)\n",
    "print(\"PRUEBA DE THREADING CON POBLACIÓN PEQUEÑA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuración reducida para prueba rápida\n",
    "TEST_CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset': 'MNIST',\n",
    "    'num_classes': 10,\n",
    "    'num_channels': 1,\n",
    "    'px_h': 28,\n",
    "    'px_w': 28,\n",
    "    \n",
    "    # Architecture constraints\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 2,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 1,\n",
    "    'min_filters': 8,\n",
    "    'max_filters': 16,\n",
    "    'min_fc_nodes': 32,\n",
    "    'max_fc_nodes': 64,\n",
    "    \n",
    "    # Evolution parameters\n",
    "    'population_size': 4,  # Población pequeña para prueba\n",
    "    'max_generations': 2,  # Solo 2 generaciones\n",
    "    'fitness_threshold': 85.0,\n",
    "    'elite_percentage': 0.5,\n",
    "    'crossover_rate': 0.8,\n",
    "    \n",
    "    # Mutation parameters (adaptive)\n",
    "    'base_mutation_rate': 0.2,\n",
    "    'current_mutation_rate': 0.2,\n",
    "    'mutation_rate_min': 0.1,\n",
    "    'mutation_rate_max': 0.6,\n",
    "    \n",
    "    # Training parameters (reducidos para prueba rápida)\n",
    "    'num_epochs': 2,  # Solo 2 épocas por modelo\n",
    "    'early_stopping_patience': 50,\n",
    "    'epoch_patience': 5,\n",
    "    'improvement_threshold': 0.5\n",
    "}\n",
    "\n",
    "# Crear datasets de prueba (tamaño reducido)\n",
    "print(\"Creando datasets de prueba...\")\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "# Usar solo un subconjunto para prueba rápida\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(1000))  # Solo 1000 muestras\n",
    "test_subset = torch.utils.data.Subset(test_dataset, range(200))     # Solo 200 muestras\n",
    "\n",
    "test_train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "test_test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Dataset de prueba creado: {len(train_subset)} train, {len(test_subset)} test\")\n",
    "\n",
    "# Crear instancia del algoritmo evolutivo con threading\n",
    "print(\"Iniciando algoritmo evolutivo con threading...\")\n",
    "test_neuroevolution = HybridNeuroevolution(TEST_CONFIG, test_train_loader, test_test_loader)\n",
    "\n",
    "# Ejecutar evolución de prueba\n",
    "print(\"Ejecutando evolución de prueba...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "best_test_genome = test_neuroevolution.evolve()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_test = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PRUEBA DE THREADING COMPLETADA\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Tiempo de ejecución: {execution_time_test:.2f} segundos\")\n",
    "print(f\"Mejor individuo encontrado:\")\n",
    "print(f\"   ID: {best_test_genome['id']}\")\n",
    "print(f\"   Fitness: {best_test_genome['fitness']:.2f}%\")\n",
    "print(f\"   Generaciones procesadas: {test_neuroevolution.generation + 1}\")\n",
    "print(f\"   Threading slots concurrentes: {test_neuroevolution.max_concurrent_training}\")\n",
    "print(f\"\\nLa prueba demuestra que el threading funciona correctamente!\")\n",
    "print(f\"Se entrenaron máximo {test_neuroevolution.max_concurrent_training} modelos concurrentemente.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Synchronous Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements the complete hybrid neuroevolution process synchronously, without the need for databases or external Kafka services. The system combines genetic algorithms with convolutional neural networks to evolve optimal architectures.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **Synchronous processing**: Complete workflow executed in a single session\n",
    "- **Configurable dataset**: Supports MNIST by default or custom dataset\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of CNN architectures\n",
    "2. Evaluate fitness of each individual\n",
    "3. Select best architectures (top 50%)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50120a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dependency installation for Hybrid Neuroevolution...\n",
      "============================================================\n",
      "Installing torch>=2.0.0...\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torch>=2.0.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.4.0->torch>=2.0.0) (68.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK torch>=2.0.0 installed correctly\n",
      "Installing torchvision>=0.15.0...\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /home/jovyan/.local/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.15.0) (1.24.4)\n",
      "Requirement already satisfied: torch==2.8.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torchvision>=0.15.0) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision>=0.15.0) (10.0.0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jovyan/.local/lib/python3.11/site-packages (from torch==2.8.0->torchvision>=0.15.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.4.0->torch==2.8.0->torchvision>=0.15.0) (68.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision>=0.15.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.8.0->torchvision>=0.15.0) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK torchvision>=0.15.0 installed correctly\n",
      "Installing numpy>=1.21.0...\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (1.24.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK numpy>=1.21.0 installed correctly\n",
      "Installing matplotlib>=3.5.0...\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/conda/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.5.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK matplotlib>=3.5.0 installed correctly\n",
      "Installing seaborn>=0.11.0...\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from seaborn>=0.11.0) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.11/site-packages (from seaborn>=0.11.0) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.11/site-packages (from seaborn>=0.11.0) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25->seaborn>=0.11.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=0.25->seaborn>=0.11.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn>=0.11.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK seaborn>=0.11.0 installed correctly\n",
      "Installing tqdm>=4.64.0...\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.11/site-packages (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK tqdm>=4.64.0 installed correctly\n",
      "Installing jupyter>=1.0.0...\n",
      "Requirement already satisfied: jupyter>=1.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.11/site-packages (from jupyter>=1.0.0) (6.5.4)\n",
      "Requirement already satisfied: jupyter-console in /home/jovyan/.local/lib/python3.11/site-packages (from jupyter>=1.0.0) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.11/site-packages (from jupyter>=1.0.0) (7.6.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.11/site-packages (from jupyter>=1.0.0) (6.23.3)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (from jupyter>=1.0.0) (8.0.6)\n",
      "Requirement already satisfied: jupyterlab in /opt/conda/lib/python3.11/site-packages (from jupyter>=1.0.0) (4.0.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (8.14.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (8.3.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (5.3.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (1.5.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (6.3.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0) (3.0.7)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from jupyter-console->jupyter>=1.0.0) (3.0.38)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.11/site-packages (from jupyter-console->jupyter>=1.0.0) (2.15.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0) (2.0.2)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /opt/conda/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0) (3.1.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0) (2.2.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0) (2.7.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0) (2.23.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /opt/conda/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0) (0.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (3.0.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (5.9.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0) (21.3.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0) (0.17.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from async-lru>=1.0.0->jupyterlab->jupyter>=1.0.0) (4.14.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter>=1.0.0) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter>=1.0.0) (0.5.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter>=1.0.0) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter>=1.0.0) (3.8.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (3.7.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (0.4.4)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (7.3.1)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (1.6.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (2.12.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.17.3 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (4.17.3)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (2.31.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0) (2.17.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter>=1.0.0) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.11/site-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->notebook->jupyter>=1.0.0) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0) (2.3.2.post1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (0.19.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (6.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (2023.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0) (1.15.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0) (2.21)\n",
      "Requirement already satisfied: fqdn in /home/jovyan/.local/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/jovyan/.local/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (2.0)\n",
      "Requirement already satisfied: uri-template in /home/jovyan/.local/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/jovyan/.local/lib/python3.11/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (24.11.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/jovyan/.local/lib/python3.11/site-packages (from isoduration->jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/jovyan/.local/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter>=1.0.0) (2.9.0.20250809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK jupyter>=1.0.0 installed correctly\n",
      "Installing ipywidgets>=8.0.0...\n",
      "Requirement already satisfied: ipywidgets>=8.0.0 in /opt/conda/lib/python3.11/site-packages (8.0.6)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets>=8.0.0) (6.23.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets>=8.0.0) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets>=8.0.0) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/conda/lib/python3.11/site-packages (from ipywidgets>=8.0.0) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/conda/lib/python3.11/site-packages (from ipywidgets>=8.0.0) (3.0.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (8.3.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (5.3.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (1.5.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0) (6.3.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.0) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=8.0.0) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets>=8.0.0) (3.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets>=8.0.0) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets>=8.0.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK ipywidgets>=8.0.0 installed correctly\n",
      "\n",
      "All dependencies have been verified/installed\n",
      "Restart the kernel if this is the first time installing torch\n",
      "============================================================\n",
      "\n",
      "PyTorch 2.8.0+cu128 installed correctly\n",
      "CUDA available: Yes\n",
      "GPU detected: NVIDIA A100-PCIE-40GB MIG 7g.40gb\n",
      "GPU memory: 39 GB\n"
     ]
    }
   ],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865869c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device configured: cuda\n",
      "PyTorch version: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device configured: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded (adaptive mutation enabled):\n",
      "   Selected dataset: CIFAR10\n",
      "   population_size: 30\n",
      "   max_generations: 30\n",
      "   fitness_threshold: 99.9\n",
      "   base_mutation_rate: 0.35\n",
      "   mutation_rate_min: 0.1\n",
      "   mutation_rate_max: 0.8\n",
      "   current_mutation_rate: 0.35\n",
      "   crossover_rate: 0.99\n",
      "   elite_percentage: 0.2\n",
      "   dataset: CIFAR10\n",
      "   num_channels: 3\n",
      "   px_h: 32\n",
      "   px_w: 32\n",
      "   num_classes: 10\n",
      "   batch_size: 512\n",
      "   test_split: 0.3\n",
      "   num_epochs: 30\n",
      "   learning_rate: 0.001\n",
      "   early_stopping_patience: 1000\n",
      "   epoch_patience: 4\n",
      "   improvement_threshold: 0.2\n",
      "   min_conv_layers: 1\n",
      "   max_conv_layers: 7\n",
      "   min_fc_layers: 1\n",
      "   max_fc_layers: 7\n",
      "   min_filters: 2\n",
      "   max_filters: 256\n",
      "   min_fc_nodes: 128\n",
      "   max_fc_nodes: 2048\n",
      "   dataset_path: None\n",
      "\n",
      "Available activation functions: ['relu', 'leaky_relu', 'tanh', 'sigmoid', 'selu']\n",
      "Available optimizers: ['adam', 'adamw', 'sgd', 'rmsprop']\n",
      "Available datasets: ['MNIST', 'CIFAR10', 'CUSTOM']\n"
     ]
    }
   ],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 30,            # Population size\n",
    "    'max_generations': 30,            # Maximum number of generations\n",
    "    'fitness_threshold': 99.9,        # Target fitness (% accuracy)\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.35,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.35,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection\n",
    "    'dataset': 'CIFAR10',             # Options: 'MNIST', 'CIFAR10', 'CUSTOM'\n",
    "\n",
    "    # Dataset parameters (auto-configured based on dataset)\n",
    "    'num_channels': 3,                # Input channels (1=grayscale, 3=RGB)\n",
    "    'px_h': 32,                       # Image height\n",
    "    'px_w': 32,                       # Image width\n",
    "    'num_classes': 10,                # Number of classes\n",
    "    'batch_size': 512,                # Batch size\n",
    "    'test_split': 0.30,               # Validation percentage (for CUSTOM)\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 30,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.001,            # Base learning rate (used only if genome doesn't override)\n",
    "    'early_stopping_patience': 1000,  # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping (new)\n",
    "    'epoch_patience': 4,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.2,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Allowed architecture range\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 7,\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 7,\n",
    "    'min_filters': 2,\n",
    "    'max_filters': 256,\n",
    "    'min_fc_nodes': 128,\n",
    "    'max_fc_nodes': 2048,\n",
    "\n",
    "    # Custom dataset configuration (only used if dataset='CUSTOM')\n",
    "    'dataset_path': None,             # Custom dataset path\n",
    "}\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIGS = {\n",
    "    'MNIST': {\n",
    "        'num_channels': 1,\n",
    "        'px_h': 28,\n",
    "        'px_w': 28,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.1307,), 'std': (0.3081,)}\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'num_channels': 3,\n",
    "        'px_h': 32,\n",
    "        'px_w': 32,\n",
    "        'num_classes': 10,\n",
    "        'normalization': {'mean': (0.4914, 0.4822, 0.4465), 'std': (0.2023, 0.1994, 0.2010)}\n",
    "    },\n",
    "    'CUSTOM': {\n",
    "        'num_channels': 1,  # Default, will be overridden\n",
    "        'px_h': 28,         # Default, will be overridden\n",
    "        'px_w': 28,         # Default, will be overridden\n",
    "        'num_classes': 10,  # Default, will be overridden\n",
    "        'normalization': {'mean': (0.5,), 'std': (0.5,)}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Auto-configure based on selected dataset\n",
    "def configure_dataset(config, dataset_name):\n",
    "    \"\"\"Auto-configures dataset parameters based on selected dataset.\"\"\"\n",
    "    if dataset_name in DATASET_CONFIGS:\n",
    "        dataset_config = DATASET_CONFIGS[dataset_name]\n",
    "        config['num_channels'] = dataset_config['num_channels']\n",
    "        config['px_h'] = dataset_config['px_h']\n",
    "        config['px_w'] = dataset_config['px_w']\n",
    "        config['num_classes'] = dataset_config['num_classes']\n",
    "        config['_normalization'] = dataset_config['normalization']\n",
    "    return config\n",
    "\n",
    "# Configure the selected dataset\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled):\")\n",
    "print(f\"   Selected dataset: {CONFIG['dataset']}\")\n",
    "for key, value in CONFIG.items():\n",
    "    if not key.startswith('_'):  # Hide internal config\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"Available datasets: {list(DATASET_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### Información sobre Datasets Disponibles\n",
    "\n",
    "**MNIST**: \n",
    "- Dígitos escritos a mano (0-9)\n",
    "- Imágenes en escala de grises (1 canal)\n",
    "- Tamaño: 28x28 píxeles\n",
    "- Dificultad: **Fácil** - Ideal para pruebas rápidas\n",
    "- Fitness objetivo recomendado: >95%\n",
    "\n",
    "**CIFAR-10**: \n",
    "- Objetos del mundo real (aviones, coches, pájaros, etc.)\n",
    "- Imágenes en color (3 canales RGB)\n",
    "- Tamaño: 32x32 píxeles\n",
    "- Dificultad: **Media-Alta** - Más realista y desafiante\n",
    "- Fitness objetivo recomendado: >80%\n",
    "- Clases: plane, car, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "\n",
    "**CUSTOM**: \n",
    "- Tu propio dataset\n",
    "- Configuración manual requerida\n",
    "- Estructura de carpetas por clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "CIFAR-10 dataset loaded:\n",
      "   Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "   Training samples: 50000\n",
      "   Test samples: 10000\n",
      "\n",
      "Dataset loaded successfully:\n",
      "   Batch shape: torch.Size([512, 3, 32, 32])\n",
      "   Data type: torch.float32\n",
      "   Device: cpu\n",
      "   Value range: [-2.429, 2.754]\n",
      "   CIFAR-10 classes: ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "   Labels in this batch: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_type = config['dataset']\n",
    "    \n",
    "    if dataset_type == 'CUSTOM' and config['dataset_path']:\n",
    "        print(f\"Loading custom dataset from: {config['dataset_path']}\")\n",
    "        \n",
    "        # Transformations for custom dataset\n",
    "        if config['num_channels'] == 1:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        else:\n",
    "            normalize = transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "        \n",
    "        # Load dataset from folders organized by class\n",
    "        full_dataset = datasets.ImageFolder(root=config['dataset_path'], transform=transform)\n",
    "        \n",
    "        # Split into train and test\n",
    "        train_size = int((1 - config['test_split']) * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "        \n",
    "        print(f\"Custom dataset loaded:\")\n",
    "        print(f\"   Classes found: {len(full_dataset.classes)}\")\n",
    "        print(f\"   Total samples: {len(full_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'MNIST':\n",
    "        print(\"Loading MNIST dataset...\")\n",
    "        \n",
    "        # Transformations for MNIST\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((config['px_h'], config['px_w'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load MNIST\n",
    "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        \n",
    "        print(f\"MNIST dataset loaded:\")\n",
    "        print(f\"   Classes: {len(train_dataset.classes)}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    elif dataset_type == 'CIFAR10':\n",
    "        print(\"Loading CIFAR-10 dataset...\")\n",
    "        \n",
    "        # Transformations for CIFAR-10\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),  # Data augmentation for training\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(config['_normalization']['mean'], config['_normalization']['std'])\n",
    "        ])\n",
    "        \n",
    "        # Load CIFAR-10\n",
    "        train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "        test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "        \n",
    "        print(f\"CIFAR-10 dataset loaded:\")\n",
    "        print(f\"   Classes: {train_dataset.classes}\")\n",
    "        print(f\"   Training samples: {len(train_dataset)}\")\n",
    "        print(f\"   Test samples: {len(test_dataset)}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_type}' not supported. Available: MNIST, CIFAR10, CUSTOM\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load the dataset\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Get a sample to verify dimensions\n",
    "sample_batch = next(iter(train_loader))\n",
    "sample_data, sample_labels = sample_batch\n",
    "print(f\"\\nDataset loaded successfully:\")\n",
    "print(f\"   Batch shape: {sample_data.shape}\")\n",
    "print(f\"   Data type: {sample_data.dtype}\")\n",
    "print(f\"   Device: {sample_data.device}\")\n",
    "print(f\"   Value range: [{sample_data.min():.3f}, {sample_data.max():.3f}]\")\n",
    "\n",
    "# Show some class information for CIFAR-10\n",
    "if CONFIG['dataset'] == 'CIFAR10':\n",
    "    cifar10_classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    print(f\"   CIFAR-10 classes: {cifar10_classes}\")\n",
    "    unique_labels = torch.unique(sample_labels)\n",
    "    print(f\"   Labels in this batch: {unique_labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvolvableCNN class defined correctly\n"
     ]
    }
   ],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable CNN class that can be dynamically configured\n",
    "    according to genome parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        \n",
    "        # Build convolutional layers\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds convolutional layers according to genome.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = self.config['num_channels']\n",
    "        \n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            out_channels = self.genome['filters'][i]\n",
    "            kernel_size = self.genome['kernel_sizes'][i]\n",
    "            \n",
    "            # Convolutional layer\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Batch normalization\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            # Activation function\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # Max pooling (except in last layer)\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                layers.append(nn.MaxPool2d(2, 2))\n",
    "            else:\n",
    "                layers.append(nn.MaxPool2d(2, 1))  # Stride 1 in last layer\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"Calculates output size after convolutional layers.\"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['px_h'], self.config['px_w'])\n",
    "        \n",
    "        # Pass through convolutional layers\n",
    "        x = dummy_input\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten and get size\n",
    "        return x.view(-1).shape[0]\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        \n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation except on last layer\n",
    "            if i < len(self.fc_layers) - 1 and not isinstance(layer, nn.Dropout):\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be19766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic functions updated for adaptive mutation\n"
     ]
    }
   ],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"Creates a random genome within specified ranges.\"\"\"\n",
    "    # Number of layers\n",
    "    num_conv_layers = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "    num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "    # Filters for each convolutional layer\n",
    "    filters = [random.randint(config['min_filters'], config['max_filters']) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Kernel sizes\n",
    "    kernel_sizes = [random.choice([3, 5, 7]) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Nodes in fully connected layers\n",
    "    fc_nodes = [random.randint(config['min_fc_nodes'], config['max_fc_nodes']) for _ in range(num_fc_layers)]\n",
    "\n",
    "    # Activation functions for each layer\n",
    "    activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "    # Other parameters\n",
    "    dropout_rate = random.uniform(0.1, 0.5)\n",
    "    learning_rate = random.choice([0.001, 0.0001, 0.01, 0.005])\n",
    "    optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    genome = {\n",
    "        'num_conv_layers': num_conv_layers,\n",
    "        'num_fc_layers': num_fc_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'fc_nodes': fc_nodes,\n",
    "        'activations': activations,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "    return genome\n",
    "\n",
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"Applies mutation to a genome using adaptive mutation rate.\"\"\"\n",
    "    mutated_genome = copy.deepcopy(genome)\n",
    "    mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "    # Mutate number of convolutional layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "        num_conv = mutated_genome['num_conv_layers']\n",
    "        mutated_genome['filters'] = mutated_genome['filters'][:num_conv]\n",
    "        mutated_genome['kernel_sizes'] = mutated_genome['kernel_sizes'][:num_conv]\n",
    "        while len(mutated_genome['filters']) < num_conv:\n",
    "            mutated_genome['filters'].append(random.randint(config['min_filters'], config['max_filters']))\n",
    "        while len(mutated_genome['kernel_sizes']) < num_conv:\n",
    "            mutated_genome['kernel_sizes'].append(random.choice([1, 3, 5, 7]))\n",
    "\n",
    "    # Mutate filters\n",
    "    for i in range(len(mutated_genome['filters'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "    # Mutate kernel sizes\n",
    "    for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['kernel_sizes'][i] = random.choice([1, 3, 5, 7])\n",
    "\n",
    "    # Mutate number of FC layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "        num_fc = mutated_genome['num_fc_layers']\n",
    "        mutated_genome['fc_nodes'] = mutated_genome['fc_nodes'][:num_fc]\n",
    "        while len(mutated_genome['fc_nodes']) < num_fc:\n",
    "            mutated_genome['fc_nodes'].append(random.randint(config['min_fc_nodes'], config['max_fc_nodes']))\n",
    "\n",
    "    # Mutate FC nodes\n",
    "    for i in range(len(mutated_genome['fc_nodes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "    # Mutate activation functions\n",
    "    for i in range(len(mutated_genome['activations'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['dropout_rate'] = random.uniform(0.1, 0.8)\n",
    "\n",
    "    # Mutate learning rate\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['learning_rate'] = random.choice([0.001, 0.0001, 0.01, 0.005, 0.000001, 0.05, 0.00005, 0.0005])\n",
    "\n",
    "    # Mutate optimizer\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    mutated_genome['fitness'] = 0.0\n",
    "    return mutated_genome\n",
    "\n",
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Performs crossover between two genomes.\"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "\n",
    "    # Crossover scalar parameters\n",
    "    for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer']:\n",
    "        if random.random() < 0.5:\n",
    "            child1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "    # Crossover lists (random cut point)\n",
    "    for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "        if random.random() < 0.5:\n",
    "            list1 = child1[list_key]\n",
    "            list2 = child2[list_key]\n",
    "            if len(list1) > 1 and len(list2) > 1:\n",
    "                point1 = random.randint(1, len(list1) - 1)\n",
    "                point2 = random.randint(1, len(list2) - 1)\n",
    "                child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    return child1, child2\n",
    "\n",
    "print(\"Genetic functions updated for adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridNeuroevolution class updated with adaptive mutation and interleaved early stopping\n"
     ]
    }
   ],
   "source": [
    "class HybridNeuroevolution:\n",
    "    \"\"\"Main class that implements hybrid neuroevolution with adaptive mutation & epoch interleaved eval.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict, train_loader: DataLoader, test_loader: DataLoader):\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_individual = None\n",
    "        self.fitness_history = []\n",
    "        self.generation_stats = []\n",
    "\n",
    "    def initialize_population(self):\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "\n",
    "    def _train_one_epoch(self, model, optimizer, criterion, genome_id: str, epoch: int):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        print(f\"USING DEVICE {device}\")\n",
    "        model.to(device)\n",
    "        max_batches = min(len(self.train_loader), self.config['early_stopping_patience'])\n",
    "        for data, target in self.train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if batch_count >= max_batches:\n",
    "                break\n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        print(f\"          Train Epoch {epoch}: loss={avg_loss:.4f} ({batch_count} batches)\")\n",
    "        return avg_loss\n",
    "\n",
    "    def _evaluate(self, model, criterion, genome_id: str, epoch: int):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        eval_batches = 0\n",
    "        max_eval_batches = min(len(self.test_loader), 20)\n",
    "        total_eval_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_eval_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                eval_batches += 1\n",
    "                if eval_batches >= max_eval_batches:\n",
    "                    break\n",
    "        accuracy = 100.0 * correct / max(1, total)\n",
    "        avg_eval_loss = total_eval_loss / max(1, eval_batches)\n",
    "        print(f\"          Eval  Epoch {epoch}: acc={accuracy:.2f}% loss={avg_eval_loss:.4f} ({eval_batches} batches)\")\n",
    "        return accuracy, avg_eval_loss\n",
    "\n",
    "    def evaluate_fitness(self, genome: dict) -> float:\n",
    "        try:\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "            optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            best_acc = 0.0\n",
    "            best_epoch = -1\n",
    "            patience_left = self.config['epoch_patience']\n",
    "            last_improvement_acc = 0.0\n",
    "\n",
    "            max_epochs = self.config['num_epochs']\n",
    "            print(f\"      Training/Evaluating model {genome['id']} (max {max_epochs} epochs interleaved)\")\n",
    "\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                # 1) Train epoch\n",
    "                self._train_one_epoch(model, optimizer, criterion, genome['id'], epoch)\n",
    "                # 2) Evaluate right after training epoch\n",
    "                acc, eval_loss = self._evaluate(model, criterion, genome['id'], epoch)\n",
    "\n",
    "                # Early stopping logic based on accuracy improvement\n",
    "                improvement = acc - last_improvement_acc\n",
    "                if improvement >= self.config['improvement_threshold']:\n",
    "                    patience_left = self.config['epoch_patience']\n",
    "                    last_improvement_acc = acc\n",
    "                else:\n",
    "                    patience_left -= 1\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                print(f\"              -> Acc={acc:.2f}% (best={best_acc:.2f}% at epoch {best_epoch}) patience_left={patience_left}\")\n",
    "\n",
    "                if patience_left <= 0:\n",
    "                    print(f\"              Early stopping triggered (no significant improvement)\")\n",
    "                    break\n",
    "\n",
    "            print(f\"      Final fitness for {genome['id']}: {best_acc:.2f}% (best epoch {best_epoch})\")\n",
    "            return best_acc\n",
    "        except Exception as e:\n",
    "            print(f\"      ERROR evaluating genome {genome['id']}: {e}\")\n",
    "            logger.warning(f\"Error evaluating genome {genome['id']}: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        print(f\"\\nEvaluating population (Generation {self.generation})...\")\n",
    "        print(f\"Processing {len(self.population)} individuals...\")\n",
    "        fitness_scores = []\n",
    "        best_fitness_so_far = 0.0\n",
    "        for i, genome in enumerate(self.population):\n",
    "            print(f\"\\n   Evaluating individual {i+1}/{len(self.population)} (ID: {genome['id']})\")\n",
    "            print(f\"      Architecture: {genome['num_conv_layers']} conv + {genome['num_fc_layers']} fc, opt={genome['optimizer']}, lr={genome['learning_rate']}\")\n",
    "            fitness = self.evaluate_fitness(genome)\n",
    "            genome['fitness'] = fitness\n",
    "            fitness_scores.append(fitness)\n",
    "            if fitness > best_fitness_so_far:\n",
    "                best_fitness_so_far = fitness\n",
    "                print(f\"      New best fitness in this generation: {fitness:.2f}%!\")\n",
    "            print(f\"      Fitness obtained: {fitness:.2f}% | Best so far: {best_fitness_so_far:.2f}%\")\n",
    "        # Generation statistics\n",
    "        if fitness_scores:\n",
    "            avg_fitness = np.mean(fitness_scores)\n",
    "            max_fitness = np.max(fitness_scores)\n",
    "            min_fitness = np.min(fitness_scores)\n",
    "            std_fitness = np.std(fitness_scores)\n",
    "        else:\n",
    "            avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "        stats = {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': avg_fitness,\n",
    "            'max_fitness': max_fitness,\n",
    "            'min_fitness': min_fitness,\n",
    "            'std_fitness': std_fitness\n",
    "        }\n",
    "        self.generation_stats.append(stats)\n",
    "        self.fitness_history.append(max_fitness)\n",
    "\n",
    "        best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "        if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "            self.best_individual = copy.deepcopy(best_genome)\n",
    "            print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "        print(f\"\\nGENERATION {self.generation} STATISTICS:\")\n",
    "        print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "        print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "        print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "        print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "        print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "        print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "    def selection_and_reproduction(self):\n",
    "        print(f\"\\nStarting selection and reproduction...\")\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "        elite = self.population[:elite_size]\n",
    "        print(f\"Selecting {elite_size} elite individuals:\")\n",
    "        for i, individual in enumerate(elite):\n",
    "            print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "        new_population = copy.deepcopy(elite)\n",
    "        offspring_needed = self.config['population_size'] - len(new_population)\n",
    "        print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "        offspring_created = 0\n",
    "        while len(new_population) < self.config['population_size']:\n",
    "            parent1 = self.tournament_selection()\n",
    "            parent2 = self.tournament_selection()\n",
    "            child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "            child1 = mutate_genome(child1, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child1)\n",
    "            child2 = mutate_genome(child2, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child2)\n",
    "            offspring_created += 2\n",
    "            if offspring_created % 4 == 0:\n",
    "                print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "        self.population = new_population[:self.config['population_size']]\n",
    "        print(f\"New generation created with {len(self.population)} individuals\")\n",
    "        print(f\"   Elite preserved: {elite_size}\")\n",
    "        print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "    def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "        tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "        return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "    def _update_adaptive_mutation(self):\n",
    "        # Diversity measured via std of fitness in last generation\n",
    "        if not self.generation_stats:\n",
    "            self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "            return\n",
    "        last_std = self.generation_stats[-1]['std_fitness']\n",
    "        # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "        # Normalize std roughly assuming fitness in [0,100]\n",
    "        diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "        # Invert: low diversity (small std) should raise mutation\n",
    "        inverted = 1 - diversity_factor\n",
    "        new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "        new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "        self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "        print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "    def check_convergence(self) -> bool:\n",
    "        if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "            print(f\"Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "            return True\n",
    "        if self.generation >= self.config['max_generations']:\n",
    "            print(f\"Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "            return True\n",
    "        if len(self.fitness_history) >= 3:\n",
    "            recent = self.fitness_history[-3:]\n",
    "            if max(recent) - min(recent) < 0.5:\n",
    "                print(\"Stagnation detected in last 3 generations\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def evolve(self) -> dict:\n",
    "        print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "        print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "        print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(\"=\"*60)\n",
    "        self.initialize_population()\n",
    "        while not self.check_convergence():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GENERATION {self.generation}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            self.evaluate_population()\n",
    "            if self.check_convergence():\n",
    "                break\n",
    "            self._update_adaptive_mutation()\n",
    "            self.selection_and_reproduction()\n",
    "            self.generation += 1\n",
    "            print(f\"\\nPreparing for next generation...\")\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVOLUTION COMPLETED!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best individual found:\")\n",
    "        print(f\"   ID: {self.best_individual['id']}\")\n",
    "        print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Origin generation: {self.generation}\")\n",
    "        print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "        return self.best_individual\n",
    "\n",
    "print(\"HybridNeuroevolution class updated with adaptive mutation and interleaved early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51ada5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration:\n",
      "   Dataset: CIFAR10\n",
      "   Image size: 32x32x3\n",
      "   Number of classes: 10\n",
      "   Population: 30 individuals\n",
      "   Maximum generations: 30\n",
      "   Target fitness: 99.9%\n",
      "   Device: cuda\n",
      "\n",
      "Reloading dataset with new configuration...\n",
      "Loading CIFAR-10 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:26:00,224 - WARNING - Error evaluating genome b3d3f8b3: Expected more than 1 value per channel when training, got input size torch.Size([1, 157, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 dataset loaded:\n",
      "   Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "   Training samples: 50000\n",
      "   Test samples: 10000\n",
      "\n",
      "Starting neuroevolution at 11:26:00\n",
      "STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation)\n",
      "============================================================\n",
      "Configuration:\n",
      "   Population: 30 individuals\n",
      "   Maximum generations: 30\n",
      "   Target fitness: 99.9%\n",
      "   Device: cuda\n",
      "============================================================\n",
      "Initializing population of 30 individuals...\n",
      "Population initialized with 30 individuals\n",
      "\n",
      "================================================================================\n",
      "GENERATION 0\n",
      "================================================================================\n",
      "\n",
      "Evaluating population (Generation 0)...\n",
      "Processing 30 individuals...\n",
      "\n",
      "   Evaluating individual 1/30 (ID: b3d3f8b3)\n",
      "      Architecture: 4 conv + 1 fc, opt=rmsprop, lr=0.0001\n",
      "      ERROR evaluating genome b3d3f8b3: Expected more than 1 value per channel when training, got input size torch.Size([1, 157, 1, 1])\n",
      "      Fitness obtained: 0.00% | Best so far: 0.00%\n",
      "\n",
      "   Evaluating individual 2/30 (ID: 09eb9c54)\n",
      "      Architecture: 3 conv + 5 fc, opt=sgd, lr=0.001\n",
      "      Training/Evaluating model 09eb9c54 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=2.3028 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3025 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3026 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3024 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3025 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3022 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3023 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3021 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3022 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:26:39,118 - WARNING - Error evaluating genome 40f38b58: Expected more than 1 value per channel when training, got input size torch.Size([1, 118, 1, 1])\n",
      "2025-08-12 11:26:39,160 - WARNING - Error evaluating genome d444d90f: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "2025-08-12 11:26:39,173 - WARNING - Error evaluating genome 7c1f80a2: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "2025-08-12 11:26:39,186 - WARNING - Error evaluating genome 37fb41fe: Expected more than 1 value per channel when training, got input size torch.Size([1, 153, 1, 1])\n",
      "2025-08-12 11:26:39,196 - WARNING - Error evaluating genome ecf629d3: Calculated padded input size per channel: (6 x 6). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 5: acc=10.00% loss=2.3019 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 09eb9c54: 10.00% (best epoch 1)\n",
      "      New best fitness in this generation: 10.00%!\n",
      "      Fitness obtained: 10.00% | Best so far: 10.00%\n",
      "\n",
      "   Evaluating individual 3/30 (ID: 40f38b58)\n",
      "      Architecture: 6 conv + 5 fc, opt=sgd, lr=0.005\n",
      "      ERROR evaluating genome 40f38b58: Expected more than 1 value per channel when training, got input size torch.Size([1, 118, 1, 1])\n",
      "      Fitness obtained: 0.00% | Best so far: 10.00%\n",
      "\n",
      "   Evaluating individual 4/30 (ID: d444d90f)\n",
      "      Architecture: 4 conv + 2 fc, opt=sgd, lr=0.01\n",
      "      ERROR evaluating genome d444d90f: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 10.00%\n",
      "\n",
      "   Evaluating individual 5/30 (ID: 7c1f80a2)\n",
      "      Architecture: 7 conv + 1 fc, opt=sgd, lr=0.01\n",
      "      ERROR evaluating genome 7c1f80a2: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 10.00%\n",
      "\n",
      "   Evaluating individual 6/30 (ID: 37fb41fe)\n",
      "      Architecture: 6 conv + 5 fc, opt=rmsprop, lr=0.0001\n",
      "      ERROR evaluating genome 37fb41fe: Expected more than 1 value per channel when training, got input size torch.Size([1, 153, 1, 1])\n",
      "      Fitness obtained: 0.00% | Best so far: 10.00%\n",
      "\n",
      "   Evaluating individual 7/30 (ID: ecf629d3)\n",
      "      Architecture: 4 conv + 1 fc, opt=rmsprop, lr=0.0001\n",
      "      ERROR evaluating genome ecf629d3: Calculated padded input size per channel: (6 x 6). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 10.00%\n",
      "\n",
      "   Evaluating individual 8/30 (ID: 868b1e52)\n",
      "      Architecture: 1 conv + 5 fc, opt=adam, lr=0.0001\n",
      "      Training/Evaluating model 868b1e52 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=1.9023 (98 batches)\n",
      "          Eval  Epoch 1: acc=41.44% loss=1.6300 (20 batches)\n",
      "              -> Acc=41.44% (best=41.44% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.6316 (98 batches)\n",
      "          Eval  Epoch 2: acc=45.99% loss=1.4899 (20 batches)\n",
      "              -> Acc=45.99% (best=45.99% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.5028 (98 batches)\n",
      "          Eval  Epoch 3: acc=51.51% loss=1.3300 (20 batches)\n",
      "              -> Acc=51.51% (best=51.51% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.4219 (98 batches)\n",
      "          Eval  Epoch 4: acc=54.14% loss=1.2501 (20 batches)\n",
      "              -> Acc=54.14% (best=54.14% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.3549 (98 batches)\n",
      "          Eval  Epoch 5: acc=55.71% loss=1.2142 (20 batches)\n",
      "              -> Acc=55.71% (best=55.71% at epoch 5) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.3037 (98 batches)\n",
      "          Eval  Epoch 6: acc=57.55% loss=1.1706 (20 batches)\n",
      "              -> Acc=57.55% (best=57.55% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=1.2649 (98 batches)\n",
      "          Eval  Epoch 7: acc=58.19% loss=1.1440 (20 batches)\n",
      "              -> Acc=58.19% (best=58.19% at epoch 7) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=1.2270 (98 batches)\n",
      "          Eval  Epoch 8: acc=59.98% loss=1.0942 (20 batches)\n",
      "              -> Acc=59.98% (best=59.98% at epoch 8) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=1.1989 (98 batches)\n",
      "          Eval  Epoch 9: acc=61.43% loss=1.0752 (20 batches)\n",
      "              -> Acc=61.43% (best=61.43% at epoch 9) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=1.1747 (98 batches)\n",
      "          Eval  Epoch 10: acc=62.03% loss=1.0606 (20 batches)\n",
      "              -> Acc=62.03% (best=62.03% at epoch 10) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=1.1531 (98 batches)\n",
      "          Eval  Epoch 11: acc=62.55% loss=1.0383 (20 batches)\n",
      "              -> Acc=62.55% (best=62.55% at epoch 11) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=1.1291 (98 batches)\n",
      "          Eval  Epoch 12: acc=63.16% loss=1.0224 (20 batches)\n",
      "              -> Acc=63.16% (best=63.16% at epoch 12) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=1.1137 (98 batches)\n",
      "          Eval  Epoch 13: acc=65.01% loss=0.9872 (20 batches)\n",
      "              -> Acc=65.01% (best=65.01% at epoch 13) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=1.0897 (98 batches)\n",
      "          Eval  Epoch 14: acc=64.78% loss=0.9814 (20 batches)\n",
      "              -> Acc=64.78% (best=65.01% at epoch 13) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=1.0753 (98 batches)\n",
      "          Eval  Epoch 15: acc=65.61% loss=0.9699 (20 batches)\n",
      "              -> Acc=65.61% (best=65.61% at epoch 15) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=1.0638 (98 batches)\n",
      "          Eval  Epoch 16: acc=64.76% loss=0.9831 (20 batches)\n",
      "              -> Acc=64.76% (best=65.61% at epoch 15) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 17: loss=1.0519 (98 batches)\n",
      "          Eval  Epoch 17: acc=65.55% loss=0.9594 (20 batches)\n",
      "              -> Acc=65.55% (best=65.61% at epoch 15) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 18: loss=1.0386 (98 batches)\n",
      "          Eval  Epoch 18: acc=66.34% loss=0.9496 (20 batches)\n",
      "              -> Acc=66.34% (best=66.34% at epoch 18) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 19: loss=1.0248 (98 batches)\n",
      "          Eval  Epoch 19: acc=67.90% loss=0.9092 (20 batches)\n",
      "              -> Acc=67.90% (best=67.90% at epoch 19) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 20: loss=1.0070 (98 batches)\n",
      "          Eval  Epoch 20: acc=67.71% loss=0.9009 (20 batches)\n",
      "              -> Acc=67.71% (best=67.90% at epoch 19) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 21: loss=1.0034 (98 batches)\n",
      "          Eval  Epoch 21: acc=67.75% loss=0.9032 (20 batches)\n",
      "              -> Acc=67.75% (best=67.90% at epoch 19) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 22: loss=0.9876 (98 batches)\n",
      "          Eval  Epoch 22: acc=67.69% loss=0.9076 (20 batches)\n",
      "              -> Acc=67.69% (best=67.90% at epoch 19) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 23: loss=0.9825 (98 batches)\n",
      "          Eval  Epoch 23: acc=68.43% loss=0.8810 (20 batches)\n",
      "              -> Acc=68.43% (best=68.43% at epoch 23) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 24: loss=0.9716 (98 batches)\n",
      "          Eval  Epoch 24: acc=69.02% loss=0.8742 (20 batches)\n",
      "              -> Acc=69.02% (best=69.02% at epoch 24) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 25: loss=0.9624 (98 batches)\n",
      "          Eval  Epoch 25: acc=70.00% loss=0.8554 (20 batches)\n",
      "              -> Acc=70.00% (best=70.00% at epoch 25) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 26: loss=0.9532 (98 batches)\n",
      "          Eval  Epoch 26: acc=70.28% loss=0.8480 (20 batches)\n",
      "              -> Acc=70.28% (best=70.28% at epoch 26) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 27: loss=0.9456 (98 batches)\n",
      "          Eval  Epoch 27: acc=69.45% loss=0.8545 (20 batches)\n",
      "              -> Acc=69.45% (best=70.28% at epoch 26) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 28: loss=0.9388 (98 batches)\n",
      "          Eval  Epoch 28: acc=69.94% loss=0.8459 (20 batches)\n",
      "              -> Acc=69.94% (best=70.28% at epoch 26) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 29: loss=0.9290 (98 batches)\n",
      "          Eval  Epoch 29: acc=69.73% loss=0.8500 (20 batches)\n",
      "              -> Acc=69.73% (best=70.28% at epoch 26) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 30: loss=0.9157 (98 batches)\n",
      "          Eval  Epoch 30: acc=70.32% loss=0.8323 (20 batches)\n",
      "              -> Acc=70.32% (best=70.32% at epoch 30) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 868b1e52: 70.32% (best epoch 30)\n",
      "      New best fitness in this generation: 70.32%!\n",
      "      Fitness obtained: 70.32% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 9/30 (ID: 4f2e98dd)\n",
      "      Architecture: 2 conv + 2 fc, opt=adam, lr=0.005\n",
      "      Training/Evaluating model 4f2e98dd (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=6.5651 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3027 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3027 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3027 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3028 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3027 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:31:24,579 - WARNING - Error evaluating genome 1c7cf593: Expected more than 1 value per channel when training, got input size torch.Size([1, 80, 1, 1])\n",
      "2025-08-12 11:31:24,587 - WARNING - Error evaluating genome c01e2752: Expected more than 1 value per channel when training, got input size torch.Size([1, 68, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 5: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 4f2e98dd: 10.00% (best epoch 1)\n",
      "      Fitness obtained: 10.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 10/30 (ID: 1c7cf593)\n",
      "      Architecture: 5 conv + 3 fc, opt=sgd, lr=0.0001\n",
      "      ERROR evaluating genome 1c7cf593: Expected more than 1 value per channel when training, got input size torch.Size([1, 80, 1, 1])\n",
      "      Fitness obtained: 0.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 11/30 (ID: c01e2752)\n",
      "      Architecture: 4 conv + 5 fc, opt=adamw, lr=0.0001\n",
      "      ERROR evaluating genome c01e2752: Expected more than 1 value per channel when training, got input size torch.Size([1, 68, 1, 1])\n",
      "      Fitness obtained: 0.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 12/30 (ID: 4585babb)\n",
      "      Architecture: 2 conv + 7 fc, opt=adam, lr=0.01\n",
      "      Training/Evaluating model 4585babb (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=69.1780 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3028 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3044 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3078 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3252 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3032 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3183 (98 batches)\n",
      "          Eval  Epoch 5: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 4585babb: 10.00% (best epoch 1)\n",
      "      Fitness obtained: 10.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 13/30 (ID: 2e92c8f2)\n",
      "      Architecture: 1 conv + 4 fc, opt=sgd, lr=0.0001\n",
      "      Training/Evaluating model 2e92c8f2 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 1: acc=9.94% loss=2.3029 (20 batches)\n",
      "              -> Acc=9.94% (best=9.94% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 2: acc=9.94% loss=2.3029 (20 batches)\n",
      "              -> Acc=9.94% (best=9.94% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 3: acc=9.95% loss=2.3028 (20 batches)\n",
      "              -> Acc=9.95% (best=9.95% at epoch 3) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3029 (98 batches)\n",
      "          Eval  Epoch 4: acc=9.95% loss=2.3028 (20 batches)\n",
      "              -> Acc=9.95% (best=9.95% at epoch 3) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3029 (98 batches)\n",
      "          Eval  Epoch 5: acc=9.96% loss=2.3028 (20 batches)\n",
      "              -> Acc=9.96% (best=9.96% at epoch 5) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 2e92c8f2: 9.96% (best epoch 5)\n",
      "      Fitness obtained: 9.96% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 14/30 (ID: 106a4e4f)\n",
      "      Architecture: 2 conv + 2 fc, opt=rmsprop, lr=0.01\n",
      "      Training/Evaluating model 106a4e4f (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=858.1587 (98 batches)\n",
      "          Eval  Epoch 1: acc=11.40% loss=3.8680 (20 batches)\n",
      "              -> Acc=11.40% (best=11.40% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3623 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.97% loss=3.0634 (20 batches)\n",
      "              -> Acc=10.97% (best=11.40% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3467 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.63% loss=2.5786 (20 batches)\n",
      "              -> Acc=10.63% (best=11.40% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3168 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.59% loss=2.4590 (20 batches)\n",
      "              -> Acc=10.59% (best=11.40% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3161 (98 batches)\n",
      "          Eval  Epoch 5: acc=10.00% loss=2.3859 (20 batches)\n",
      "              -> Acc=10.00% (best=11.40% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 106a4e4f: 11.40% (best epoch 1)\n",
      "      Fitness obtained: 11.40% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 15/30 (ID: 0a31c8c8)\n",
      "      Architecture: 4 conv + 5 fc, opt=sgd, lr=0.005\n",
      "      Training/Evaluating model 0a31c8c8 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=2.3029 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3027 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3029 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3028 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3027 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3025 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3026 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:33:50,163 - WARNING - Error evaluating genome c3ebeb21: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "2025-08-12 11:33:50,211 - WARNING - Error evaluating genome cce58996: Calculated padded input size per channel: (6 x 6). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 5: acc=10.00% loss=2.3024 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 0a31c8c8: 10.00% (best epoch 1)\n",
      "      Fitness obtained: 10.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 16/30 (ID: c3ebeb21)\n",
      "      Architecture: 5 conv + 5 fc, opt=adam, lr=0.01\n",
      "      ERROR evaluating genome c3ebeb21: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 17/30 (ID: cce58996)\n",
      "      Architecture: 7 conv + 2 fc, opt=sgd, lr=0.005\n",
      "      ERROR evaluating genome cce58996: Calculated padded input size per channel: (6 x 6). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 18/30 (ID: 4daafcd6)\n",
      "      Architecture: 2 conv + 4 fc, opt=sgd, lr=0.0001\n",
      "      Training/Evaluating model 4daafcd6 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=2.3028 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.04% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.04% (best=10.04% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3027 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.03% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.03% (best=10.04% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3026 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.04% loss=2.3025 (20 batches)\n",
      "              -> Acc=10.04% (best=10.04% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3027 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.04% loss=2.3025 (20 batches)\n",
      "              -> Acc=10.04% (best=10.04% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3026 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:34:27,439 - WARNING - Error evaluating genome 2abed6f3: Calculated padded input size per channel: (4 x 4). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 5: acc=10.04% loss=2.3024 (20 batches)\n",
      "              -> Acc=10.04% (best=10.04% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 4daafcd6: 10.04% (best epoch 1)\n",
      "      Fitness obtained: 10.04% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 19/30 (ID: 2abed6f3)\n",
      "      Architecture: 6 conv + 2 fc, opt=adamw, lr=0.01\n",
      "      ERROR evaluating genome 2abed6f3: Calculated padded input size per channel: (4 x 4). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 20/30 (ID: dd50db24)\n",
      "      Architecture: 2 conv + 6 fc, opt=adamw, lr=0.01\n",
      "      Training/Evaluating model dd50db24 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=29.7370 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3028 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3044 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3028 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3027 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3089 (98 batches)\n",
      "          Eval  Epoch 5: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for dd50db24: 10.00% (best epoch 1)\n",
      "      Fitness obtained: 10.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 21/30 (ID: bd8fd5f8)\n",
      "      Architecture: 2 conv + 6 fc, opt=sgd, lr=0.0001\n",
      "      Training/Evaluating model bd8fd5f8 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=2.3031 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3031 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3031 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3031 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:35:42,554 - WARNING - Error evaluating genome e832ccfb: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 5: acc=10.00% loss=2.3031 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for bd8fd5f8: 10.00% (best epoch 1)\n",
      "      Fitness obtained: 10.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 22/30 (ID: e832ccfb)\n",
      "      Architecture: 6 conv + 1 fc, opt=adam, lr=0.001\n",
      "      ERROR evaluating genome e832ccfb: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 23/30 (ID: f2e9f165)\n",
      "      Architecture: 1 conv + 4 fc, opt=adamw, lr=0.005\n",
      "      Training/Evaluating model f2e9f165 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=20.1610 (98 batches)\n",
      "          Eval  Epoch 1: acc=15.16% loss=2.2329 (20 batches)\n",
      "              -> Acc=15.16% (best=15.16% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.1900 (98 batches)\n",
      "          Eval  Epoch 2: acc=18.81% loss=2.1128 (20 batches)\n",
      "              -> Acc=18.81% (best=18.81% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.1425 (98 batches)\n",
      "          Eval  Epoch 3: acc=20.14% loss=2.0955 (20 batches)\n",
      "              -> Acc=20.14% (best=20.14% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.1251 (98 batches)\n",
      "          Eval  Epoch 4: acc=20.38% loss=2.0993 (20 batches)\n",
      "              -> Acc=20.38% (best=20.38% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.1107 (98 batches)\n",
      "          Eval  Epoch 5: acc=19.33% loss=2.0646 (20 batches)\n",
      "              -> Acc=19.33% (best=20.38% at epoch 4) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=2.1000 (98 batches)\n",
      "          Eval  Epoch 6: acc=19.29% loss=2.0797 (20 batches)\n",
      "              -> Acc=19.29% (best=20.38% at epoch 4) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=2.0913 (98 batches)\n",
      "          Eval  Epoch 7: acc=20.97% loss=2.0772 (20 batches)\n",
      "              -> Acc=20.97% (best=20.97% at epoch 7) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=2.0760 (98 batches)\n",
      "          Eval  Epoch 8: acc=23.54% loss=2.0400 (20 batches)\n",
      "              -> Acc=23.54% (best=23.54% at epoch 8) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=2.0576 (98 batches)\n",
      "          Eval  Epoch 9: acc=23.34% loss=2.0103 (20 batches)\n",
      "              -> Acc=23.34% (best=23.54% at epoch 8) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=2.0482 (98 batches)\n",
      "          Eval  Epoch 10: acc=23.71% loss=1.9863 (20 batches)\n",
      "              -> Acc=23.71% (best=23.71% at epoch 10) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=2.0308 (98 batches)\n",
      "          Eval  Epoch 11: acc=24.39% loss=1.9777 (20 batches)\n",
      "              -> Acc=24.39% (best=24.39% at epoch 11) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=2.0139 (98 batches)\n",
      "          Eval  Epoch 12: acc=25.55% loss=1.9655 (20 batches)\n",
      "              -> Acc=25.55% (best=25.55% at epoch 12) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=2.0149 (98 batches)\n",
      "          Eval  Epoch 13: acc=25.30% loss=1.9277 (20 batches)\n",
      "              -> Acc=25.30% (best=25.55% at epoch 12) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=2.1061 (98 batches)\n",
      "          Eval  Epoch 14: acc=21.92% loss=2.0097 (20 batches)\n",
      "              -> Acc=21.92% (best=25.55% at epoch 12) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=2.0589 (98 batches)\n",
      "          Eval  Epoch 15: acc=22.45% loss=1.9480 (20 batches)\n",
      "              -> Acc=22.45% (best=25.55% at epoch 12) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=2.0467 (98 batches)\n",
      "          Eval  Epoch 16: acc=24.12% loss=1.9649 (20 batches)\n",
      "              -> Acc=24.12% (best=25.55% at epoch 12) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for f2e9f165: 25.55% (best epoch 12)\n",
      "      Fitness obtained: 25.55% | Best so far: 70.32%\n",
      "\n",
      "   Evaluating individual 24/30 (ID: d46dff3a)\n",
      "      Architecture: 2 conv + 4 fc, opt=adam, lr=0.001\n",
      "      Training/Evaluating model d46dff3a (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=1.6821 (98 batches)\n",
      "          Eval  Epoch 1: acc=51.38% loss=1.3438 (20 batches)\n",
      "              -> Acc=51.38% (best=51.38% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.3677 (98 batches)\n",
      "          Eval  Epoch 2: acc=59.30% loss=1.1467 (20 batches)\n",
      "              -> Acc=59.30% (best=59.30% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.2439 (98 batches)\n",
      "          Eval  Epoch 3: acc=59.79% loss=1.1067 (20 batches)\n",
      "              -> Acc=59.79% (best=59.79% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.1640 (98 batches)\n",
      "          Eval  Epoch 4: acc=63.39% loss=1.0344 (20 batches)\n",
      "              -> Acc=63.39% (best=63.39% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.0928 (98 batches)\n",
      "          Eval  Epoch 5: acc=66.82% loss=0.9344 (20 batches)\n",
      "              -> Acc=66.82% (best=66.82% at epoch 5) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.0510 (98 batches)\n",
      "          Eval  Epoch 6: acc=67.22% loss=0.9219 (20 batches)\n",
      "              -> Acc=67.22% (best=67.22% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=1.0119 (98 batches)\n",
      "          Eval  Epoch 7: acc=68.85% loss=0.9019 (20 batches)\n",
      "              -> Acc=68.85% (best=68.85% at epoch 7) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=0.9772 (98 batches)\n",
      "          Eval  Epoch 8: acc=70.92% loss=0.8366 (20 batches)\n",
      "              -> Acc=70.92% (best=70.92% at epoch 8) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=0.9514 (98 batches)\n",
      "          Eval  Epoch 9: acc=70.73% loss=0.8272 (20 batches)\n",
      "              -> Acc=70.73% (best=70.92% at epoch 8) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=0.9311 (98 batches)\n",
      "          Eval  Epoch 10: acc=70.41% loss=0.8552 (20 batches)\n",
      "              -> Acc=70.41% (best=70.92% at epoch 8) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=0.9049 (98 batches)\n",
      "          Eval  Epoch 11: acc=71.72% loss=0.8066 (20 batches)\n",
      "              -> Acc=71.72% (best=71.72% at epoch 11) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=0.8872 (98 batches)\n",
      "          Eval  Epoch 12: acc=71.03% loss=0.8358 (20 batches)\n",
      "              -> Acc=71.03% (best=71.72% at epoch 11) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=0.8828 (98 batches)\n",
      "          Eval  Epoch 13: acc=73.54% loss=0.7610 (20 batches)\n",
      "              -> Acc=73.54% (best=73.54% at epoch 13) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=0.8577 (98 batches)\n",
      "          Eval  Epoch 14: acc=74.00% loss=0.7605 (20 batches)\n",
      "              -> Acc=74.00% (best=74.00% at epoch 14) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=0.8428 (98 batches)\n",
      "          Eval  Epoch 15: acc=73.85% loss=0.7524 (20 batches)\n",
      "              -> Acc=73.85% (best=74.00% at epoch 14) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=0.8260 (98 batches)\n",
      "          Eval  Epoch 16: acc=75.46% loss=0.7249 (20 batches)\n",
      "              -> Acc=75.46% (best=75.46% at epoch 16) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 17: loss=0.8122 (98 batches)\n",
      "          Eval  Epoch 17: acc=74.41% loss=0.7207 (20 batches)\n",
      "              -> Acc=74.41% (best=75.46% at epoch 16) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 18: loss=0.8036 (98 batches)\n",
      "          Eval  Epoch 18: acc=75.27% loss=0.7249 (20 batches)\n",
      "              -> Acc=75.27% (best=75.46% at epoch 16) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 19: loss=0.8004 (98 batches)\n",
      "          Eval  Epoch 19: acc=73.59% loss=0.7684 (20 batches)\n",
      "              -> Acc=73.59% (best=75.46% at epoch 16) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 20: loss=0.7930 (98 batches)\n",
      "          Eval  Epoch 20: acc=75.73% loss=0.7045 (20 batches)\n",
      "              -> Acc=75.73% (best=75.73% at epoch 20) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 21: loss=0.7778 (98 batches)\n",
      "          Eval  Epoch 21: acc=75.52% loss=0.7122 (20 batches)\n",
      "              -> Acc=75.52% (best=75.73% at epoch 20) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 22: loss=0.7723 (98 batches)\n",
      "          Eval  Epoch 22: acc=76.00% loss=0.6986 (20 batches)\n",
      "              -> Acc=76.00% (best=76.00% at epoch 22) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 23: loss=0.7688 (98 batches)\n",
      "          Eval  Epoch 23: acc=75.42% loss=0.7101 (20 batches)\n",
      "              -> Acc=75.42% (best=76.00% at epoch 22) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 24: loss=0.7610 (98 batches)\n",
      "          Eval  Epoch 24: acc=75.64% loss=0.7063 (20 batches)\n",
      "              -> Acc=75.64% (best=76.00% at epoch 22) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 25: loss=0.7546 (98 batches)\n",
      "          Eval  Epoch 25: acc=75.37% loss=0.7239 (20 batches)\n",
      "              -> Acc=75.37% (best=76.00% at epoch 22) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 26: loss=0.7446 (98 batches)\n",
      "          Eval  Epoch 26: acc=76.63% loss=0.6893 (20 batches)\n",
      "              -> Acc=76.63% (best=76.63% at epoch 26) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 27: loss=0.7334 (98 batches)\n",
      "          Eval  Epoch 27: acc=77.72% loss=0.6548 (20 batches)\n",
      "              -> Acc=77.72% (best=77.72% at epoch 27) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 28: loss=0.7315 (98 batches)\n",
      "          Eval  Epoch 28: acc=77.25% loss=0.6479 (20 batches)\n",
      "              -> Acc=77.25% (best=77.72% at epoch 27) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 29: loss=0.7245 (98 batches)\n",
      "          Eval  Epoch 29: acc=77.78% loss=0.6482 (20 batches)\n",
      "              -> Acc=77.78% (best=77.78% at epoch 29) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 30: loss=0.7116 (98 batches)\n",
      "          Eval  Epoch 30: acc=77.83% loss=0.6616 (20 batches)\n",
      "              -> Acc=77.83% (best=77.83% at epoch 30) patience_left=1\n",
      "      Final fitness for d46dff3a: 77.83% (best epoch 30)\n",
      "      New best fitness in this generation: 77.83%!\n",
      "      Fitness obtained: 77.83% | Best so far: 77.83%\n",
      "\n",
      "   Evaluating individual 25/30 (ID: f5d25ac0)\n",
      "      Architecture: 3 conv + 5 fc, opt=rmsprop, lr=0.0001\n",
      "      Training/Evaluating model f5d25ac0 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=1.8989 (98 batches)\n",
      "          Eval  Epoch 1: acc=17.76% loss=2.8330 (20 batches)\n",
      "              -> Acc=17.76% (best=17.76% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.5456 (98 batches)\n",
      "          Eval  Epoch 2: acc=28.28% loss=2.1844 (20 batches)\n",
      "              -> Acc=28.28% (best=28.28% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.3817 (98 batches)\n",
      "          Eval  Epoch 3: acc=49.65% loss=1.3664 (20 batches)\n",
      "              -> Acc=49.65% (best=49.65% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.2721 (98 batches)\n",
      "          Eval  Epoch 4: acc=39.55% loss=1.8538 (20 batches)\n",
      "              -> Acc=39.55% (best=49.65% at epoch 3) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.1880 (98 batches)\n",
      "          Eval  Epoch 5: acc=45.04% loss=1.5815 (20 batches)\n",
      "              -> Acc=45.04% (best=49.65% at epoch 3) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.1263 (98 batches)\n",
      "          Eval  Epoch 6: acc=54.11% loss=1.3665 (20 batches)\n",
      "              -> Acc=54.11% (best=54.11% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=1.0778 (98 batches)\n",
      "          Eval  Epoch 7: acc=43.72% loss=1.7027 (20 batches)\n",
      "              -> Acc=43.72% (best=54.11% at epoch 6) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=1.0317 (98 batches)\n",
      "          Eval  Epoch 8: acc=38.17% loss=2.3528 (20 batches)\n",
      "              -> Acc=38.17% (best=54.11% at epoch 6) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=0.9945 (98 batches)\n",
      "          Eval  Epoch 9: acc=52.26% loss=1.5300 (20 batches)\n",
      "              -> Acc=52.26% (best=54.11% at epoch 6) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=0.9646 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:42:41,212 - WARNING - Error evaluating genome 5662e53f: Expected more than 1 value per channel when training, got input size torch.Size([1, 69, 1, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 10: acc=47.09% loss=1.6774 (20 batches)\n",
      "              -> Acc=47.09% (best=54.11% at epoch 6) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for f5d25ac0: 54.11% (best epoch 6)\n",
      "      Fitness obtained: 54.11% | Best so far: 77.83%\n",
      "\n",
      "   Evaluating individual 26/30 (ID: 5662e53f)\n",
      "      Architecture: 6 conv + 5 fc, opt=adamw, lr=0.005\n",
      "      ERROR evaluating genome 5662e53f: Expected more than 1 value per channel when training, got input size torch.Size([1, 69, 1, 1])\n",
      "      Fitness obtained: 0.00% | Best so far: 77.83%\n",
      "\n",
      "   Evaluating individual 27/30 (ID: a668a41b)\n",
      "      Architecture: 1 conv + 1 fc, opt=adam, lr=0.005\n",
      "      Training/Evaluating model a668a41b (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=7.3926 (98 batches)\n",
      "          Eval  Epoch 1: acc=30.89% loss=1.9277 (20 batches)\n",
      "              -> Acc=30.89% (best=30.89% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.8182 (98 batches)\n",
      "          Eval  Epoch 2: acc=38.55% loss=1.7341 (20 batches)\n",
      "              -> Acc=38.55% (best=38.55% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.7018 (98 batches)\n",
      "          Eval  Epoch 3: acc=41.38% loss=1.6596 (20 batches)\n",
      "              -> Acc=41.38% (best=41.38% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.6194 (98 batches)\n",
      "          Eval  Epoch 4: acc=44.47% loss=1.5292 (20 batches)\n",
      "              -> Acc=44.47% (best=44.47% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.5726 (98 batches)\n",
      "          Eval  Epoch 5: acc=45.34% loss=1.5064 (20 batches)\n",
      "              -> Acc=45.34% (best=45.34% at epoch 5) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.5185 (98 batches)\n",
      "          Eval  Epoch 6: acc=49.42% loss=1.4577 (20 batches)\n",
      "              -> Acc=49.42% (best=49.42% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=1.4689 (98 batches)\n",
      "          Eval  Epoch 7: acc=49.45% loss=1.4358 (20 batches)\n",
      "              -> Acc=49.45% (best=49.45% at epoch 7) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=1.4392 (98 batches)\n",
      "          Eval  Epoch 8: acc=53.37% loss=1.2947 (20 batches)\n",
      "              -> Acc=53.37% (best=53.37% at epoch 8) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=1.4397 (98 batches)\n",
      "          Eval  Epoch 9: acc=52.29% loss=1.3173 (20 batches)\n",
      "              -> Acc=52.29% (best=53.37% at epoch 8) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=1.4086 (98 batches)\n",
      "          Eval  Epoch 10: acc=54.96% loss=1.2574 (20 batches)\n",
      "              -> Acc=54.96% (best=54.96% at epoch 10) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=1.3653 (98 batches)\n",
      "          Eval  Epoch 11: acc=54.73% loss=1.2806 (20 batches)\n",
      "              -> Acc=54.73% (best=54.96% at epoch 10) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=1.3467 (98 batches)\n",
      "          Eval  Epoch 12: acc=53.80% loss=1.3266 (20 batches)\n",
      "              -> Acc=53.80% (best=54.96% at epoch 10) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=1.3493 (98 batches)\n",
      "          Eval  Epoch 13: acc=56.29% loss=1.2944 (20 batches)\n",
      "              -> Acc=56.29% (best=56.29% at epoch 13) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=1.3246 (98 batches)\n",
      "          Eval  Epoch 14: acc=56.58% loss=1.3146 (20 batches)\n",
      "              -> Acc=56.58% (best=56.58% at epoch 14) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=1.3328 (98 batches)\n",
      "          Eval  Epoch 15: acc=56.86% loss=1.2657 (20 batches)\n",
      "              -> Acc=56.86% (best=56.86% at epoch 15) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=1.3310 (98 batches)\n",
      "          Eval  Epoch 16: acc=58.10% loss=1.2143 (20 batches)\n",
      "              -> Acc=58.10% (best=58.10% at epoch 16) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 17: loss=1.2758 (98 batches)\n",
      "          Eval  Epoch 17: acc=54.21% loss=1.2938 (20 batches)\n",
      "              -> Acc=54.21% (best=58.10% at epoch 16) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 18: loss=1.2907 (98 batches)\n",
      "          Eval  Epoch 18: acc=56.24% loss=1.2309 (20 batches)\n",
      "              -> Acc=56.24% (best=58.10% at epoch 16) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 19: loss=1.2828 (98 batches)\n",
      "          Eval  Epoch 19: acc=58.28% loss=1.3127 (20 batches)\n",
      "              -> Acc=58.28% (best=58.28% at epoch 19) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 20: loss=1.2631 (98 batches)\n",
      "          Eval  Epoch 20: acc=56.21% loss=1.3889 (20 batches)\n",
      "              -> Acc=56.21% (best=58.28% at epoch 19) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for a668a41b: 58.28% (best epoch 19)\n",
      "      Fitness obtained: 58.28% | Best so far: 77.83%\n",
      "\n",
      "   Evaluating individual 28/30 (ID: 9e5af779)\n",
      "      Architecture: 3 conv + 7 fc, opt=adam, lr=0.01\n",
      "      Training/Evaluating model 9e5af779 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=68.6094 (98 batches)\n",
      "          Eval  Epoch 1: acc=10.00% loss=2.3030 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.3032 (98 batches)\n",
      "          Eval  Epoch 2: acc=10.00% loss=2.3027 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.3030 (98 batches)\n",
      "          Eval  Epoch 3: acc=10.00% loss=2.3027 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.3029 (98 batches)\n",
      "          Eval  Epoch 4: acc=10.00% loss=2.3026 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.3028 (98 batches)\n",
      "          Eval  Epoch 5: acc=10.00% loss=2.3027 (20 batches)\n",
      "              -> Acc=10.00% (best=10.00% at epoch 1) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 9e5af779: 10.00% (best epoch 1)\n",
      "      Fitness obtained: 10.00% | Best so far: 77.83%\n",
      "\n",
      "   Evaluating individual 29/30 (ID: 62848650)\n",
      "      Architecture: 2 conv + 7 fc, opt=adamw, lr=0.005\n",
      "      Training/Evaluating model 62848650 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=8.5402 (98 batches)\n",
      "          Eval  Epoch 1: acc=18.27% loss=2.1410 (20 batches)\n",
      "              -> Acc=18.27% (best=18.27% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=2.1780 (98 batches)\n",
      "          Eval  Epoch 2: acc=20.00% loss=2.0686 (20 batches)\n",
      "              -> Acc=20.00% (best=20.00% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=2.1178 (98 batches)\n",
      "          Eval  Epoch 3: acc=19.77% loss=2.0677 (20 batches)\n",
      "              -> Acc=19.77% (best=20.00% at epoch 2) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=2.0757 (98 batches)\n",
      "          Eval  Epoch 4: acc=20.11% loss=2.0366 (20 batches)\n",
      "              -> Acc=20.11% (best=20.11% at epoch 4) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=2.2112 (98 batches)\n",
      "          Eval  Epoch 5: acc=12.78% loss=2.2760 (20 batches)\n",
      "              -> Acc=12.78% (best=20.11% at epoch 4) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=2.1841 (98 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 11:46:34,220 - WARNING - Error evaluating genome 02b400dd: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Eval  Epoch 6: acc=16.28% loss=2.1455 (20 batches)\n",
      "              -> Acc=16.28% (best=20.11% at epoch 4) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for 62848650: 20.11% (best epoch 4)\n",
      "      Fitness obtained: 20.11% | Best so far: 77.83%\n",
      "\n",
      "   Evaluating individual 30/30 (ID: 02b400dd)\n",
      "      Architecture: 4 conv + 3 fc, opt=adamw, lr=0.005\n",
      "      ERROR evaluating genome 02b400dd: Calculated padded input size per channel: (3 x 3). Kernel size: (7 x 7). Kernel size can't be greater than actual input size\n",
      "      Fitness obtained: 0.00% | Best so far: 77.83%\n",
      "\n",
      "New global best individual found!\n",
      "\n",
      "GENERATION 0 STATISTICS:\n",
      "   Maximum fitness: 77.83%\n",
      "   Average fitness: 13.59%\n",
      "   Minimum fitness: 0.00%\n",
      "   Standard deviation: 21.51%\n",
      "   Best individual: d46dff3a with 77.83%\n",
      "   Global best individual: d46dff3a with 77.83%\n",
      "Adaptive mutation rate updated to 0.15 (std_fitness=21.51)\n",
      "\n",
      "Starting selection and reproduction...\n",
      "Selecting 6 elite individuals:\n",
      "   Elite 1: d46dff3a (fitness: 77.83%)\n",
      "   Elite 2: 868b1e52 (fitness: 70.32%)\n",
      "   Elite 3: a668a41b (fitness: 58.28%)\n",
      "   Elite 4: f5d25ac0 (fitness: 54.11%)\n",
      "   Elite 5: f2e9f165 (fitness: 25.55%)\n",
      "   Elite 6: 62848650 (fitness: 20.11%)\n",
      "Creating 24 new individuals through crossover and mutation...\n",
      "   Created 4 of 24 new individuals...\n",
      "   Created 8 of 24 new individuals...\n",
      "   Created 12 of 24 new individuals...\n",
      "   Created 16 of 24 new individuals...\n",
      "   Created 20 of 24 new individuals...\n",
      "   Created 24 of 24 new individuals...\n",
      "New generation created with 30 individuals\n",
      "   Elite preserved: 6\n",
      "   New individuals: 24\n",
      "\n",
      "Preparing for next generation...\n",
      "\n",
      "================================================================================\n",
      "GENERATION 1\n",
      "================================================================================\n",
      "\n",
      "Evaluating population (Generation 1)...\n",
      "Processing 30 individuals...\n",
      "\n",
      "   Evaluating individual 1/30 (ID: d46dff3a)\n",
      "      Architecture: 2 conv + 4 fc, opt=adam, lr=0.001\n",
      "      Training/Evaluating model d46dff3a (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=1.6555 (98 batches)\n",
      "          Eval  Epoch 1: acc=51.92% loss=1.2938 (20 batches)\n",
      "              -> Acc=51.92% (best=51.92% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.3245 (98 batches)\n",
      "          Eval  Epoch 2: acc=59.81% loss=1.1215 (20 batches)\n",
      "              -> Acc=59.81% (best=59.81% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.2046 (98 batches)\n",
      "          Eval  Epoch 3: acc=63.37% loss=1.0285 (20 batches)\n",
      "              -> Acc=63.37% (best=63.37% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.1259 (98 batches)\n",
      "          Eval  Epoch 4: acc=64.93% loss=0.9768 (20 batches)\n",
      "              -> Acc=64.93% (best=64.93% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.0724 (98 batches)\n",
      "          Eval  Epoch 5: acc=66.75% loss=0.9318 (20 batches)\n",
      "              -> Acc=66.75% (best=66.75% at epoch 5) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.0355 (98 batches)\n",
      "          Eval  Epoch 6: acc=68.86% loss=0.8912 (20 batches)\n",
      "              -> Acc=68.86% (best=68.86% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=0.9931 (98 batches)\n",
      "          Eval  Epoch 7: acc=69.34% loss=0.8605 (20 batches)\n",
      "              -> Acc=69.34% (best=69.34% at epoch 7) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=0.9676 (98 batches)\n",
      "          Eval  Epoch 8: acc=68.07% loss=0.9248 (20 batches)\n",
      "              -> Acc=68.07% (best=69.34% at epoch 7) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=0.9426 (98 batches)\n",
      "          Eval  Epoch 9: acc=71.02% loss=0.8214 (20 batches)\n",
      "              -> Acc=71.02% (best=71.02% at epoch 9) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=0.9222 (98 batches)\n",
      "          Eval  Epoch 10: acc=71.58% loss=0.8253 (20 batches)\n",
      "              -> Acc=71.58% (best=71.58% at epoch 10) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=0.9065 (98 batches)\n",
      "          Eval  Epoch 11: acc=70.41% loss=0.8499 (20 batches)\n",
      "              -> Acc=70.41% (best=71.58% at epoch 10) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=0.8817 (98 batches)\n",
      "          Eval  Epoch 12: acc=72.41% loss=0.7856 (20 batches)\n",
      "              -> Acc=72.41% (best=72.41% at epoch 12) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=0.8656 (98 batches)\n",
      "          Eval  Epoch 13: acc=73.66% loss=0.7663 (20 batches)\n",
      "              -> Acc=73.66% (best=73.66% at epoch 13) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=0.8520 (98 batches)\n",
      "          Eval  Epoch 14: acc=73.40% loss=0.7782 (20 batches)\n",
      "              -> Acc=73.40% (best=73.66% at epoch 13) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=0.8358 (98 batches)\n",
      "          Eval  Epoch 15: acc=74.19% loss=0.7381 (20 batches)\n",
      "              -> Acc=74.19% (best=74.19% at epoch 15) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=0.8287 (98 batches)\n",
      "          Eval  Epoch 16: acc=73.99% loss=0.7658 (20 batches)\n",
      "              -> Acc=73.99% (best=74.19% at epoch 15) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 17: loss=0.8196 (98 batches)\n",
      "          Eval  Epoch 17: acc=74.21% loss=0.7592 (20 batches)\n",
      "              -> Acc=74.21% (best=74.21% at epoch 17) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 18: loss=0.8156 (98 batches)\n",
      "          Eval  Epoch 18: acc=74.27% loss=0.7375 (20 batches)\n",
      "              -> Acc=74.27% (best=74.27% at epoch 18) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 19: loss=0.7968 (98 batches)\n",
      "          Eval  Epoch 19: acc=74.57% loss=0.7293 (20 batches)\n",
      "              -> Acc=74.57% (best=74.57% at epoch 19) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 20: loss=0.7815 (98 batches)\n",
      "          Eval  Epoch 20: acc=75.64% loss=0.6995 (20 batches)\n",
      "              -> Acc=75.64% (best=75.64% at epoch 20) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 21: loss=0.7821 (98 batches)\n",
      "          Eval  Epoch 21: acc=72.36% loss=0.8082 (20 batches)\n",
      "              -> Acc=72.36% (best=75.64% at epoch 20) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 22: loss=0.7654 (98 batches)\n",
      "          Eval  Epoch 22: acc=76.73% loss=0.6883 (20 batches)\n",
      "              -> Acc=76.73% (best=76.73% at epoch 22) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 23: loss=0.7545 (98 batches)\n",
      "          Eval  Epoch 23: acc=74.93% loss=0.7193 (20 batches)\n",
      "              -> Acc=74.93% (best=76.73% at epoch 22) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 24: loss=0.7434 (98 batches)\n",
      "          Eval  Epoch 24: acc=75.97% loss=0.6928 (20 batches)\n",
      "              -> Acc=75.97% (best=76.73% at epoch 22) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 25: loss=0.7402 (98 batches)\n",
      "          Eval  Epoch 25: acc=77.14% loss=0.6712 (20 batches)\n",
      "              -> Acc=77.14% (best=77.14% at epoch 25) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 26: loss=0.7322 (98 batches)\n",
      "          Eval  Epoch 26: acc=76.04% loss=0.7029 (20 batches)\n",
      "              -> Acc=76.04% (best=77.14% at epoch 25) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 27: loss=0.7185 (98 batches)\n",
      "          Eval  Epoch 27: acc=77.13% loss=0.6645 (20 batches)\n",
      "              -> Acc=77.13% (best=77.14% at epoch 25) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 28: loss=0.7224 (98 batches)\n",
      "          Eval  Epoch 28: acc=75.69% loss=0.7096 (20 batches)\n",
      "              -> Acc=75.69% (best=77.14% at epoch 25) patience_left=1\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 29: loss=0.7141 (98 batches)\n",
      "          Eval  Epoch 29: acc=74.76% loss=0.7343 (20 batches)\n",
      "              -> Acc=74.76% (best=77.14% at epoch 25) patience_left=0\n",
      "              Early stopping triggered (no significant improvement)\n",
      "      Final fitness for d46dff3a: 77.14% (best epoch 25)\n",
      "      New best fitness in this generation: 77.14%!\n",
      "      Fitness obtained: 77.14% | Best so far: 77.14%\n",
      "\n",
      "   Evaluating individual 2/30 (ID: 868b1e52)\n",
      "      Architecture: 1 conv + 5 fc, opt=adam, lr=0.0001\n",
      "      Training/Evaluating model 868b1e52 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=1.8983 (98 batches)\n",
      "          Eval  Epoch 1: acc=41.53% loss=1.6101 (20 batches)\n",
      "              -> Acc=41.53% (best=41.53% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.6264 (98 batches)\n",
      "          Eval  Epoch 2: acc=47.77% loss=1.4520 (20 batches)\n",
      "              -> Acc=47.77% (best=47.77% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.5002 (98 batches)\n",
      "          Eval  Epoch 3: acc=49.74% loss=1.3765 (20 batches)\n",
      "              -> Acc=49.74% (best=49.74% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.4230 (98 batches)\n",
      "          Eval  Epoch 4: acc=52.58% loss=1.3061 (20 batches)\n",
      "              -> Acc=52.58% (best=52.58% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.3606 (98 batches)\n",
      "          Eval  Epoch 5: acc=55.27% loss=1.2385 (20 batches)\n",
      "              -> Acc=55.27% (best=55.27% at epoch 5) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.3048 (98 batches)\n",
      "          Eval  Epoch 6: acc=57.55% loss=1.1670 (20 batches)\n",
      "              -> Acc=57.55% (best=57.55% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=1.2637 (98 batches)\n",
      "          Eval  Epoch 7: acc=58.37% loss=1.1545 (20 batches)\n",
      "              -> Acc=58.37% (best=58.37% at epoch 7) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=1.2245 (98 batches)\n",
      "          Eval  Epoch 8: acc=58.80% loss=1.1296 (20 batches)\n",
      "              -> Acc=58.80% (best=58.80% at epoch 8) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=1.1925 (98 batches)\n",
      "          Eval  Epoch 9: acc=60.72% loss=1.0869 (20 batches)\n",
      "              -> Acc=60.72% (best=60.72% at epoch 9) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=1.1681 (98 batches)\n",
      "          Eval  Epoch 10: acc=61.33% loss=1.0713 (20 batches)\n",
      "              -> Acc=61.33% (best=61.33% at epoch 10) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=1.1484 (98 batches)\n",
      "          Eval  Epoch 11: acc=62.45% loss=1.0412 (20 batches)\n",
      "              -> Acc=62.45% (best=62.45% at epoch 11) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=1.1267 (98 batches)\n",
      "          Eval  Epoch 12: acc=63.22% loss=1.0186 (20 batches)\n",
      "              -> Acc=63.22% (best=63.22% at epoch 12) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=1.1154 (98 batches)\n",
      "          Eval  Epoch 13: acc=64.43% loss=1.0052 (20 batches)\n",
      "              -> Acc=64.43% (best=64.43% at epoch 13) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=1.0928 (98 batches)\n",
      "          Eval  Epoch 14: acc=65.85% loss=0.9630 (20 batches)\n",
      "              -> Acc=65.85% (best=65.85% at epoch 14) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=1.0742 (98 batches)\n",
      "          Eval  Epoch 15: acc=66.96% loss=0.9457 (20 batches)\n",
      "              -> Acc=66.96% (best=66.96% at epoch 15) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=1.0657 (98 batches)\n",
      "          Eval  Epoch 16: acc=66.29% loss=0.9677 (20 batches)\n",
      "              -> Acc=66.29% (best=66.96% at epoch 15) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 17: loss=1.0430 (98 batches)\n",
      "          Eval  Epoch 17: acc=66.94% loss=0.9418 (20 batches)\n",
      "              -> Acc=66.94% (best=66.96% at epoch 15) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 18: loss=1.0379 (98 batches)\n",
      "          Eval  Epoch 18: acc=67.34% loss=0.9283 (20 batches)\n",
      "              -> Acc=67.34% (best=67.34% at epoch 18) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 19: loss=1.0248 (98 batches)\n",
      "          Eval  Epoch 19: acc=67.48% loss=0.9242 (20 batches)\n",
      "              -> Acc=67.48% (best=67.48% at epoch 19) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 20: loss=1.0127 (98 batches)\n",
      "          Eval  Epoch 20: acc=67.80% loss=0.9062 (20 batches)\n",
      "              -> Acc=67.80% (best=67.80% at epoch 20) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 21: loss=0.9986 (98 batches)\n",
      "          Eval  Epoch 21: acc=68.99% loss=0.8867 (20 batches)\n",
      "              -> Acc=68.99% (best=68.99% at epoch 21) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 22: loss=0.9851 (98 batches)\n",
      "          Eval  Epoch 22: acc=68.93% loss=0.8842 (20 batches)\n",
      "              -> Acc=68.93% (best=68.99% at epoch 21) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 23: loss=0.9783 (98 batches)\n",
      "          Eval  Epoch 23: acc=69.13% loss=0.8843 (20 batches)\n",
      "              -> Acc=69.13% (best=69.13% at epoch 23) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 24: loss=0.9759 (98 batches)\n",
      "          Eval  Epoch 24: acc=69.41% loss=0.8727 (20 batches)\n",
      "              -> Acc=69.41% (best=69.41% at epoch 24) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 25: loss=0.9607 (98 batches)\n",
      "          Eval  Epoch 25: acc=68.71% loss=0.8833 (20 batches)\n",
      "              -> Acc=68.71% (best=69.41% at epoch 24) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 26: loss=0.9512 (98 batches)\n",
      "          Eval  Epoch 26: acc=70.02% loss=0.8540 (20 batches)\n",
      "              -> Acc=70.02% (best=70.02% at epoch 26) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 27: loss=0.9384 (98 batches)\n",
      "          Eval  Epoch 27: acc=70.42% loss=0.8445 (20 batches)\n",
      "              -> Acc=70.42% (best=70.42% at epoch 27) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 28: loss=0.9352 (98 batches)\n",
      "          Eval  Epoch 28: acc=70.33% loss=0.8357 (20 batches)\n",
      "              -> Acc=70.33% (best=70.42% at epoch 27) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 29: loss=0.9270 (98 batches)\n",
      "          Eval  Epoch 29: acc=71.23% loss=0.8255 (20 batches)\n",
      "              -> Acc=71.23% (best=71.23% at epoch 29) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 30: loss=0.9162 (98 batches)\n",
      "          Eval  Epoch 30: acc=71.03% loss=0.8220 (20 batches)\n",
      "              -> Acc=71.03% (best=71.23% at epoch 29) patience_left=3\n",
      "      Final fitness for 868b1e52: 71.23% (best epoch 29)\n",
      "      Fitness obtained: 71.23% | Best so far: 77.14%\n",
      "\n",
      "   Evaluating individual 3/30 (ID: a668a41b)\n",
      "      Architecture: 1 conv + 1 fc, opt=adam, lr=0.005\n",
      "      Training/Evaluating model a668a41b (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=7.9015 (98 batches)\n",
      "          Eval  Epoch 1: acc=29.76% loss=2.1229 (20 batches)\n",
      "              -> Acc=29.76% (best=29.76% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.8081 (98 batches)\n",
      "          Eval  Epoch 2: acc=35.88% loss=1.7624 (20 batches)\n",
      "              -> Acc=35.88% (best=35.88% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.6783 (98 batches)\n",
      "          Eval  Epoch 3: acc=43.13% loss=1.6148 (20 batches)\n",
      "              -> Acc=43.13% (best=43.13% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.6113 (98 batches)\n",
      "          Eval  Epoch 4: acc=45.08% loss=1.8322 (20 batches)\n",
      "              -> Acc=45.08% (best=45.08% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 5: loss=1.5650 (98 batches)\n",
      "          Eval  Epoch 5: acc=46.87% loss=1.4610 (20 batches)\n",
      "              -> Acc=46.87% (best=46.87% at epoch 5) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 6: loss=1.4939 (98 batches)\n",
      "          Eval  Epoch 6: acc=47.25% loss=1.4427 (20 batches)\n",
      "              -> Acc=47.25% (best=47.25% at epoch 6) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 7: loss=1.4772 (98 batches)\n",
      "          Eval  Epoch 7: acc=50.47% loss=1.4036 (20 batches)\n",
      "              -> Acc=50.47% (best=50.47% at epoch 7) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 8: loss=1.4453 (98 batches)\n",
      "          Eval  Epoch 8: acc=50.80% loss=1.3963 (20 batches)\n",
      "              -> Acc=50.80% (best=50.80% at epoch 8) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 9: loss=1.4596 (98 batches)\n",
      "          Eval  Epoch 9: acc=49.43% loss=1.5259 (20 batches)\n",
      "              -> Acc=49.43% (best=50.80% at epoch 8) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 10: loss=1.4391 (98 batches)\n",
      "          Eval  Epoch 10: acc=53.19% loss=1.3164 (20 batches)\n",
      "              -> Acc=53.19% (best=53.19% at epoch 10) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 11: loss=1.3785 (98 batches)\n",
      "          Eval  Epoch 11: acc=54.41% loss=1.2762 (20 batches)\n",
      "              -> Acc=54.41% (best=54.41% at epoch 11) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 12: loss=1.3664 (98 batches)\n",
      "          Eval  Epoch 12: acc=53.15% loss=1.4659 (20 batches)\n",
      "              -> Acc=53.15% (best=54.41% at epoch 11) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 13: loss=1.3484 (98 batches)\n",
      "          Eval  Epoch 13: acc=56.51% loss=1.2384 (20 batches)\n",
      "              -> Acc=56.51% (best=56.51% at epoch 13) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 14: loss=1.3169 (98 batches)\n",
      "          Eval  Epoch 14: acc=55.81% loss=1.2307 (20 batches)\n",
      "              -> Acc=55.81% (best=56.51% at epoch 13) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 15: loss=1.3218 (98 batches)\n",
      "          Eval  Epoch 15: acc=57.31% loss=1.2037 (20 batches)\n",
      "              -> Acc=57.31% (best=57.31% at epoch 15) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 16: loss=1.2998 (98 batches)\n",
      "          Eval  Epoch 16: acc=57.26% loss=1.2078 (20 batches)\n",
      "              -> Acc=57.26% (best=57.31% at epoch 15) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 17: loss=1.2872 (98 batches)\n",
      "          Eval  Epoch 17: acc=57.94% loss=1.2243 (20 batches)\n",
      "              -> Acc=57.94% (best=57.94% at epoch 17) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 18: loss=1.2687 (98 batches)\n",
      "          Eval  Epoch 18: acc=58.70% loss=1.2036 (20 batches)\n",
      "              -> Acc=58.70% (best=58.70% at epoch 18) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 19: loss=1.2599 (98 batches)\n",
      "          Eval  Epoch 19: acc=56.47% loss=1.2873 (20 batches)\n",
      "              -> Acc=56.47% (best=58.70% at epoch 18) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 20: loss=1.2568 (98 batches)\n",
      "          Eval  Epoch 20: acc=59.53% loss=1.1542 (20 batches)\n",
      "              -> Acc=59.53% (best=59.53% at epoch 20) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 21: loss=1.2488 (98 batches)\n",
      "          Eval  Epoch 21: acc=59.06% loss=1.1630 (20 batches)\n",
      "              -> Acc=59.06% (best=59.53% at epoch 20) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 22: loss=1.2471 (98 batches)\n",
      "          Eval  Epoch 22: acc=58.50% loss=1.1606 (20 batches)\n",
      "              -> Acc=58.50% (best=59.53% at epoch 20) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 23: loss=1.2238 (98 batches)\n",
      "          Eval  Epoch 23: acc=60.45% loss=1.1126 (20 batches)\n",
      "              -> Acc=60.45% (best=60.45% at epoch 23) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 24: loss=1.2297 (98 batches)\n",
      "          Eval  Epoch 24: acc=58.29% loss=1.2109 (20 batches)\n",
      "              -> Acc=58.29% (best=60.45% at epoch 23) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 25: loss=1.2062 (98 batches)\n",
      "          Eval  Epoch 25: acc=58.04% loss=1.1990 (20 batches)\n",
      "              -> Acc=58.04% (best=60.45% at epoch 23) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 26: loss=1.2161 (98 batches)\n",
      "          Eval  Epoch 26: acc=61.05% loss=1.1075 (20 batches)\n",
      "              -> Acc=61.05% (best=61.05% at epoch 26) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 27: loss=1.1934 (98 batches)\n",
      "          Eval  Epoch 27: acc=62.37% loss=1.0952 (20 batches)\n",
      "              -> Acc=62.37% (best=62.37% at epoch 27) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 28: loss=1.1841 (98 batches)\n",
      "          Eval  Epoch 28: acc=59.65% loss=1.1577 (20 batches)\n",
      "              -> Acc=59.65% (best=62.37% at epoch 27) patience_left=3\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 29: loss=1.1673 (98 batches)\n",
      "          Eval  Epoch 29: acc=62.29% loss=1.0829 (20 batches)\n",
      "              -> Acc=62.29% (best=62.37% at epoch 27) patience_left=2\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 30: loss=1.1678 (98 batches)\n",
      "          Eval  Epoch 30: acc=63.76% loss=1.0477 (20 batches)\n",
      "              -> Acc=63.76% (best=63.76% at epoch 30) patience_left=4\n",
      "      Final fitness for a668a41b: 63.76% (best epoch 30)\n",
      "      Fitness obtained: 63.76% | Best so far: 77.14%\n",
      "\n",
      "   Evaluating individual 4/30 (ID: f5d25ac0)\n",
      "      Architecture: 3 conv + 5 fc, opt=rmsprop, lr=0.0001\n",
      "      Training/Evaluating model f5d25ac0 (max 30 epochs interleaved)\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 1: loss=1.9414 (98 batches)\n",
      "          Eval  Epoch 1: acc=22.90% loss=2.1711 (20 batches)\n",
      "              -> Acc=22.90% (best=22.90% at epoch 1) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 2: loss=1.5795 (98 batches)\n",
      "          Eval  Epoch 2: acc=27.58% loss=1.9943 (20 batches)\n",
      "              -> Acc=27.58% (best=27.58% at epoch 2) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 3: loss=1.4048 (98 batches)\n",
      "          Eval  Epoch 3: acc=36.45% loss=1.7508 (20 batches)\n",
      "              -> Acc=36.45% (best=36.45% at epoch 3) patience_left=4\n",
      "USING DEVICE cuda\n",
      "          Train Epoch 4: loss=1.2877 (98 batches)\n",
      "          Eval  Epoch 4: acc=41.87% loss=1.6996 (20 batches)\n",
      "              -> Acc=41.87% (best=41.87% at epoch 4) patience_left=4\n",
      "USING DEVICE cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACIÓN DE DATASET - MODIFICAR AQUÍ\n",
    "# ==========================================\n",
    "\n",
    "# Para cambiar el dataset, modifica la línea correspondiente y ejecuta esta celda:\n",
    "\n",
    "# Opción 1: Usar MNIST (28x28, grayscale, 10 classes)\n",
    "# CONFIG['dataset'] = 'MNIST'\n",
    "\n",
    "# Opción 2: Usar CIFAR-10 (32x32, RGB, 10 classes) - RECOMENDADO para mayor challenge\n",
    "CONFIG['dataset'] = 'CIFAR10'\n",
    "\n",
    "# Opción 3: Usar dataset personalizado\n",
    "# CONFIG['dataset'] = 'CUSTOM'\n",
    "# CONFIG['dataset_path'] = r'E:\\Neuroevolution\\data\\phd_data'  # Ajustar ruta según tu dataset\n",
    "\n",
    "# ==========================================\n",
    "# OTRAS CONFIGURACIONES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# Reconfigurar el dataset con los nuevos parámetros\n",
    "CONFIG = configure_dataset(CONFIG, CONFIG['dataset'])\n",
    "\n",
    "print(\"Current configuration:\")\n",
    "print(f\"   Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"   Image size: {CONFIG['px_h']}x{CONFIG['px_w']}x{CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Recargar el dataset con la nueva configuración\n",
    "print(f\"\\nReloading dataset with new configuration...\")\n",
    "train_loader, test_loader = load_dataset(CONFIG)\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Create system instance\n",
    "neuroevolution = HybridNeuroevolution(CONFIG, train_loader, test_loader)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nProcess completed at {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 8. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   • Errors in model architecture\")\n",
    "        print(f\"   • Memory problems (GPU/RAM)\")\n",
    "        print(f\"   • Invalid hyperparameter configurations\")\n",
    "        print(f\"   • Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"        BEST EVOLVED ARCHITECTURE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Convolutional Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS:\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv{i+1}: {filters} filters, kernel {kernel}x{kernel}, activation {activation}\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (classes)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.4f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(best_genome['activations'])}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Convolutional layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   TARGET: OK Fitness objective REACHED ({best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   TARGET: ERROR Fitness objective NOT reached ({best_genome['fitness']:.2f}% < {config['fitness_threshold']}%)\")\n",
    "    \n",
    "    print(f\"   TIME: Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'config_used': config,\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, defau lt=str)\n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\nHYBRID NEUROEVOLUTION COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

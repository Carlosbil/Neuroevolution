{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Audio Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements a hybrid neuroevolution process for audio classification (Parkinson detection). The system combines genetic algorithms with 1D convolutional neural networks to evolve optimal architectures for audio processing.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **1D Convolutional Networks**: Optimized for audio waveform processing\n",
    "- **Parallel 5-Fold Cross-Validation**: Each individual is evaluated on all 5 folds IN PARALLEL (fitness = average accuracy)\n",
    "- **Multi-threading**: Folds are trained simultaneously in separate threads for faster evaluation\n",
    "- **Adaptive mutation**: Dynamic mutation rate based on population diversity\n",
    "- **Audio dataset support**: Loads .npy files with train/val/test splits\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of 1D CNN architectures\n",
    "2. Evaluate fitness of each individual using **parallel 5-fold CV** (robust and faster with threading)\n",
    "3. Select best architectures (elitism)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found for Parkinson classification\n",
    "\n",
    "**‚úÖ Performance**: Multi-threaded 5-fold CV provides robustness against overfitting while being much faster than sequential training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f49a8",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ú® CONFIGURACI√ìN ACTUAL DEL DATASET ‚ú®\n",
    "\n",
    "**Dataset configurado**: `files_all_real_syn_n` (Datos Reales + Sint√©ticos Mezclados)\n",
    "\n",
    "Este notebook est√° configurado para usar el **nuevo dataset** que combina:\n",
    "- üéµ **Datos Reales**: Audios originales de pacientes  \n",
    "- ü§ñ **Datos Sint√©ticos**: Audios generados por GANs (BigVSAN 40_1e5)\n",
    "\n",
    "**Ventajas de este dataset**:\n",
    "- Mayor diversidad de datos para entrenamiento\n",
    "- Combina la autenticidad de datos reales con la variedad de datos generados\n",
    "- Ideal para mejorar la generalizaci√≥n del modelo\n",
    "- Estratificaci√≥n balanceada entre clases (control/patol√≥gico)\n",
    "\n",
    "**üöÄ Parallel 5-Fold Cross-Validation durante la Evoluci√≥n**: \n",
    "- **CADA** individuo se eval√∫a en **TODOS** los 5 folds **EN PARALELO**\n",
    "- Los 5 folds se entrenan **simult√°neamente** en threads separados\n",
    "- El fitness es el **promedio** de accuracy de los 5 folds\n",
    "- ‚úÖ **Mucho m√°s r√°pido** que entrenamiento secuencial\n",
    "- ‚úÖ **M√°s robusto** - evita sobreajuste a un fold espec√≠fico\n",
    "\n",
    "**üìä Evaluaci√≥n Final**: \n",
    "- Al terminar la evoluci√≥n, la mejor arquitectura se vuelve a evaluar con 5-fold CV\n",
    "- Se reportan m√©tricas completas (accuracy, sensitivity, specificity, F1, AUC)\n",
    "\n",
    "Para cambiar el dataset, modifica los par√°metros `dataset_id` y `fold_id` en la celda de **Configuraci√≥n** (Secci√≥n 2).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Threading for parallel fold training\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device configured: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded (adaptive mutation enabled, 1D Conv for audio):\n",
      "   Dataset: Audio (Parkinson Classification)\n",
      "   population_size: 20\n",
      "   max_generations: 100\n",
      "   fitness_threshold: 80.0\n",
      "   base_mutation_rate: 0.25\n",
      "   mutation_rate_min: 0.1\n",
      "   mutation_rate_max: 0.8\n",
      "   current_mutation_rate: 0.25\n",
      "   crossover_rate: 0.99\n",
      "   elite_percentage: 0.2\n",
      "   dataset: AUDIO\n",
      "   num_channels: 1\n",
      "   sequence_length: 240000\n",
      "   num_classes: 2\n",
      "   batch_size: 64\n",
      "   test_split: 0.2\n",
      "   num_epochs: 100\n",
      "   learning_rate: 1e-05\n",
      "   early_stopping_patience: 100000\n",
      "   epoch_patience: 10\n",
      "   improvement_threshold: 0.01\n",
      "   early_stopping_generations: 20\n",
      "   min_improvement_threshold: 0.01\n",
      "   min_conv_layers: 1\n",
      "   max_conv_layers: 30\n",
      "   min_fc_layers: 1\n",
      "   max_fc_layers: 10\n",
      "   min_filters: 1\n",
      "   max_filters: 256\n",
      "   min_fc_nodes: 64\n",
      "   max_fc_nodes: 1024\n",
      "   kernel_size_options: [1, 3, 5, 7, 9, 11, 13, 15]\n",
      "   min_dropout: 0.2\n",
      "   max_dropout: 0.6\n",
      "   learning_rate_options: [0.001, 0.0005, 0.0001, 5e-05, 1e-05, 0.01, 0.1, 1e-05]\n",
      "   normalization_batch_weight: 0.8\n",
      "   normalization_layer_weight: 0.2\n",
      "   dataset_id: 40_1e5_N\n",
      "   fold_id: 40_1e5_N\n",
      "   num_folds: 5\n",
      "   data_path: data\\sets\\folds_5\n",
      "   fold_files_subdirectory: files_real_40_1e5_N\n",
      "\n",
      "Available activation functions: ['relu', 'leaky_relu', 'tanh', 'sigmoid', 'selu']\n",
      "Available optimizers: ['adam', 'adamw', 'sgd', 'rmsprop']\n",
      "\n",
      "OS-independent path configured: data\\sets\\folds_5\n"
     ]
    }
   ],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 20,            # Population size\n",
    "    'max_generations': 100,            # Maximum number of generations\n",
    "    'fitness_threshold': 80.0,        # Target fitness (% accuracy) - Adjusted for audio\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.25,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.25,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection (AUDIO ONLY)\n",
    "    'dataset': 'AUDIO',               # Audio dataset for Parkinson classification\n",
    "\n",
    "    # Dataset parameters for audio\n",
    "    'num_channels': 1,                # Input channels (1 for audio waveform)\n",
    "    'sequence_length': 240000,        # Audio sequence length (will be auto-detected)\n",
    "    'num_classes': 2,                 # Number of classes (control vs pathological)\n",
    "    'batch_size': 64,                 # Batch size for audio\n",
    "    'test_split': 0.2,                # Validation percentage\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 100,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.00001,           # Base learning rate\n",
    "    'early_stopping_patience': 100000,   # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping\n",
    "    'epoch_patience': 10,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.01,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Generation-level early stopping \n",
    "    'early_stopping_generations': 20, # Stop if no improvement in X generations\n",
    "    'min_improvement_threshold': 0.01, # Minimum fitness improvement (%) to reset counter\n",
    "\n",
    "    # Allowed architecture range for 1D Conv\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 30,             # Less layers for 1D audio\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 10,               # Less FC layers\n",
    "    'min_filters': 1,\n",
    "    'max_filters': 256,               # Adjusted for 1D\n",
    "    'min_fc_nodes': 64,\n",
    "    'max_fc_nodes': 1024,              # Smaller for audio classification\n",
    "\n",
    "    # Mutation parameters - Kernel sizes for 1D Conv\n",
    "    'kernel_size_options': [1, 3, 5, 7, 9, 11, 13, 15],  # Available kernel sizes for Conv1D\n",
    "    \n",
    "    # Mutation parameters - Dropout range\n",
    "    'min_dropout': 0.2,               # Minimum dropout rate\n",
    "    'max_dropout': 0.6,               # Maximum dropout rate\n",
    "    \n",
    "    # Mutation parameters - Learning rate options\n",
    "    'learning_rate_options': [0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.01, 0.1, 0.00001 ],  # Available learning rates\n",
    "    \n",
    "    # Mutation parameters - Normalization type weights\n",
    "    'normalization_batch_weight': 0.8,  # Probability to use batch normalization\n",
    "    'normalization_layer_weight': 0.2,  # Probability to use layer normalization\n",
    "\n",
    "    # Audio dataset configuration (OS-independent paths)\n",
    "    \n",
    "    'dataset_id': '40_1e5_N',   # Dataset ID - Mixed real + synthetic data\n",
    "    'fold_id': '40_1e5_N',      # Fold ID for files\n",
    "    'num_folds': 5,                   # Number of folds (all used during evolution)\n",
    "    'data_path': os.path.join('data', 'sets', 'folds_5'),  # OS-independent path\n",
    "    'fold_files_subdirectory': 'files_real_40_1e5_N',  # Subdirectory containing fold .npy files\n",
    "    'normalization': {'mean': (0.0,), 'std': (1.0,)}  # Audio normalization\n",
    "}\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled, 1D Conv for audio):\")\n",
    "print(f\"   Dataset: Audio (Parkinson Classification)\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key not in ['normalization']:  # Hide normalization details\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"\\nOS-independent path configured: {CONFIG['data_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d2e08",
   "metadata": {},
   "source": [
    "### üìã Par√°metros de Mutaci√≥n Configurables\n",
    "\n",
    "**Todos los par√°metros de mutaci√≥n ahora son configurables desde `CONFIG`**:\n",
    "\n",
    "#### Kernel Sizes (Tama√±os de kernel para Conv1D)\n",
    "- **`kernel_size_options`**: `[3, 5, 7, 9, 11, 13, 15]`\n",
    "  - Opciones disponibles para los tama√±os de kernel en capas convolucionales 1D\n",
    "  - Se usa tanto en la creaci√≥n inicial como en la mutaci√≥n\n",
    "\n",
    "#### Dropout Range (Rango de dropout)\n",
    "- **`min_dropout`**: `0.2` - Tasa m√≠nima de dropout\n",
    "- **`max_dropout`**: `0.6` - Tasa m√°xima de dropout\n",
    "  - Durante la creaci√≥n y mutaci√≥n, el dropout se selecciona aleatoriamente dentro de este rango\n",
    "\n",
    "#### Learning Rate Options (Opciones de learning rate)\n",
    "- **`learning_rate_options`**: `[0.001, 0.0005, 0.0001, 0.00005, 0.00001]`\n",
    "  - Opciones disponibles para el learning rate\n",
    "  - Se selecciona aleatoriamente de esta lista durante creaci√≥n y mutaci√≥n\n",
    "\n",
    "#### Normalization Type Weights (Pesos para tipo de normalizaci√≥n)\n",
    "- **`normalization_batch_weight`**: `0.8` - Probabilidad de usar batch normalization (80%)\n",
    "- **`normalization_layer_weight`**: `0.2` - Probabilidad de usar layer normalization (20%)\n",
    "  - Durante la mutaci√≥n, se selecciona el tipo de normalizaci√≥n con estas probabilidades\n",
    "\n",
    "‚úÖ **Beneficio**: Ahora puedes ajustar todos estos par√°metros desde un solo lugar (CONFIG) sin modificar las funciones de mutaci√≥n o creaci√≥n de genomas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### Informaci√≥n sobre el Dataset de Audio\n",
    "\n",
    "**Dataset de Audio para Clasificaci√≥n de Parkinson**: \n",
    "- Archivos de audio de voz (clasificaci√≥n Parkinson)\n",
    "- Datos 1D de forma de onda procesada\n",
    "- Estructura: archivos .npy con train/val/test splits\n",
    "- Dificultad: **Alta** - Clasificaci√≥n m√©dica\n",
    "- Fitness objetivo recomendado: >85%\n",
    "- Clases: Control vs Pathological\n",
    "- Formato de archivos: `{data_path}/files_{fold_id}/X_train_{dataset_id}_fold_{fold}.npy`\n",
    "- Arquitectura: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC Layers\n",
    "\n",
    "**Configuraci√≥n del Dataset:**\n",
    "- Modifica los par√°metros en la celda de configuraci√≥n:\n",
    "  - `dataset_id`: ID del dataset (ej: 'all_real_syn_n')\n",
    "  - `fold_id`: ID de la carpeta de folds (ej: 'all_real_syn_n')\n",
    "  - `data_path`: Ruta base a los datos (usa `os.path.join` para compatibilidad multiplataforma)\n",
    "\n",
    "**üîÑ Uso de 5-Fold CV:**\n",
    "- **Durante la evoluci√≥n**: Cada individuo se eval√∫a en los 5 folds autom√°ticamente\n",
    "- **No se necesita** especificar `current_fold` (se usan todos)\n",
    "- El fitness es el **promedio** de los 5 folds\n",
    "\n",
    "**Nota sobre Rutas:**\n",
    "- Las rutas son **independientes del sistema operativo** (Windows/Linux/Mac)\n",
    "- Usa `os.path.join()` para construir rutas compatibles\n",
    "- Ejemplo: `os.path.join('data', 'sets', 'folds_5')` funciona en cualquier OS\n",
    "\n",
    "---\n",
    "\n",
    "### Tipos de Carpetas de Folds Disponibles (generadas por `create_5_folds.ipynb`)\n",
    "\n",
    "El notebook `generating_csv/create_5_folds.ipynb` genera **5 tipos de carpetas** con diferentes combinaciones de datos **reales** y **sint√©ticos** (generados por GANs) para experimentaci√≥n:\n",
    "\n",
    "#### 1. **`files_real_N`** - Solo Datos Reales\n",
    "   - **Train**: Datos reales (`test_together_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Baseline con datos 100% reales\n",
    "   - **fold_id**: `'real_N'`\n",
    "   - **dataset_id**: `'real_N'`\n",
    "\n",
    "#### 2. **`files_real_40_1e5_N`** - Entrenamiento Sint√©tico, Test Real\n",
    "   - **Train**: Datos sint√©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Evaluar si modelos entrenados con sint√©ticos generalizan a datos reales\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 3. **`files_syn_40_1e5_N`** - Solo Datos Sint√©ticos (mismo conjunto)\n",
    "   - **Train**: Datos sint√©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos sint√©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Uso**: Evaluar capacidad del modelo con datos 100% sint√©ticos\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 4. **`files_syn_1_N`** - Entrenamiento Sint√©tico, Test Sint√©tico Diferente\n",
    "   - **Train**: Datos sint√©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos sint√©ticos diferentes (`test_together_syn_1_N`)\n",
    "   - **Uso**: Evaluar generalizaci√≥n entre diferentes conjuntos sint√©ticos\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 5. **`files_syn_all_N`** - Solo Datos Reales (mal nombrado probablemente)\n",
    "   - **Train**: Datos reales (`test_together_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Similar a `files_real_N` (posible duplicado o error de nomenclatura)\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 6. **`files_all_real_syn_n`** - ‚ú® Datos Reales + Sint√©ticos Mezclados ‚ú® **(NUEVO)**\n",
    "   - **Train**: Datos reales + sint√©ticos mezclados\n",
    "   - **Validation**: Datos reales + sint√©ticos mezclados\n",
    "   - **Test**: Datos reales + sint√©ticos mezclados\n",
    "   - **Uso**: Entrenar y evaluar con una mezcla equilibrada de datos reales y generados por GANs\n",
    "   - **fold_id**: `'all_real_syn_n'`\n",
    "   - **dataset_id**: `'all_real_syn_n'`\n",
    "   - **Ventajas**: Combina diversidad de datos sint√©ticos con autenticidad de datos reales\n",
    "   - **Configuraci√≥n actual**: üîµ **ESTE ES EL DATASET CONFIGURADO POR DEFECTO**\n",
    "\n",
    "**Nota**: Cada carpeta contiene 5 folds de validaci√≥n cruzada con:\n",
    "- `X_train_{dataset_id}_fold_{1-5}.npy` y `y_train_{dataset_id}_fold_{1-5}.npy`\n",
    "- `X_val_{dataset_id}_fold_{1-5}.npy` y `y_val_{dataset_id}_fold_{1-5}.npy`\n",
    "- `X_test_{dataset_id}_fold_{1-5}.npy` y `y_test_{dataset_id}_fold_{1-5}.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICANDO DISPONIBILIDAD DE DATOS\n",
      "============================================================\n",
      "Dataset ID: 40_1e5_N, Verificando los 5 folds...\n",
      "   Looking for: e:\\Neuroevolution\\data\\sets\\folds_5\\files_real_40_1e5_N\n",
      "   ‚úì Directory found: e:\\Neuroevolution\\data\\sets\\folds_5\\files_real_40_1e5_N\n",
      "\n",
      "Checking for all 5 folds...\n",
      "   ‚úì Fold 1: All files present\n",
      "   ‚úì Fold 2: All files present\n",
      "   ‚úì Fold 3: All files present\n",
      "   ‚úì Fold 4: All files present\n",
      "   ‚úì Fold 5: All files present\n",
      "\n",
      "‚úì All 5 folds verified successfully!\n",
      "\n",
      "Loading Fold 1 to detect sequence length...\n",
      "   Train samples: (7200, 11520)\n",
      "   Sequence length detected: 11520\n",
      "\n",
      "‚úì Dataset verification complete!\n",
      "   During evolution, each individual will train on all 5 folds.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DATASET READY FOR 5-FOLD CROSS-VALIDATION\n",
      "============================================================\n",
      "   Sequence length: 11520\n",
      "   Input channels: 1\n",
      "   Number of classes: 2\n",
      "   Batch size: 64\n",
      "   Audio classification task: Control (0) vs Pathological (1)\n",
      "\n",
      "   ‚ö†Ô∏è Each individual will be evaluated on ALL 5 folds\n",
      "   ‚ö†Ô∏è This makes evolution ~5x slower but much more robust\n",
      "   Train samples: (7200, 11520)\n",
      "   Sequence length detected: 11520\n",
      "\n",
      "‚úì Dataset verification complete!\n",
      "   During evolution, each individual will train on all 5 folds.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DATASET READY FOR 5-FOLD CROSS-VALIDATION\n",
      "============================================================\n",
      "   Sequence length: 11520\n",
      "   Input channels: 1\n",
      "   Number of classes: 2\n",
      "   Batch size: 64\n",
      "   Audio classification task: Control (0) vs Pathological (1)\n",
      "\n",
      "   ‚ö†Ô∏è Each individual will be evaluated on ALL 5 folds\n",
      "   ‚ö†Ô∏è This makes evolution ~5x slower but much more robust\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the audio dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading audio dataset from: {config['data_path']}\")\n",
    "    print(f\"Dataset ID: {config['dataset_id']}, Fold: {config['current_fold']}\")\n",
    "    \n",
    "    # Construct paths following ResNet convention\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_real_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    # Check if directory exists\n",
    "    print(f\"\\nChecking data directory...\")\n",
    "    print(f\"   Looking for: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    if not os.path.exists(fold_files_directory):\n",
    "        print(f\"\\n‚ùå ERROR: Directory not found!\")\n",
    "        print(f\"   Expected: {os.path.abspath(fold_files_directory)}\")\n",
    "        \n",
    "        # Try to find the correct path\n",
    "        possible_paths = [\n",
    "            os.path.join('..', 'data', 'sets', 'folds_5', f\"files_real_{config['fold_id']}\"),\n",
    "            os.path.join('data', 'sets', 'folds_5', f\"files_real_{config['fold_id']}\"),\n",
    "            os.path.join('.', 'data', 'sets', 'folds_5', f\"files_real_{config['fold_id']}\"),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSearching for data in alternative locations:\")\n",
    "        for path in possible_paths:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"   {'‚úì' if exists else '‚úó'} {abs_path}\")\n",
    "            if exists:\n",
    "                fold_files_directory = path\n",
    "                print(f\"\\n‚úì Found data at: {os.path.abspath(fold_files_directory)}\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\n‚ùå Could not find data directory!\\n\"\n",
    "                f\"   Tried paths:\\n\" + \n",
    "                \"\\n\".join([f\"      - {os.path.abspath(p)}\" for p in possible_paths]) +\n",
    "                f\"\\n\\n   Please check:\\n\"\n",
    "                f\"      1. CONFIG['data_path'] is correct\\n\"\n",
    "                f\"      2. The data files exist\\n\"\n",
    "                f\"      3. The fold_files_subdirectory '{config['fold_files_subdirectory']}' is correct\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   ‚úì Directory found: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "def load_dataset(config: dict):\n",
    "    \"\"\"\n",
    "    Verifica que los datos existen y carga el primer fold para detectar sequence_length.\n",
    "    Durante la evoluci√≥n, cada individuo cargar√° todos los folds autom√°ticamente.\n",
    "    \n",
    "    Args:\n",
    "        config: Diccionario de configuraci√≥n\n",
    "    \n",
    "    Returns:\n",
    "        None (solo actualiza config['sequence_length'])\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VERIFICANDO DISPONIBILIDAD DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Dataset ID: {config['dataset_id']}, Verificando los 5 folds...\")\n",
    "    \n",
    "    # Build directory path using the configured subdirectory\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        config['fold_files_subdirectory']\n",
    "    )\n",
    "    \n",
    "    print(f\"   Looking for: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    # If directory not found, try alternative locations\n",
    "    if not os.path.exists(fold_files_directory):\n",
    "        possible_paths = [\n",
    "            os.path.join('..', 'data', 'sets', 'folds_5', config['fold_files_subdirectory']),\n",
    "            os.path.join('data', 'sets', 'folds_5', config['fold_files_subdirectory']),\n",
    "            os.path.join('.', 'data', 'sets', 'folds_5', config['fold_files_subdirectory']),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSearching for data in alternative locations:\")\n",
    "        for path in possible_paths:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"   {'‚úì' if exists else '‚úó'} {abs_path}\")\n",
    "            if exists:\n",
    "                fold_files_directory = path\n",
    "                print(f\"\\n‚úì Found data at: {os.path.abspath(fold_files_directory)}\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\n‚ùå Could not find data directory!\\n\"\n",
    "                f\"   Tried paths:\\n\" + \n",
    "                \"\\n\".join([f\"      - {os.path.abspath(p)}\" for p in possible_paths]) +\n",
    "                f\"\\n\\n   Please check:\\n\"\n",
    "                f\"      1. CONFIG['data_path'] is correct\\n\"\n",
    "                f\"      2. The data files exist\\n\"\n",
    "                f\"      3. The fold_id '{config['fold_id']}' is correct\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   ‚úì Directory found: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    dataset_id = config['dataset_id']\n",
    "    \n",
    "    # Check that all 5 folds exist\n",
    "    print(f\"\\nChecking for all 5 folds...\")\n",
    "    all_folds_ok = True\n",
    "    \n",
    "    for fold_num in range(1, 6):\n",
    "        required_files = [\n",
    "            f'X_train_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_train_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'X_val_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_val_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'X_test_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_test_{dataset_id}_fold_{fold_num}.npy',\n",
    "        ]\n",
    "        \n",
    "        fold_ok = True\n",
    "        for filename in required_files:\n",
    "            filepath = os.path.join(fold_files_directory, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                fold_ok = False\n",
    "                all_folds_ok = False\n",
    "                print(f\"   ‚úó Fold {fold_num}: Missing {filename}\")\n",
    "                break\n",
    "        \n",
    "        if fold_ok:\n",
    "            print(f\"   ‚úì Fold {fold_num}: All files present\")\n",
    "    \n",
    "    if not all_folds_ok:\n",
    "        raise FileNotFoundError(\n",
    "            f\"\\n‚ùå Some fold files are missing!\\n\"\n",
    "            f\"   Please ensure all 5 folds have complete data files.\\n\"\n",
    "            f\"   dataset_id: '{dataset_id}'\\n\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úì All 5 folds verified successfully!\")\n",
    "    \n",
    "    # Load first fold to detect sequence_length\n",
    "    print(f\"\\nLoading Fold 1 to detect sequence length...\")\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_1.npy'))\n",
    "    \n",
    "    print(f\"   Train samples: {x_train.shape}\")\n",
    "    \n",
    "    # Update sequence length from actual data\n",
    "    if len(x_train.shape) == 2:  # (samples, sequence_length)\n",
    "        config['sequence_length'] = x_train.shape[1]\n",
    "    elif len(x_train.shape) == 3:  # Already (samples, channels, sequence_length)\n",
    "        config['sequence_length'] = x_train.shape[2]\n",
    "    \n",
    "    print(f\"   Sequence length detected: {config['sequence_length']}\")\n",
    "    print(f\"\\n‚úì Dataset verification complete!\")\n",
    "    print(f\"   During evolution, each individual will train on all 5 folds.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Verify dataset availability\n",
    "load_dataset(CONFIG)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READY FOR 5-FOLD CROSS-VALIDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Sequence length: {CONFIG['sequence_length']}\")\n",
    "print(f\"   Input channels: {CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Audio classification task: Control (0) vs Pathological (1)\")\n",
    "print(f\"\\n   ‚ö†Ô∏è Each individual will be evaluated on ALL 5 folds\")\n",
    "print(f\"   ‚ö†Ô∏è This makes evolution ~5x slower but much more robust\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvolvableCNN class defined correctly (Conv1D for audio)\n"
     ]
    }
   ],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable CNN class for 1D audio processing.\n",
    "    Uses Conv1D layers for audio/sequential data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        \n",
    "        # Validate and fix genome structure before building\n",
    "        self._validate_genome()\n",
    "        \n",
    "        # Build convolutional layers (1D for audio)\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "    \n",
    "    def _validate_genome(self):\n",
    "        \"\"\"Validates and fixes genome structure to ensure consistency.\"\"\"\n",
    "        # Ensure conv-related lists match num_conv_layers\n",
    "        num_conv = self.genome['num_conv_layers']\n",
    "        \n",
    "        if len(self.genome['filters']) != num_conv:\n",
    "            # Fix filters list\n",
    "            self.genome['filters'] = self.genome['filters'][:num_conv]\n",
    "            while len(self.genome['filters']) < num_conv:\n",
    "                self.genome['filters'].append(\n",
    "                    random.randint(self.config['min_filters'], self.config['max_filters'])\n",
    "                )\n",
    "        \n",
    "        if len(self.genome['kernel_sizes']) != num_conv:\n",
    "            # Fix kernel_sizes list\n",
    "            self.genome['kernel_sizes'] = self.genome['kernel_sizes'][:num_conv]\n",
    "            while len(self.genome['kernel_sizes']) < num_conv:\n",
    "                self.genome['kernel_sizes'].append(\n",
    "                    random.choice(self.config['kernel_size_options'])\n",
    "                )\n",
    "        \n",
    "        # Ensure fc-related lists match num_fc_layers\n",
    "        num_fc = self.genome['num_fc_layers']\n",
    "        \n",
    "        if len(self.genome['fc_nodes']) != num_fc:\n",
    "            # Fix fc_nodes list\n",
    "            self.genome['fc_nodes'] = self.genome['fc_nodes'][:num_fc]\n",
    "            while len(self.genome['fc_nodes']) < num_fc:\n",
    "                self.genome['fc_nodes'].append(\n",
    "                    random.randint(self.config['min_fc_nodes'], self.config['max_fc_nodes'])\n",
    "                )\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds 1D convolutional layers according to genome.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = self.config['num_channels']\n",
    "        normalization_type = self.genome.get('normalization_type', 'batch')  # Default to batch normalization\n",
    "\n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            # Safe access with validation\n",
    "            if i >= len(self.genome['filters']) or i >= len(self.genome['kernel_sizes']):\n",
    "                raise IndexError(\n",
    "                    f\"Genome list mismatch: i={i}, num_conv_layers={self.genome['num_conv_layers']}, \"\n",
    "                    f\"filters_len={len(self.genome['filters'])}, kernel_sizes_len={len(self.genome['kernel_sizes'])}\"\n",
    "                )\n",
    "            \n",
    "            out_channels = self.genome['filters'][i]\n",
    "            kernel_size = self.genome['kernel_sizes'][i]\n",
    "            \n",
    "            # Ensure kernel size is odd and reasonable for 1D\n",
    "            kernel_size = max(3, kernel_size if kernel_size % 2 == 1 else kernel_size + 1)\n",
    "            padding = kernel_size // 2\n",
    "            \n",
    "            # 1D Convolutional layer\n",
    "            conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Normalization layer (Layer Normalization or Batch Normalization)\n",
    "            if normalization_type == 'layer':\n",
    "                # Layer Normalization: normaliza sobre features, no sobre batch\n",
    "                # Para Conv1d output de shape (batch, channels, length), normalizamos los channels\n",
    "                layers.append(nn.LayerNorm(out_channels))\n",
    "            else:\n",
    "                # Batch normalization (default)\n",
    "                layers.append(nn.BatchNorm1d(out_channels))\n",
    "            \n",
    "            # Activation function\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # Max pooling (1D) - reduce sequence length\n",
    "            pool_size = 2 if i < self.genome['num_conv_layers'] - 1 else 2\n",
    "            layers.append(nn.MaxPool1d(pool_size, pool_size))\n",
    "            \n",
    "            # Optional dropout after pooling\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculates output size after convolutional layers.\n",
    "        Raises ValueError if the architecture produces invalid dimensions.\n",
    "        \"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['sequence_length'])\n",
    "        \n",
    "        # Pass through convolutional layers with validation\n",
    "        x = dummy_input\n",
    "        normalization_type = self.genome.get('normalization_type', 'batch')\n",
    "        \n",
    "        try:\n",
    "            # Set model to eval mode to avoid BatchNorm training issues with batch_size=1\n",
    "            self.eval()\n",
    "            \n",
    "            for layer in self.conv_layers:\n",
    "                # Check dimensions before BatchNorm layers\n",
    "                if isinstance(layer, nn.BatchNorm1d) and normalization_type == 'batch':\n",
    "                    # Check if spatial dimension is too small for BatchNorm\n",
    "                    if x.shape[2] <= 1:  # spatial dimension\n",
    "                        raise ValueError(\n",
    "                            f\"Invalid architecture: spatial dimension too small ({x.shape[2]}) \"\n",
    "                            f\"for BatchNorm1d. This genome produces architectures that are too deep. \"\n",
    "                            f\"Genome: num_conv_layers={self.genome['num_conv_layers']}, \"\n",
    "                            f\"sequence_length={self.config['sequence_length']}\"\n",
    "                        )\n",
    "                \n",
    "                x = layer(x)\n",
    "                \n",
    "                # Additional check after each layer\n",
    "                if x.shape[2] < 1:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid architecture: sequence length became zero or negative. \"\n",
    "                        f\"Current shape: {x.shape}, \"\n",
    "                        f\"Genome: num_conv_layers={self.genome['num_conv_layers']}\"\n",
    "                    )\n",
    "            \n",
    "            # Back to training mode\n",
    "            self.train()\n",
    "            \n",
    "        except ValueError as e:\n",
    "            # Re-raise our custom validation errors\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            # Catch any other errors during size calculation\n",
    "            raise ValueError(\n",
    "                f\"Error calculating conv output size: {str(e)}. \"\n",
    "                f\"Genome may produce invalid architecture. \"\n",
    "                f\"num_conv_layers={self.genome['num_conv_layers']}, \"\n",
    "                f\"sequence_length={self.config['sequence_length']}\"\n",
    "            )\n",
    "        \n",
    "        # Flatten and get size\n",
    "        flattened_size = x.view(-1).shape[0]\n",
    "        \n",
    "        # Ensure we have a reasonable output size\n",
    "        if flattened_size < 1:\n",
    "            raise ValueError(\n",
    "                f\"Invalid architecture: flattened size is {flattened_size}. \"\n",
    "                f\"The architecture is too aggressive in dimension reduction.\"\n",
    "            )\n",
    "        \n",
    "        return flattened_size\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        normalization_type = self.genome.get('normalization_type', 'batch')  # Default to batch normalization\n",
    "\n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            # Safe access with validation\n",
    "            if i >= len(self.genome['fc_nodes']):\n",
    "                raise IndexError(\n",
    "                    f\"Genome list mismatch: i={i}, num_fc_layers={self.genome['num_fc_layers']}, \"\n",
    "                    f\"fc_nodes_len={len(self.genome['fc_nodes'])}\"\n",
    "                )\n",
    "            \n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Normalization layer (Layer Normalization or Batch Normalization)\n",
    "            if normalization_type == 'layer':\n",
    "                # Layer Normalization for FC layers\n",
    "                layers.append(nn.LayerNorm(output_size))\n",
    "            else:\n",
    "                # Batch normalization for FC layers (default)\n",
    "                layers.append(nn.BatchNorm1d(output_size))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Ensure input is in correct format for Conv1d\n",
    "        # Expected: (batch, channels, sequence_length)\n",
    "        if len(x.shape) == 2:  # (batch, sequence)\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv1D Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Normalization: {self.genome.get('normalization_type', 'batch')}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly (Conv1D for audio with architecture validation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components\n",
    "\n",
    "### 5.1 Genome Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be19766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì create_random_genome function defined (using configurable parameters)\n"
     ]
    }
   ],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a random genome within specified ranges (optimized for 1D audio, using configurable parameters).\n",
    "    Ensures the genome will produce a valid architecture.\n",
    "    \"\"\"\n",
    "    max_attempts = 100\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt < max_attempts:\n",
    "        # Calculate maximum safe conv layers based on sequence length\n",
    "        sequence_length = config['sequence_length']\n",
    "        min_required_length = 4\n",
    "        max_safe_conv_layers = int(np.log2(sequence_length / min_required_length))\n",
    "        \n",
    "        # Limit conv layers to safe maximum\n",
    "        safe_max_conv = min(config['max_conv_layers'], max_safe_conv_layers)\n",
    "        \n",
    "        # Number of layers\n",
    "        num_conv_layers = random.randint(config['min_conv_layers'], safe_max_conv)\n",
    "        num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "        # Filters for each convolutional layer (progressive increase)\n",
    "        filters = []\n",
    "        base_filters = random.randint(config['min_filters'], config['min_filters'] * 2)\n",
    "        for i in range(num_conv_layers):\n",
    "            # Gradually increase filters in deeper layers\n",
    "            layer_filters = min(base_filters * (2 ** i), config['max_filters'])\n",
    "            filters.append(layer_filters)\n",
    "\n",
    "        # Kernel sizes (using configured options)\n",
    "        kernel_sizes = [random.choice(config['kernel_size_options']) for _ in range(num_conv_layers)]\n",
    "\n",
    "        # Nodes in fully connected layers (progressive decrease)\n",
    "        fc_nodes = []\n",
    "        base_fc = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "        for i in range(num_fc_layers):\n",
    "            layer_nodes = max(config['min_fc_nodes'], base_fc // (2 ** i))\n",
    "            fc_nodes.append(layer_nodes)\n",
    "\n",
    "        # Activation functions for each layer\n",
    "        activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "        # Other parameters (using configured ranges and options)\n",
    "        dropout_rate = random.uniform(config['min_dropout'], config['max_dropout'])\n",
    "        learning_rate = random.choice(config['learning_rate_options'])\n",
    "        optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "        normalization_type = 'batch'  # Default to batch normalization\n",
    "\n",
    "        genome = {\n",
    "            'num_conv_layers': num_conv_layers,\n",
    "            'num_fc_layers': num_fc_layers,\n",
    "            'filters': filters,\n",
    "            'kernel_sizes': kernel_sizes,\n",
    "            'fc_nodes': fc_nodes,\n",
    "            'activations': activations,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'learning_rate': learning_rate,\n",
    "            'optimizer': optimizer,\n",
    "            'normalization_type': normalization_type,\n",
    "            'fitness': 0.0,\n",
    "            'id': str(uuid.uuid4())[:8]\n",
    "        }\n",
    "        \n",
    "        # Validate genome\n",
    "        if is_genome_valid(genome, config):\n",
    "            return genome\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    # If we couldn't create a valid genome, create a minimal safe one\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not create random genome after {max_attempts} attempts. Creating minimal safe genome.\")\n",
    "    return {\n",
    "        'num_conv_layers': 1,\n",
    "        'num_fc_layers': 1,\n",
    "        'filters': [32],\n",
    "        'kernel_sizes': [3],\n",
    "        'fc_nodes': [64],\n",
    "        'activations': ['relu', 'relu'],\n",
    "        'dropout_rate': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'normalization_type': 'batch',\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "\n",
    "print(\"‚úì create_random_genome function defined (using configurable parameters with validation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc26f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì validate_and_fix_genome function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_and_fix_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates and fixes a genome to ensure all lists match their corresponding layer counts.\n",
    "    This prevents IndexError when building the model.\n",
    "    \n",
    "    Args:\n",
    "        genome: The genome to validate\n",
    "        config: Configuration dictionary with min/max values\n",
    "    \n",
    "    Returns:\n",
    "        Fixed genome with correct list lengths\n",
    "    \"\"\"\n",
    "    # Fix filters and kernel_sizes to match num_conv_layers\n",
    "    num_conv = genome['num_conv_layers']\n",
    "    \n",
    "    # Fix filters list\n",
    "    if len(genome['filters']) != num_conv:\n",
    "        genome['filters'] = genome['filters'][:num_conv]\n",
    "        while len(genome['filters']) < num_conv:\n",
    "            genome['filters'].append(\n",
    "                random.randint(config['min_filters'], config['max_filters'])\n",
    "            )\n",
    "    \n",
    "    # Fix kernel_sizes list\n",
    "    if len(genome['kernel_sizes']) != num_conv:\n",
    "        genome['kernel_sizes'] = genome['kernel_sizes'][:num_conv]\n",
    "        while len(genome['kernel_sizes']) < num_conv:\n",
    "            genome['kernel_sizes'].append(\n",
    "                random.choice(config['kernel_size_options'])\n",
    "            )\n",
    "    \n",
    "    # Fix fc_nodes to match num_fc_layers\n",
    "    num_fc = genome['num_fc_layers']\n",
    "    \n",
    "    if len(genome['fc_nodes']) != num_fc:\n",
    "        genome['fc_nodes'] = genome['fc_nodes'][:num_fc]\n",
    "        while len(genome['fc_nodes']) < num_fc:\n",
    "            genome['fc_nodes'].append(\n",
    "                random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "            )\n",
    "    \n",
    "    return genome\n",
    "\n",
    "print(\"‚úì validate_and_fix_genome function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b0659",
   "metadata": {},
   "source": [
    "### 5.2 Genome Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3205dc8",
   "metadata": {},
   "source": [
    "### 5.1.5 Genome Architecture Validation\n",
    "\n",
    "Esta funci√≥n valida que un genoma no produzca arquitecturas inv√°lidas que causar√≠an errores con BatchNorm.\n",
    "\n",
    "**Problema:** BatchNorm1d requiere m√°s de 1 valor en la dimensi√≥n espacial. Si tenemos demasiadas capas convolucionales con MaxPooling, la dimensi√≥n espacial se reduce a 1 o menos, causando el error:\n",
    "```\n",
    "ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1])\n",
    "```\n",
    "\n",
    "**Soluci√≥n:** Validamos que el n√∫mero de capas convolucionales no reduzca excesivamente las dimensiones:\n",
    "- Cada capa convolucional con MaxPool(2) divide la longitud de secuencia por 2\n",
    "- Necesitamos mantener al menos 4 valores en la dimensi√≥n espacial para BatchNorm\n",
    "- Calculamos el m√°ximo n√∫mero seguro de capas: `max_safe_conv_layers = log2(sequence_length / 4)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ff89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_genome_valid(genome: dict, config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validates if a genome will produce a valid architecture.\n",
    "    Checks if the convolutional layers will reduce dimensions too much.\n",
    "    \n",
    "    Args:\n",
    "        genome: The genome to validate\n",
    "        config: Configuration dictionary\n",
    "        \n",
    "    Returns:\n",
    "        True if genome is valid, False otherwise\n",
    "    \"\"\"\n",
    "    # Calculate expected output size after all conv layers\n",
    "    # Each MaxPool layer reduces by factor of 2\n",
    "    num_conv_layers = genome['num_conv_layers']\n",
    "    sequence_length = config['sequence_length']\n",
    "    \n",
    "    # Each conv layer has a MaxPool that divides by 2\n",
    "    expected_length = sequence_length / (2 ** num_conv_layers)\n",
    "    \n",
    "    # We need at least 2 values for BatchNorm to work properly\n",
    "    # Use a safety margin\n",
    "    min_required_length = 4\n",
    "    \n",
    "    if expected_length < min_required_length:\n",
    "        return False\n",
    "    \n",
    "    # Also check that we don't have too many conv layers for the sequence length\n",
    "    max_allowed_conv_layers = int(np.log2(sequence_length / min_required_length))\n",
    "    \n",
    "    if num_conv_layers > max_allowed_conv_layers:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"‚úì is_genome_valid function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì mutate_genome function defined (using configurable parameters)\n"
     ]
    }
   ],
   "source": [
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Applies mutation to a genome using adaptive mutation rate and configurable parameters.\n",
    "    Ensures the mutated genome produces a valid architecture.\n",
    "    \"\"\"\n",
    "    max_attempts = 50\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        mutated_genome = copy.deepcopy(genome)\n",
    "        mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "        # Calculate maximum safe conv layers\n",
    "        sequence_length = config['sequence_length']\n",
    "        min_required_length = 4\n",
    "        max_safe_conv_layers = int(np.log2(sequence_length / min_required_length))\n",
    "        safe_max_conv = min(config['max_conv_layers'], max_safe_conv_layers)\n",
    "\n",
    "        # Mutate number of convolutional layers (with safety limit)\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], safe_max_conv)\n",
    "\n",
    "        # Mutate number of FC layers\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "        # Validate and fix lists to match layer counts\n",
    "        mutated_genome = validate_and_fix_genome(mutated_genome, config)\n",
    "\n",
    "        # Mutate filters\n",
    "        for i in range(len(mutated_genome['filters'])):\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "        # Mutate kernel sizes (using configured options)\n",
    "        for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_genome['kernel_sizes'][i] = random.choice(config['kernel_size_options'])\n",
    "\n",
    "        # Mutate FC nodes\n",
    "        for i in range(len(mutated_genome['fc_nodes'])):\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "        # Mutate activation functions\n",
    "        for i in range(len(mutated_genome['activations'])):\n",
    "            if random.random() < mutation_rate:\n",
    "                mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "        # Mutate dropout (using configured range)\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['dropout_rate'] = random.uniform(config['min_dropout'], config['max_dropout'])\n",
    "\n",
    "        # Mutate learning rate (using configured options)\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['learning_rate'] = random.choice(config['learning_rate_options'])\n",
    "\n",
    "        # Mutate optimizer\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "        # Mutate normalization type (using configured weights)\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['normalization_type'] = random.choices(\n",
    "                ['batch', 'layer'], \n",
    "                weights=[config['normalization_batch_weight'], config['normalization_layer_weight']]\n",
    "            )[0]\n",
    "\n",
    "        mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "        mutated_genome['fitness'] = 0.0\n",
    "        \n",
    "        # Final validation to ensure consistency\n",
    "        mutated_genome = validate_and_fix_genome(mutated_genome, config)\n",
    "        \n",
    "        # Check if mutated genome is valid\n",
    "        if is_genome_valid(mutated_genome, config):\n",
    "            return mutated_genome\n",
    "    \n",
    "    # If we couldn't create a valid mutation, return a slightly modified version\n",
    "    # that we know is safe (reduce conv layers if needed)\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not create valid mutation after {max_attempts} attempts. Using safe fallback.\")\n",
    "    safe_genome = copy.deepcopy(genome)\n",
    "    safe_genome['num_conv_layers'] = min(safe_genome['num_conv_layers'], safe_max_conv)\n",
    "    safe_genome = validate_and_fix_genome(safe_genome, config)\n",
    "    safe_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    safe_genome['fitness'] = 0.0\n",
    "    \n",
    "    return safe_genome\n",
    "\n",
    "print(\"‚úì mutate_genome function defined (using configurable parameters with validation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b06729",
   "metadata": {},
   "source": [
    "### 5.3 Genome Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029effe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Genetic functions updated for adaptive mutation and 1D audio processing\n"
     ]
    }
   ],
   "source": [
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Performs crossover between two genomes.\n",
    "    Ensures resulting children produce valid architectures.\n",
    "    \"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    max_attempts = 20\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        child1 = copy.deepcopy(parent1)\n",
    "        child2 = copy.deepcopy(parent2)\n",
    "        \n",
    "        # Calculate maximum safe conv layers\n",
    "        sequence_length = config['sequence_length']\n",
    "        min_required_length = 4\n",
    "        max_safe_conv_layers = int(np.log2(sequence_length / min_required_length))\n",
    "        safe_max_conv = min(config['max_conv_layers'], max_safe_conv_layers)\n",
    "\n",
    "        # Crossover scalar parameters\n",
    "        for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer', 'normalization_type']:\n",
    "            if random.random() < 0.5:\n",
    "                child1[key], child2[key] = child2[key], child1[key]\n",
    "        \n",
    "        # Ensure conv layers don't exceed safe maximum\n",
    "        child1['num_conv_layers'] = min(child1['num_conv_layers'], safe_max_conv)\n",
    "        child2['num_conv_layers'] = min(child2['num_conv_layers'], safe_max_conv)\n",
    "\n",
    "        # Crossover lists (random cut point)\n",
    "        for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "            if random.random() < 0.5:\n",
    "                list1 = child1[list_key]\n",
    "                list2 = child2[list_key]\n",
    "                if len(list1) > 1 and len(list2) > 1:\n",
    "                    point1 = random.randint(1, len(list1) - 1)\n",
    "                    point2 = random.randint(1, len(list2) - 1)\n",
    "                    child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                    child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "        child1['id'] = str(uuid.uuid4())[:8]\n",
    "        child2['id'] = str(uuid.uuid4())[:8]\n",
    "        child1['fitness'] = 0.0\n",
    "        child2['fitness'] = 0.0\n",
    "        \n",
    "        # Validate and fix both children to ensure consistency\n",
    "        child1 = validate_and_fix_genome(child1, config)\n",
    "        child2 = validate_and_fix_genome(child2, config)\n",
    "        \n",
    "        # Check if both children are valid\n",
    "        if is_genome_valid(child1, config) and is_genome_valid(child2, config):\n",
    "            return child1, child2\n",
    "    \n",
    "    # If we couldn't create valid children, return copies of parents\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not create valid crossover after {max_attempts} attempts. Returning parent copies.\")\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    \n",
    "    return child1, child2\n",
    "\n",
    "print(\"‚úì Genetic functions updated for adaptive mutation, 1D audio processing, and architecture validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation\n",
    "\n",
    "### 6.1 Class Initialization and Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HybridNeuroevolution class (Part 1/5): Initialization and Checkpoint Management\n"
     ]
    }
   ],
   "source": [
    "class HybridNeuroevolution:\n",
    "    \"\"\"Main class that implements hybrid neuroevolution with 5-fold CV and adaptive mutation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_individual = None\n",
    "        self.fitness_history = []\n",
    "        self.generation_stats = []\n",
    "        self.best_checkpoint_path = None  # Ruta del checkpoint del mejor modelo\n",
    "        \n",
    "        # Early stopping configuration at generation level\n",
    "        self.generations_without_improvement = 0\n",
    "        self.best_fitness_overall = -float('inf')\n",
    "        self.min_improvement_threshold = 0.1  # M√≠nima mejora en fitness (%) para resetear contador\n",
    "        self.max_generations_without_improvement = config.get('early_stopping_generations', 10)\n",
    "\n",
    "    def initialize_population(self):\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "    \n",
    "    def save_best_checkpoint(self, genome: dict, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Guarda el checkpoint del mejor modelo global y elimina el anterior.\n",
    "        \n",
    "        Args:\n",
    "            genome: Genoma del mejor modelo\n",
    "            model: Modelo de PyTorch a guardar\n",
    "        \"\"\"\n",
    "        # Crear directorio para checkpoints si no existe\n",
    "        checkpoint_dir = \"checkpoints\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Eliminar checkpoint anterior si existe\n",
    "        if self.best_checkpoint_path and os.path.exists(self.best_checkpoint_path):\n",
    "            try:\n",
    "                os.remove(self.best_checkpoint_path)\n",
    "                print(f\"      ‚úì Checkpoint anterior eliminado: {self.best_checkpoint_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚úó Error eliminando checkpoint anterior: {e}\")\n",
    "        \n",
    "        # Crear nuevo checkpoint\n",
    "        checkpoint_filename = f\"best_model_gen{self.generation}_id{genome['id']}_fitness{genome['fitness']:.2f}.pth\"\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "        \n",
    "        # Guardar modelo y genoma\n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'genome': genome,\n",
    "            'generation': self.generation,\n",
    "            'fitness': genome['fitness'],\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            self.best_checkpoint_path = checkpoint_path\n",
    "            print(f\"      ‚úì Nuevo checkpoint guardado: {checkpoint_path}\")\n",
    "            print(f\"        Fitness: {genome['fitness']:.2f}%, ID: {genome['id']}, Gen: {self.generation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚úó Error guardando checkpoint: {e}\")\n",
    "    \n",
    "    def load_best_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Carga el mejor checkpoint guardado.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (genome, model) o (None, None) si no hay checkpoint\n",
    "        \"\"\"\n",
    "        if not self.best_checkpoint_path or not os.path.exists(self.best_checkpoint_path):\n",
    "            print(\"No hay checkpoint disponible para cargar\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            checkpoint_data = torch.load(self.best_checkpoint_path, map_location=device, weights_only=False)\n",
    "            genome = checkpoint_data['genome']\n",
    "            \n",
    "            # Crear modelo y cargar pesos\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "            \n",
    "            print(f\"‚úì Checkpoint cargado exitosamente: {self.best_checkpoint_path}\")\n",
    "            print(f\"  Fitness: {checkpoint_data['fitness']:.2f}%, Gen: {checkpoint_data['generation']}, ID: {genome['id']}\")\n",
    "            \n",
    "            return genome, model\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error cargando checkpoint: {e}\")\n",
    "            return None, None\n",
    "\n",
    "print(\"‚úì HybridNeuroevolution class (Part 1/5): Initialization and Checkpoint Management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a69b0",
   "metadata": {},
   "source": [
    "### 6.2 Training Functions (Single Fold & Thread-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HybridNeuroevolution class (Part 2/5): Training Functions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continued from Part 1: Training Functions\n",
    "\n",
    "# Add training methods to HybridNeuroevolution class\n",
    "def _train_one_fold(self, model, optimizer, criterion, train_loader, test_loader, genome_id: str, fold_num: int):\n",
    "    \"\"\"\n",
    "    Entrena y eval√∫a un modelo en un fold espec√≠fico.\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy del fold\n",
    "    \"\"\"\n",
    "    best_acc = 0.0\n",
    "    best_epoch = -1\n",
    "    patience_left = self.config['epoch_patience']\n",
    "    last_improvement_acc = 0.0\n",
    "    max_epochs = self.config['num_epochs']\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        max_batches = min(len(train_loader), self.config['early_stopping_patience'])\n",
    "        \n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if batch_count >= max_batches:\n",
    "                break\n",
    "        \n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        eval_batches = 0\n",
    "        max_eval_batches = min(len(test_loader), 20)\n",
    "        total_eval_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                total_eval_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                eval_batches += 1\n",
    "                if eval_batches >= max_eval_batches:\n",
    "                    break\n",
    "        \n",
    "        acc = 100.0 * correct / max(1, total)\n",
    "        avg_eval_loss = total_eval_loss / max(1, eval_batches)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        improvement = acc - last_improvement_acc\n",
    "        if improvement >= self.config['improvement_threshold']:\n",
    "            patience_left = self.config['epoch_patience']\n",
    "            last_improvement_acc = acc\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "        # Solo mostrar cada 30 √©pocas para no saturar el log\n",
    "        if epoch % 30 == 0 or epoch == 1 or epoch == max_epochs:\n",
    "            print(f\"          Fold {fold_num} Epoch {epoch}: loss={avg_loss:.4f}, acc={acc:.2f}% (best={best_acc:.2f}%)\")\n",
    "\n",
    "        if patience_left <= 0:\n",
    "            print(f\"          Fold {fold_num}: Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return best_acc\n",
    "\n",
    "def _train_fold_in_thread(self, genome: dict, fold_num: int) -> Tuple[int, float, nn.Module]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo en un fold espec√≠fico (dise√±ado para ejecutarse en un thread).\n",
    "    \n",
    "    Args:\n",
    "        genome: Genoma del modelo\n",
    "        fold_num: N√∫mero de fold (1-5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (fold_num, accuracy, model)\n",
    "        Si la arquitectura es inv√°lida, retorna (fold_num, 0.0, None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar datos del fold\n",
    "        fold_train_loader, fold_test_loader = self._load_fold_data(fold_num)\n",
    "        \n",
    "        # Crear nuevo modelo para este fold\n",
    "        try:\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "        except ValueError as e:\n",
    "            # Error de arquitectura inv√°lida\n",
    "            if \"Invalid architecture\" in str(e) or \"Expected more than 1 value per channel\" in str(e):\n",
    "                print(f\"      ‚úó Fold {fold_num}: Invalid architecture detected - {str(e)[:100]}\")\n",
    "                return fold_num, 0.0, None\n",
    "            else:\n",
    "                # Otro tipo de ValueError\n",
    "                raise\n",
    "        \n",
    "        optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "        optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Entrenar y evaluar en este fold\n",
    "        fold_acc = self._train_one_fold(\n",
    "            model, optimizer, criterion, \n",
    "            fold_train_loader, fold_test_loader,\n",
    "            genome['id'], fold_num\n",
    "        )\n",
    "        \n",
    "        print(f\"      ‚Üí Fold {fold_num} completed: {fold_acc:.2f}%\")\n",
    "        \n",
    "        return fold_num, fold_acc, model\n",
    "        \n",
    "    except ValueError as e:\n",
    "        # Capturar espec√≠ficamente errores de arquitectura inv√°lida\n",
    "        if \"Invalid architecture\" in str(e) or \"Expected more than 1 value per channel\" in str(e):\n",
    "            print(f\"      ‚úó Fold {fold_num}: Invalid architecture - genome will receive fitness 0.0\")\n",
    "            return fold_num, 0.0, None\n",
    "        else:\n",
    "            print(f\"      ERROR in Fold {fold_num}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return fold_num, 0.0, None\n",
    "    except Exception as e:\n",
    "        print(f\"      ERROR in Fold {fold_num}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return fold_num, 0.0, None\n",
    "\n",
    "# Add methods to class\n",
    "HybridNeuroevolution._train_one_fold = _train_one_fold\n",
    "HybridNeuroevolution._train_fold_in_thread = _train_fold_in_thread\n",
    "\n",
    "print(\"‚úì HybridNeuroevolution class (Part 2/5): Training Functions (with invalid architecture handling)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c1fea",
   "metadata": {},
   "source": [
    "### 6.3 Fitness Evaluation and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b0d74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HybridNeuroevolution class (Part 3/5): Fitness Evaluation and Data Loading\n"
     ]
    }
   ],
   "source": [
    "# Continued from Part 2: Fitness Evaluation\n",
    "\n",
    "def evaluate_fitness(self, genome: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Eval√∫a el fitness de un genoma usando 5-fold cross-validation PARALELO.\n",
    "    Los 5 folds se entrenan en threads separados y se espera a que terminen todos.\n",
    "    El fitness final es el promedio de accuracy de los 5 folds.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (fitness, model) donde:\n",
    "            - fitness: promedio de accuracies de los 5 folds\n",
    "            - model: modelo entrenado en el mejor fold (para checkpoint)\n",
    "    \"\"\"\n",
    "    print(f\"      Training/Evaluating model {genome['id']} with PARALLEL 5-FOLD CROSS-VALIDATION\")\n",
    "    \n",
    "    fold_accuracies = {}\n",
    "    fold_models = {}\n",
    "    \n",
    "    try:\n",
    "        # Usar ThreadPoolExecutor para ejecutar los 5 folds en paralelo\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            # Enviar los 5 folds a threads separados\n",
    "            print(f\"      ‚Üí Submitting 5 folds to thread pool...\")\n",
    "            futures = {\n",
    "                executor.submit(self._train_fold_in_thread, genome, fold_num): fold_num\n",
    "                for fold_num in range(1, 6)\n",
    "            }\n",
    "            \n",
    "            # Esperar a que todos los folds terminen\n",
    "            print(f\"      ‚Üí Waiting for all 5 folds to complete...\")\n",
    "            for future in as_completed(futures):\n",
    "                fold_num, fold_acc, model = future.result()\n",
    "                fold_accuracies[fold_num] = fold_acc\n",
    "                fold_models[fold_num] = model\n",
    "        \n",
    "        # Ordenar resultados por fold_num\n",
    "        sorted_folds = sorted(fold_accuracies.keys())\n",
    "        accuracies_list = [fold_accuracies[f] for f in sorted_folds]\n",
    "        \n",
    "        # Encontrar el mejor modelo\n",
    "        best_fold_num = max(fold_accuracies, key=fold_accuracies.get)\n",
    "        best_fold_acc = fold_accuracies[best_fold_num]\n",
    "        best_model = fold_models[best_fold_num]\n",
    "        \n",
    "        # Calcular fitness como promedio de los 5 folds\n",
    "        avg_fitness = np.mean(accuracies_list)\n",
    "        std_fitness = np.std(accuracies_list)\n",
    "        \n",
    "        print(f\"      ‚úì PARALLEL 5-Fold CV Results for {genome['id']}:\")\n",
    "        print(f\"        Fold accuracies: {[f'{acc:.2f}%' for acc in accuracies_list]}\")\n",
    "        print(f\"        Average fitness: {avg_fitness:.2f}% ¬± {std_fitness:.2f}%\")\n",
    "        print(f\"        Best fold: Fold {best_fold_num} with {best_fold_acc:.2f}%\")\n",
    "        \n",
    "        return avg_fitness, best_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ERROR evaluating genome {genome['id']}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.0, None\n",
    "\n",
    "def _load_fold_data(self, fold_number: int):\n",
    "    \"\"\"\n",
    "    Carga los datos de un fold espec√≠fico para el entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        fold_number: N√∫mero de fold (1-5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    fold_files_directory = os.path.join(\n",
    "        self.config['data_path'], \n",
    "        f\"files_real_{self.config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    dataset_id = self.config['dataset_id']\n",
    "    \n",
    "    # Cargar datos del fold\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_{fold_number}.npy'))\n",
    "    y_train = np.load(os.path.join(fold_files_directory, f'y_train_{dataset_id}_fold_{fold_number}.npy'))\n",
    "    x_val = np.load(os.path.join(fold_files_directory, f'X_val_{dataset_id}_fold_{fold_number}.npy'))\n",
    "    y_val = np.load(os.path.join(fold_files_directory, f'y_val_{dataset_id}_fold_{fold_number}.npy'))\n",
    "    x_test = np.load(os.path.join(fold_files_directory, f'X_test_{dataset_id}_fold_{fold_number}.npy'))\n",
    "    y_test = np.load(os.path.join(fold_files_directory, f'y_test_{dataset_id}_fold_{fold_number}.npy'))\n",
    "    \n",
    "    # Reshape si es necesario\n",
    "    if len(x_train.shape) == 2:\n",
    "        x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "        x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "        x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    x_train_tensor = torch.FloatTensor(x_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train.astype(np.int64))\n",
    "    x_val_tensor = torch.FloatTensor(x_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val.astype(np.int64))\n",
    "    x_test_tensor = torch.FloatTensor(x_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test.astype(np.int64))\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    x_eval = torch.cat([x_val_tensor, x_test_tensor], dim=0)\n",
    "    y_eval = torch.cat([y_val_tensor, y_test_tensor], dim=0)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_eval, y_eval)\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    fold_train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=self.config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    fold_test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=self.config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return fold_train_loader, fold_test_loader\n",
    "\n",
    "# Add methods to class\n",
    "HybridNeuroevolution.evaluate_fitness = evaluate_fitness\n",
    "HybridNeuroevolution._load_fold_data = _load_fold_data\n",
    "\n",
    "print(\"‚úì HybridNeuroevolution class (Part 3/5): Fitness Evaluation and Data Loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9ae0c",
   "metadata": {},
   "source": [
    "### 6.4 Population Evaluation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31ce67f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HybridNeuroevolution class (Part 4/5): Population Evaluation and Selection\n"
     ]
    }
   ],
   "source": [
    "# Continued from Part 3: Population Evaluation\n",
    "\n",
    "def evaluate_population(self):\n",
    "    print(f\"\\nEvaluating population (Generation {self.generation})...\")\n",
    "    print(f\"Processing {len(self.population)} individuals...\")\n",
    "    fitness_scores = []\n",
    "    best_fitness_so_far = 0.0\n",
    "    current_global_best_fitness = self.best_individual['fitness'] if self.best_individual else 0.0\n",
    "    \n",
    "    for i, genome in enumerate(self.population):\n",
    "        print(f\"\\n   Evaluating individual {i+1}/{len(self.population)} (ID: {genome['id']})\")\n",
    "        print(f\"      Architecture: {genome['num_conv_layers']} conv + {genome['num_fc_layers']} fc, opt={genome['optimizer']}, lr={genome['learning_rate']}\")\n",
    "        \n",
    "        # Evaluar y obtener fitness y modelo\n",
    "        fitness, model = self.evaluate_fitness(genome)\n",
    "        genome['fitness'] = fitness\n",
    "        fitness_scores.append(fitness)\n",
    "        \n",
    "        if fitness > best_fitness_so_far:\n",
    "            best_fitness_so_far = fitness\n",
    "            print(f\"      New best fitness in this generation: {fitness:.2f}%!\")\n",
    "        \n",
    "        # Verificar si es un nuevo mejor global\n",
    "        if fitness > current_global_best_fitness:\n",
    "            print(f\"      üåü NEW GLOBAL BEST! {fitness:.2f}% > {current_global_best_fitness:.2f}%\")\n",
    "            current_global_best_fitness = fitness\n",
    "            \n",
    "            # Guardar checkpoint (elimina el anterior autom√°ticamente)\n",
    "            if model is not None:\n",
    "                self.save_best_checkpoint(genome, model)\n",
    "        \n",
    "        print(f\"      Fitness obtained: {fitness:.2f}% | Best in generation: {best_fitness_so_far:.2f}% | Global best: {current_global_best_fitness:.2f}%\")\n",
    "    \n",
    "    # Generation statistics\n",
    "    if fitness_scores:\n",
    "        avg_fitness = np.mean(fitness_scores)\n",
    "        max_fitness = np.max(fitness_scores)\n",
    "        min_fitness = np.min(fitness_scores)\n",
    "        std_fitness = np.std(fitness_scores)\n",
    "    else:\n",
    "        avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "    stats = {\n",
    "        'generation': self.generation,\n",
    "        'avg_fitness': avg_fitness,\n",
    "        'max_fitness': max_fitness,\n",
    "        'min_fitness': min_fitness,\n",
    "        'std_fitness': std_fitness\n",
    "    }\n",
    "    self.generation_stats.append(stats)\n",
    "    self.fitness_history.append(max_fitness)\n",
    "\n",
    "    best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "    if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "        self.best_individual = copy.deepcopy(best_genome)\n",
    "        print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "    print(f\"\\nGENERATION {self.generation} STATISTICS:\")\n",
    "    print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "    print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "    print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "    print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "    print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "def selection_and_reproduction(self):\n",
    "    print(f\"\\nStarting selection and reproduction...\")\n",
    "    # Sort by fitness\n",
    "    self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "    elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "    elite = self.population[:elite_size]\n",
    "    print(f\"Selecting {elite_size} elite individuals:\")\n",
    "    for i, individual in enumerate(elite):\n",
    "        print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "    new_population = copy.deepcopy(elite)\n",
    "    offspring_needed = self.config['population_size'] - len(new_population)\n",
    "    print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "    offspring_created = 0\n",
    "    while len(new_population) < self.config['population_size']:\n",
    "        parent1 = self.tournament_selection()\n",
    "        parent2 = self.tournament_selection()\n",
    "        child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "        child1 = mutate_genome(child1, self.config)\n",
    "        if len(new_population) < self.config['population_size']:\n",
    "            new_population.append(child1)\n",
    "        child2 = mutate_genome(child2, self.config)\n",
    "        if len(new_population) < self.config['population_size']:\n",
    "            new_population.append(child2)\n",
    "        offspring_created += 2\n",
    "        if offspring_created % 4 == 0:\n",
    "            print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "    self.population = new_population[:self.config['population_size']]\n",
    "    print(f\"New generation created with {len(self.population)} individuals\")\n",
    "    print(f\"   Elite preserved: {elite_size}\")\n",
    "    print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "    tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "    return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "# Add methods to class\n",
    "HybridNeuroevolution.evaluate_population = evaluate_population\n",
    "HybridNeuroevolution.selection_and_reproduction = selection_and_reproduction\n",
    "HybridNeuroevolution.tournament_selection = tournament_selection\n",
    "\n",
    "print(\"‚úì HybridNeuroevolution class (Part 4/5): Population Evaluation and Selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4aae0",
   "metadata": {},
   "source": [
    "### 6.5 Convergence Check and Main Evolution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea180cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HybridNeuroevolution class (Part 5/5): Convergence and Main Evolution Loop\n",
      "‚úì HybridNeuroevolution class COMPLETE with PARALLEL 5-fold CV and adaptive mutation\n"
     ]
    }
   ],
   "source": [
    "# Continued from Part 4: Convergence and Evolution\n",
    "\n",
    "def _update_adaptive_mutation(self):\n",
    "    # Diversity measured via std of fitness in last generation\n",
    "    if not self.generation_stats:\n",
    "        self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "        return\n",
    "    last_std = self.generation_stats[-1]['std_fitness']\n",
    "    # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "    # Normalize std roughly assuming fitness in [0,100]\n",
    "    diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "    # Invert: low diversity (small std) should raise mutation\n",
    "    inverted = 1 - diversity_factor\n",
    "    new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "    new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "    self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "    print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "def check_convergence(self) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica criterios de convergencia:\n",
    "    1. Target fitness alcanzado\n",
    "    2. M√°ximo de generaciones alcanzado\n",
    "    3. Early stopping: sin mejora en N generaciones\n",
    "    4. Estancamiento detectado en √∫ltimas generaciones\n",
    "    \"\"\"\n",
    "    # Criterion 1: Target fitness reached\n",
    "    if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "        print(f\"\\n‚úÖ Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "        return True\n",
    "    \n",
    "    # Criterion 2: Maximum generations reached\n",
    "    if self.generation >= self.config['max_generations']:\n",
    "        print(f\"\\n‚è±Ô∏è Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "        return True\n",
    "    \n",
    "    # Criterion 3: Early stopping - no improvement in N generations\n",
    "    if self.generation > 0:  # No check on generation 0\n",
    "        current_best = self.best_individual['fitness'] if self.best_individual else 0.0\n",
    "        \n",
    "        # Check if there's improvement compared to best overall\n",
    "        improvement = current_best - self.best_fitness_overall\n",
    "        \n",
    "        if improvement >= self.min_improvement_threshold:\n",
    "            # Significant improvement! Reset counter\n",
    "            self.best_fitness_overall = current_best\n",
    "            self.generations_without_improvement = 0\n",
    "            print(f\"\\nüîÑ Improvement detected: {improvement:.2f}% | Generations without improvement: {self.generations_without_improvement}\")\n",
    "        else:\n",
    "            # No significant improvement\n",
    "            self.generations_without_improvement += 1\n",
    "            print(f\"\\n‚è≥ No significant improvement | Generations without improvement: {self.generations_without_improvement}/{self.max_generations_without_improvement}\")\n",
    "            \n",
    "            if self.generations_without_improvement >= self.max_generations_without_improvement:\n",
    "                print(f\"\\nüõë EARLY STOPPING: No improvement for {self.max_generations_without_improvement} generations\")\n",
    "                print(f\"   Best fitness plateau: {self.best_fitness_overall:.2f}%\")\n",
    "                return True\n",
    "    \n",
    "    # Criterion 4: Stagnation in last 3 generations (additional safety check)\n",
    "    if len(self.fitness_history) >= 3:\n",
    "        recent = self.fitness_history[-3:]\n",
    "        if max(recent) - min(recent) < 0.5:\n",
    "            print(f\"\\nüìâ Stagnation detected in last 3 generations (all within {max(recent) - min(recent):.2f}%)\")\n",
    "            # Don't stop immediately, let generation-level early stopping handle it\n",
    "    \n",
    "    return False\n",
    "\n",
    "def evolve(self) -> dict:\n",
    "    print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + generation-level early stopping)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "    print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "    print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "    print(f\"   Early stopping (generations): {self.config['early_stopping_generations']} without improvement\")\n",
    "    print(f\"   Min improvement threshold: {self.config['min_improvement_threshold']}%\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(\"=\"*80)\n",
    "    self.initialize_population()\n",
    "    while not self.check_convergence():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"GENERATION {self.generation}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        self.evaluate_population()\n",
    "        if self.check_convergence():\n",
    "            break\n",
    "        self._update_adaptive_mutation()\n",
    "        self.selection_and_reproduction()\n",
    "        self.generation += 1\n",
    "        print(f\"\\nPreparing for next generation...\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVOLUTION COMPLETED!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Best individual found:\")\n",
    "    print(f\"   ID: {self.best_individual['id']}\")\n",
    "    print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "    print(f\"   Origin generation: {self.generation}\")\n",
    "    print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "    print(f\"   Generations without improvement: {self.generations_without_improvement}/{self.max_generations_without_improvement}\")\n",
    "    print(\"=\"*80)\n",
    "    return self.best_individual\n",
    "\n",
    "# Add methods to class\n",
    "HybridNeuroevolution._update_adaptive_mutation = _update_adaptive_mutation\n",
    "HybridNeuroevolution.check_convergence = check_convergence\n",
    "HybridNeuroevolution.evolve = evolve\n",
    "\n",
    "print(\"‚úì HybridNeuroevolution class (Part 5/5): Convergence and Main Evolution Loop\")\n",
    "print(\"‚úì HybridNeuroevolution class COMPLETE with PARALLEL 5-fold CV and adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution\n",
    "\n",
    "### üöÄ Importante: Parallel 5-Fold Cross-Validation durante la Evoluci√≥n\n",
    "\n",
    "**Cambio clave**: Ahora cada individuo se eval√∫a con **5-fold cross-validation PARALELO** durante el proceso evolutivo:\n",
    "\n",
    "1. **Durante la evoluci√≥n**:\n",
    "   - Cada individuo se entrena y eval√∫a en **cada uno de los 5 folds SIMULT√ÅNEAMENTE**\n",
    "   - Los 5 folds se ejecutan en **threads separados** (paralelizaci√≥n)\n",
    "   - El **fitness final** es el **promedio** de las accuracies de los 5 folds\n",
    "   - Se espera a que **todos los folds terminen** antes de calcular el fitness\n",
    "   - Esto garantiza que la arquitectura seleccionada no est√© sobreajustada a un fold espec√≠fico\n",
    "\n",
    "2. **Ventajas de la paralelizaci√≥n**:\n",
    "   - üöÄ **Mucho m√°s r√°pido**: Los 5 folds se entrenan simult√°neamente (en threads)\n",
    "   - ‚úÖ **M√°s robusto**: La mejor arquitectura generaliza mejor\n",
    "   - ‚úÖ **Menos sesgado**: No depende de un solo split de datos\n",
    "   - üí° **Aprovecha multi-core**: Usa m√∫ltiples n√∫cleos de CPU para acelerar\n",
    "   \n",
    "3. **Proceso paralelo**:\n",
    "   - Generaci√≥n 0: Se crean N individuos aleatorios\n",
    "   - Para cada individuo:\n",
    "     - **Thread Pool**: Se crean 5 threads (uno por fold)\n",
    "     - **Fold 1-5**: Se entrenan y eval√∫an **SIMULT√ÅNEAMENTE** ‚Üí accuracy‚ÇÅ...accuracy‚ÇÖ\n",
    "     - **Espera**: Se espera a que **todos los threads terminen**\n",
    "     - **Fitness** = (accuracy‚ÇÅ + accuracy‚ÇÇ + accuracy‚ÇÉ + accuracy‚ÇÑ + accuracy‚ÇÖ) / 5\n",
    "   - Se seleccionan los mejores seg√∫n fitness promedio\n",
    "   - Se aplica crossover y mutaci√≥n\n",
    "   - Siguiente generaci√≥n...\n",
    "\n",
    "4. **Rendimiento**:\n",
    "   - Tiempo de evaluaci√≥n ‚âà tiempo del fold m√°s lento (en lugar de suma de todos)\n",
    "   - Aceleraci√≥n te√≥rica: ~5x m√°s r√°pido que secuencial\n",
    "   - Aceleraci√≥n real: depende del n√∫mero de cores disponibles\n",
    "\n",
    "**Nota**: Para hacer pruebas r√°pidas, puedes reducir `population_size` y `max_generations` en la configuraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51ada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUDIO NEUROEVOLUTION CONFIGURATION\n",
      "============================================================\n",
      "   Dataset: Audio (Parkinson Classification)\n",
      "   Dataset ID: 40_1e5_N\n",
      "   Fold ID: 40_1e5_N\n",
      "   Number of folds: 5 (all used during evolution)\n",
      "   Data Path: data\\sets\\folds_5\n",
      "   Number of channels: 1 (1D audio)\n",
      "   Sequence length: 11520 (will be auto-detected)\n",
      "   Number of classes: 2 (Control vs Pathological)\n",
      "   Batch size: 64\n",
      "   Population: 20 individuals\n",
      "   Maximum generations: 100\n",
      "   Target fitness: 80.0%\n",
      "   Device: cuda\n",
      "   Platform: nt (Windows)\n",
      "   Parallelization: Enabled (5 threads per individual)\n",
      "============================================================\n",
      "\n",
      "Verifying audio dataset...\n",
      "\n",
      "============================================================\n",
      "VERIFICANDO DISPONIBILIDAD DE DATOS\n",
      "============================================================\n",
      "Dataset ID: 40_1e5_N, Verificando los 5 folds...\n",
      "   Looking for: e:\\Neuroevolution\\data\\sets\\folds_5\\files_real_40_1e5_N\n",
      "   ‚úì Directory found: e:\\Neuroevolution\\data\\sets\\folds_5\\files_real_40_1e5_N\n",
      "\n",
      "Checking for all 5 folds...\n",
      "   ‚úì Fold 1: All files present\n",
      "   ‚úì Fold 2: All files present\n",
      "   ‚úì Fold 3: All files present\n",
      "   ‚úì Fold 4: All files present\n",
      "   ‚úì Fold 5: All files present\n",
      "\n",
      "‚úì All 5 folds verified successfully!\n",
      "\n",
      "Loading Fold 1 to detect sequence length...\n",
      "\n",
      "AUDIO NEUROEVOLUTION CONFIGURATION\n",
      "============================================================\n",
      "   Dataset: Audio (Parkinson Classification)\n",
      "   Dataset ID: 40_1e5_N\n",
      "   Fold ID: 40_1e5_N\n",
      "   Number of folds: 5 (all used during evolution)\n",
      "   Data Path: data\\sets\\folds_5\n",
      "   Number of channels: 1 (1D audio)\n",
      "   Sequence length: 11520 (will be auto-detected)\n",
      "   Number of classes: 2 (Control vs Pathological)\n",
      "   Batch size: 64\n",
      "   Population: 20 individuals\n",
      "   Maximum generations: 100\n",
      "   Target fitness: 80.0%\n",
      "   Device: cuda\n",
      "   Platform: nt (Windows)\n",
      "   Parallelization: Enabled (5 threads per individual)\n",
      "============================================================\n",
      "\n",
      "Verifying audio dataset...\n",
      "\n",
      "============================================================\n",
      "VERIFICANDO DISPONIBILIDAD DE DATOS\n",
      "============================================================\n",
      "Dataset ID: 40_1e5_N, Verificando los 5 folds...\n",
      "   Looking for: e:\\Neuroevolution\\data\\sets\\folds_5\\files_real_40_1e5_N\n",
      "   ‚úì Directory found: e:\\Neuroevolution\\data\\sets\\folds_5\\files_real_40_1e5_N\n",
      "\n",
      "Checking for all 5 folds...\n",
      "   ‚úì Fold 1: All files present\n",
      "   ‚úì Fold 2: All files present\n",
      "   ‚úì Fold 3: All files present\n",
      "   ‚úì Fold 4: All files present\n",
      "   ‚úì Fold 5: All files present\n",
      "\n",
      "‚úì All 5 folds verified successfully!\n",
      "\n",
      "Loading Fold 1 to detect sequence length...\n",
      "   Train samples: (7200, 11520)\n",
      "   Sequence length detected: 11520\n",
      "\n",
      "‚úì Dataset verification complete!\n",
      "   During evolution, each individual will train on all 5 folds.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DATASET VERIFIED - READY FOR PARALLEL 5-FOLD CV EVOLUTION\n",
      "============================================================\n",
      "\n",
      "Starting audio neuroevolution at 20:01:06\n",
      "Architecture: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC\n",
      "Each individual will be evaluated on all 5 folds IN PARALLEL\n",
      "Using ThreadPoolExecutor with 5 workers (one per fold)\n",
      "============================================================\n",
      "\n",
      "STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + generation-level early stopping)\n",
      "================================================================================\n",
      "Configuration:\n",
      "   Population: 20 individuals\n",
      "   Maximum generations: 100\n",
      "   Target fitness: 80.0%\n",
      "   Early stopping (generations): 20 without improvement\n",
      "   Min improvement threshold: 0.01%\n",
      "   Device: cuda\n",
      "================================================================================\n",
      "Initializing population of 20 individuals...\n",
      "Population initialized with 20 individuals\n",
      "\n",
      "================================================================================\n",
      "GENERATION 0\n",
      "================================================================================\n",
      "\n",
      "Evaluating population (Generation 0)...\n",
      "Processing 20 individuals...\n",
      "\n",
      "   Evaluating individual 1/20 (ID: 6fcc469b)\n",
      "      Architecture: 14 conv + 5 fc, opt=adamw, lr=0.0001\n",
      "      Training/Evaluating model 6fcc469b with PARALLEL 5-FOLD CROSS-VALIDATION\n",
      "      ‚Üí Submitting 5 folds to thread pool...\n",
      "      ‚Üí Waiting for all 5 folds to complete...\n",
      "   Train samples: (7200, 11520)\n",
      "   Sequence length detected: 11520\n",
      "\n",
      "‚úì Dataset verification complete!\n",
      "   During evolution, each individual will train on all 5 folds.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DATASET VERIFIED - READY FOR PARALLEL 5-FOLD CV EVOLUTION\n",
      "============================================================\n",
      "\n",
      "Starting audio neuroevolution at 20:01:06\n",
      "Architecture: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC\n",
      "Each individual will be evaluated on all 5 folds IN PARALLEL\n",
      "Using ThreadPoolExecutor with 5 workers (one per fold)\n",
      "============================================================\n",
      "\n",
      "STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + generation-level early stopping)\n",
      "================================================================================\n",
      "Configuration:\n",
      "   Population: 20 individuals\n",
      "   Maximum generations: 100\n",
      "   Target fitness: 80.0%\n",
      "   Early stopping (generations): 20 without improvement\n",
      "   Min improvement threshold: 0.01%\n",
      "   Device: cuda\n",
      "================================================================================\n",
      "Initializing population of 20 individuals...\n",
      "Population initialized with 20 individuals\n",
      "\n",
      "================================================================================\n",
      "GENERATION 0\n",
      "================================================================================\n",
      "\n",
      "Evaluating population (Generation 0)...\n",
      "Processing 20 individuals...\n",
      "\n",
      "   Evaluating individual 1/20 (ID: 6fcc469b)\n",
      "      Architecture: 14 conv + 5 fc, opt=adamw, lr=0.0001\n",
      "      Training/Evaluating model 6fcc469b with PARALLEL 5-FOLD CROSS-VALIDATION\n",
      "      ‚Üí Submitting 5 folds to thread pool...\n",
      "      ‚Üí Waiting for all 5 folds to complete...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACI√ìN DE DATASET DE AUDIO\n",
    "# ==========================================\n",
    "\n",
    "# Ruta OS-independiente usando os.path.join\n",
    "CONFIG['data_path'] = os.path.join('data', 'sets', 'folds_5')\n",
    "\n",
    "# ==========================================\n",
    "# AJUSTES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "# Ajustar poblaci√≥n y generaciones si es necesario\n",
    "# CONFIG['population_size'] = 8\n",
    "# CONFIG['max_generations'] = 20\n",
    "# CONFIG['fitness_threshold'] = 85.0  # Para audio, 85% es buen objetivo\n",
    "# CONFIG['batch_size'] = 16  # Reducir si hay problemas de memoria\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO NEUROEVOLUTION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Dataset: Audio (Parkinson Classification)\")\n",
    "print(f\"   Dataset ID: {CONFIG['dataset_id']}\")\n",
    "print(f\"   Fold ID: {CONFIG['fold_id']}\")\n",
    "print(f\"   Number of folds: {CONFIG['num_folds']} (all used during evolution)\")\n",
    "print(f\"   Data Path: {CONFIG['data_path']}\")\n",
    "print(f\"   Number of channels: {CONFIG['num_channels']} (1D audio)\")\n",
    "print(f\"   Sequence length: {CONFIG['sequence_length']} (will be auto-detected)\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']} (Control vs Pathological)\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Platform: {os.name} ({'Windows' if os.name == 'nt' else 'Unix/Linux/Mac'})\")\n",
    "print(f\"   Parallelization: Enabled (5 threads per individual)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify dataset availability with the new configuration\n",
    "print(f\"\\nVerifying audio dataset...\")\n",
    "load_dataset(CONFIG)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET VERIFIED - READY FOR PARALLEL 5-FOLD CV EVOLUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting audio neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Architecture: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC\")\n",
    "print(f\"Each individual will be evaluated on all 5 folds IN PARALLEL\")\n",
    "print(f\"Using ThreadPoolExecutor with 5 workers (one per fold)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create system instance (no need for train/test loaders anymore)\n",
    "neuroevolution = HybridNeuroevolution(CONFIG)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVOLUTION PROCESS COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Completed at: {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis\n",
    "\n",
    "### 8.1 Fitness Evolution Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "print(\"‚úì Fitness visualization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f69d6",
   "metadata": {},
   "source": [
    "### 8.2 Detailed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29684a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "print(\"‚úì Evolution statistics function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a17a2",
   "metadata": {},
   "source": [
    "### 8.3 Failure Analysis and Visualization Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   ‚Ä¢ Errors in model architecture\")\n",
    "        print(f\"   ‚Ä¢ Memory problems (GPU/RAM)\")\n",
    "        print(f\"   ‚Ä¢ Invalid hyperparameter configurations\")\n",
    "        print(f\"   ‚Ä¢ Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"    BEST EVOLVED ARCHITECTURE (1D AUDIO)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    print(f\"   Dataset: {config['dataset']}\")\n",
    "    print(f\"   Dataset ID: {config.get('dataset_id', 'N/A')}\")\n",
    "    print(f\"   Fold: {config.get('current_fold', 'N/A')}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Input: 1D Audio Signal (length={config['sequence_length']})\")\n",
    "    print(f\"   Convolutional Layers (Conv1D): {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    print(f\"   Output: {config['num_classes']} classes\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS (1D):\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv1D-{i+1}: {filters} filters, kernel_size={kernel}, activation={activation}\")\n",
    "        print(f\"             -> BatchNorm1D -> {activation.upper()} -> MaxPool1D(2)\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons -> BatchNorm1D -> ReLU -> Dropout({best_genome['dropout_rate']:.3f})\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (Control vs Pathological)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.6f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(set(best_genome['activations']))}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Architecture':<25} {'Conv1D + FC':<30} {'1D Convolutional':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Conv1D layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30.6f} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'Input Length':<25} {config['sequence_length']:<30} {'Audio sequence length':<25}\")\n",
    "    print(f\"{'Classes':<25} {config['num_classes']:<30} {'Binary classification':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   ‚úì TARGET REACHED: {best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%\")\n",
    "    else:\n",
    "        print(f\"   ‚úó TARGET NOT REACHED: {best_genome['fitness']:.2f}% < {config['fitness_threshold']}%\")\n",
    "        print(f\"     Gap: {config['fitness_threshold'] - best_genome['fitness']:.2f}%\")\n",
    "    \n",
    "    print(f\"   Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_audio_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'dataset_type': 'audio_1D',\n",
    "        'dataset_id': config.get('dataset_id', 'N/A'),\n",
    "        'fold': config.get('current_fold', 'N/A'),\n",
    "        'config_used': {k: v for k, v in config.items() if not k.startswith('_')},\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        print(f\"\\n‚úì Results saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó WARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"HYBRID NEUROEVOLUTION FOR AUDIO COMPLETED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar informaci√≥n del checkpoint guardado\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMACI√ìN DEL CHECKPOINT DEL MEJOR MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if neuroevolution.best_checkpoint_path:\n",
    "    print(f\"\\n‚úì Checkpoint guardado en: {neuroevolution.best_checkpoint_path}\")\n",
    "    \n",
    "    # Obtener informaci√≥n del archivo\n",
    "    import os\n",
    "    if os.path.exists(neuroevolution.best_checkpoint_path):\n",
    "        file_size = os.path.getsize(neuroevolution.best_checkpoint_path)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        print(f\"  Tama√±o: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Cargar y mostrar informaci√≥n del checkpoint\n",
    "        checkpoint_data = torch.load(neuroevolution.best_checkpoint_path, map_location=device, weights_only=False)\n",
    "        print(f\"\\n  Informaci√≥n del modelo guardado:\")\n",
    "        print(f\"    Generaci√≥n: {checkpoint_data['generation']}\")\n",
    "        print(f\"    Fitness: {checkpoint_data['fitness']:.2f}%\")\n",
    "        print(f\"    ID Genoma: {checkpoint_data['genome']['id']}\")\n",
    "        print(f\"    Arquitectura: {checkpoint_data['genome']['num_conv_layers']} Conv1D + {checkpoint_data['genome']['num_fc_layers']} FC\")\n",
    "        print(f\"    Optimizador: {checkpoint_data['genome']['optimizer']}\")\n",
    "        print(f\"    Learning Rate: {checkpoint_data['genome']['learning_rate']}\")\n",
    "        \n",
    "        print(f\"\\n  Este checkpoint se usar√° como punto de partida para el 5-fold CV\")\n",
    "        print(f\"  (Transfer learning desde el modelo pre-entrenado)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Archivo no encontrado\")\n",
    "else:\n",
    "    print(\"\\n‚úó No hay checkpoint disponible\")\n",
    "    print(\"  El 5-fold CV entrenar√° desde cero\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cb925",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Resumen del Flujo con Checkpoints\n",
    "\n",
    "```\n",
    "PROCESO DE NEUROEVOLUCI√ìN CON CHECKPOINTS:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. EVOLUCI√ìN (M√∫ltiples Generaciones)\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ Para cada individuo:\n",
    "   ‚îÇ  ‚îú‚îÄ Entrenar y evaluar\n",
    "   ‚îÇ  ‚îú‚îÄ Calcular fitness\n",
    "   ‚îÇ  ‚îÇ\n",
    "   ‚îÇ  ‚îî‚îÄ SI fitness > mejor_global:\n",
    "   ‚îÇ     ‚îú‚îÄ üåü NUEVO MEJOR GLOBAL\n",
    "   ‚îÇ     ‚îú‚îÄ ‚úó Eliminar checkpoint anterior\n",
    "   ‚îÇ     ‚îî‚îÄ ‚úì Guardar nuevo checkpoint\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ Continuar hasta convergencia\n",
    "\n",
    "2. AL FINALIZAR LA EVOLUCI√ìN\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ Se tiene el checkpoint del MEJOR modelo global\n",
    "\n",
    "3. EVALUACI√ìN 5-FOLD CROSS-VALIDATION\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ ‚úì Cargar checkpoint del mejor modelo\n",
    "   ‚îÇ\n",
    "   ‚îú‚îÄ Para cada fold (1 a 5):\n",
    "   ‚îÇ  ‚îú‚îÄ Crear modelo nuevo\n",
    "   ‚îÇ  ‚îú‚îÄ Inicializar con pesos pre-entrenados (Transfer Learning)\n",
    "   ‚îÇ  ‚îú‚îÄ Fine-tuning con datos del fold\n",
    "   ‚îÇ  ‚îî‚îÄ Evaluar y guardar m√©tricas\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ Calcular promedios y desviaciones est√°ndar\n",
    "\n",
    "4. RESULTADOS FINALES\n",
    "   ‚îî‚îÄ M√©tricas robustas para la tabla de comparaci√≥n\n",
    "```\n",
    "\n",
    "### ‚ú® Beneficios de este enfoque:\n",
    "\n",
    "- ‚úÖ **Ahorro de espacio**: Solo 1 checkpoint (el mejor)\n",
    "- ‚úÖ **Eficiencia**: Transfer learning en lugar de entrenar desde cero\n",
    "- ‚úÖ **Robustez**: M√©tricas con 5-fold CV\n",
    "- ‚úÖ **Trazabilidad**: Se mantiene el historial del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acc991",
   "metadata": {},
   "source": [
    "## üìù Nota Importante\n",
    "\n",
    "**Este enfoque tiene mucho sentido porque:**\n",
    "\n",
    "1. **Durante la evoluci√≥n**, cada vez que un modelo supera el mejor fitness global:\n",
    "   - Se guarda autom√°ticamente su checkpoint\n",
    "   - Se elimina el checkpoint anterior (ahorro de espacio)\n",
    "   - Se asegura que siempre tenemos el mejor modelo disponible\n",
    "\n",
    "2. **Para la evaluaci√≥n 5-fold CV**:\n",
    "   - En lugar de entrenar 5 modelos desde cero (aleatorio)\n",
    "   - Se usan los pesos pre-entrenados del mejor modelo como inicio\n",
    "   - Esto es **Transfer Learning**, que t√≠picamente da mejores resultados\n",
    "   - Cada fold hace fine-tuning con sus propios datos\n",
    "\n",
    "3. **Ventajas pr√°cticas**:\n",
    "   - Si el proceso se interrumpe, no se pierde el mejor modelo\n",
    "   - Se puede reanudar la evaluaci√≥n 5-fold desde el checkpoint\n",
    "   - Las m√©tricas son m√°s estables y representativas\n",
    "   - Se optimiza el uso de recursos (disco y tiempo de entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b04a",
   "metadata": {},
   "source": [
    "## 10. Evaluaci√≥n Completa de M√©tricas (Tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def load_fold_data(config, fold_number):\n",
    "    \"\"\"\n",
    "    Carga los datos de un fold espec√≠fico.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuraci√≥n del sistema\n",
    "        fold_number: N√∫mero de fold (1-5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (train_loader, test_loader) para ese fold\n",
    "    \"\"\"\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_real_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    fold_index = fold_number\n",
    "    dataset_id = config['dataset_id']\n",
    "    \n",
    "    # Cargar datos del fold\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_train = np.load(os.path.join(fold_files_directory, f'y_train_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    x_val = np.load(os.path.join(fold_files_directory, f'X_val_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_val = np.load(os.path.join(fold_files_directory, f'y_val_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    x_test = np.load(os.path.join(fold_files_directory, f'X_test_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_test = np.load(os.path.join(fold_files_directory, f'y_test_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    \n",
    "    # Reshape si es necesario\n",
    "    if len(x_train.shape) == 2:\n",
    "        x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "        x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "        x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    x_train_tensor = torch.FloatTensor(x_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train.astype(np.int64))\n",
    "    x_val_tensor = torch.FloatTensor(x_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val.astype(np.int64))\n",
    "    x_test_tensor = torch.FloatTensor(x_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test.astype(np.int64))\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    x_eval = torch.cat([x_val_tensor, x_test_tensor], dim=0)\n",
    "    y_eval = torch.cat([y_val_tensor, y_test_tensor], dim=0)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_eval, y_eval)\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    fold_train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    fold_test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return fold_train_loader, fold_test_loader\n",
    "\n",
    "\n",
    "def evaluate_single_fold(best_genome, config, fold_train_loader, fold_test_loader, fold_num, num_epochs=20, use_pretrained=False, pretrained_model=None):\n",
    "    \"\"\"\n",
    "    Entrena y eval√∫a el modelo en un solo fold.\n",
    "    \n",
    "    Args:\n",
    "        best_genome: Genoma de la mejor arquitectura\n",
    "        config: Configuraci√≥n del sistema\n",
    "        fold_train_loader: DataLoader de entrenamiento del fold\n",
    "        fold_test_loader: DataLoader de test del fold\n",
    "        fold_num: N√∫mero del fold\n",
    "        num_epochs: √âpocas de entrenamiento\n",
    "        use_pretrained: Si True, usa el modelo pre-entrenado como inicio\n",
    "        pretrained_model: Modelo pre-entrenado opcional\n",
    "    \n",
    "    Returns:\n",
    "        dict: M√©tricas del fold\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_num}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Crear modelo nuevo para este fold\n",
    "    model = EvolvableCNN(best_genome, config).to(device)\n",
    "    \n",
    "    # Si hay un modelo pre-entrenado, copiar sus pesos como punto de partida\n",
    "    if use_pretrained and pretrained_model is not None:\n",
    "        print(\"   Inicializando desde modelo pre-entrenado...\")\n",
    "        try:\n",
    "            model.load_state_dict(pretrained_model.state_dict())\n",
    "            print(\"   ‚úì Pesos pre-entrenados cargados exitosamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚úó Error cargando pesos pre-entrenados: {e}\")\n",
    "            print(\"   Continuando con pesos aleatorios...\")\n",
    "    \n",
    "    # Configurar optimizer y criterion\n",
    "    optimizer_class = OPTIMIZERS[best_genome['optimizer']]\n",
    "    optimizer = optimizer_class(model.parameters(), lr=best_genome['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenamiento\n",
    "    print(f\"Entrenando por {num_epochs} √©pocas...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data, target in fold_train_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        \n",
    "        if epoch % 30 == 0 or epoch == 1:\n",
    "            print(f\"   √âpoca {epoch}/{num_epochs}: loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluaci√≥n\n",
    "    print(\"Evaluando...\")\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in fold_test_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Probabilidades para AUC\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            \n",
    "            # Predicciones\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1, zero_division=0) * 100\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0) * 100\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_probs[:, 1]) * 100\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nResultados Fold {fold_num}:\")\n",
    "    print(f\"   Accuracy:     {accuracy:.2f}%\")\n",
    "    print(f\"   Sensitivity:  {sensitivity:.2f}%\")\n",
    "    print(f\"   Specificity:  {specificity:.2f}%\")\n",
    "    print(f\"   F1-Score:     {f1:.2f}%\")\n",
    "    print(f\"   AUC:          {auc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_num,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'n_samples': len(y_true)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_5fold_cross_validation(best_genome, config, num_epochs=20, neuroevolution_instance=None):\n",
    "    \"\"\"\n",
    "    Eval√∫a la mejor arquitectura usando 5-fold cross-validation.\n",
    "    Utiliza el checkpoint del mejor modelo si est√° disponible.\n",
    "    \n",
    "    Args:\n",
    "        best_genome: Genoma de la mejor arquitectura\n",
    "        config: Configuraci√≥n del sistema\n",
    "        num_epochs: √âpocas de entrenamiento por fold\n",
    "        neuroevolution_instance: Instancia de HybridNeuroevolution para cargar checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados agregados de todos los folds\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUACI√ìN 5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nArquitectura a evaluar:\")\n",
    "    print(f\"   Conv1D Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   FC Layers: {best_genome['num_fc_layers']}\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer']}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']}\")\n",
    "    print(f\"   √âpocas por fold: {num_epochs}\")\n",
    "    \n",
    "    # Intentar cargar el checkpoint del mejor modelo\n",
    "    pretrained_model = None\n",
    "    use_pretrained = False\n",
    "    \n",
    "    if neuroevolution_instance is not None:\n",
    "        print(f\"\\nIntentando cargar checkpoint del mejor modelo...\")\n",
    "        genome_from_checkpoint, pretrained_model = neuroevolution_instance.load_best_checkpoint()\n",
    "        \n",
    "        if pretrained_model is not None:\n",
    "            use_pretrained = True\n",
    "            print(f\"‚úì Checkpoint cargado exitosamente\")\n",
    "            print(f\"  Los modelos de cada fold se inicializar√°n con estos pesos pre-entrenados\")\n",
    "        else:\n",
    "            print(f\"‚úó No se pudo cargar checkpoint, se entrenar√°n desde cero\")\n",
    "    else:\n",
    "        print(f\"\\nNo se proporcion√≥ instancia de neuroevolution, entrenando desde cero\")\n",
    "    \n",
    "    # Almacenar resultados de cada fold\n",
    "    fold_results = []\n",
    "    \n",
    "    # Evaluar cada fold\n",
    "    for fold_num in range(1, 6):  # 5 folds\n",
    "        print(f\"\\n\\nCargando datos del Fold {fold_num}...\")\n",
    "        \n",
    "        try:\n",
    "            fold_train_loader, fold_test_loader = load_fold_data(config, fold_num)\n",
    "            print(f\"   Train batches: {len(fold_train_loader)}\")\n",
    "            print(f\"   Test batches: {len(fold_test_loader)}\")\n",
    "            \n",
    "            # Evaluar este fold (usando modelo pre-entrenado si est√° disponible)\n",
    "            fold_result = evaluate_single_fold(\n",
    "                best_genome, \n",
    "                config, \n",
    "                fold_train_loader, \n",
    "                fold_test_loader, \n",
    "                fold_num, \n",
    "                num_epochs,\n",
    "                use_pretrained=use_pretrained,\n",
    "                pretrained_model=pretrained_model\n",
    "            )\n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR en Fold {fold_num}: {e}\")\n",
    "            print(f\"   Saltando este fold...\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Calcular estad√≠sticas agregadas\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS AGREGADOS (5-FOLD CROSS-VALIDATION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not fold_results:\n",
    "        print(\"ERROR: No se pudo evaluar ning√∫n fold\")\n",
    "        return None\n",
    "    \n",
    "    # Extraer m√©tricas de todos los folds\n",
    "    accuracies = [r['accuracy'] for r in fold_results]\n",
    "    sensitivities = [r['sensitivity'] for r in fold_results]\n",
    "    specificities = [r['specificity'] for r in fold_results]\n",
    "    f1_scores = [r['f1_score'] for r in fold_results]\n",
    "    aucs = [r['auc'] for r in fold_results]\n",
    "    \n",
    "    # Calcular promedios y desviaciones est√°ndar\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    \n",
    "    mean_sensitivity = np.mean(sensitivities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "    \n",
    "    mean_specificity = np.mean(specificities)\n",
    "    std_specificity = np.std(specificities)\n",
    "    \n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    # Mostrar resultados por fold\n",
    "    print(f\"\\nRESULTADOS POR FOLD:\")\n",
    "    print(f\"{'Fold':<6} {'Accuracy':<12} {'Sensitivity':<14} {'Specificity':<14} {'F1-Score':<12} {'AUC':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in fold_results:\n",
    "        print(f\"{r['fold']:<6} {r['accuracy']:>6.2f}%      {r['sensitivity']:>6.2f}%        {r['specificity']:>6.2f}%        {r['f1_score']:>6.2f}%      {r['auc']:>6.2f}%\")\n",
    "    \n",
    "    # Mostrar promedios\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Mean':<6} {mean_accuracy:>6.2f}%      {mean_sensitivity:>6.2f}%        {mean_specificity:>6.2f}%        {mean_f1:>6.2f}%      {mean_auc:>6.2f}%\")\n",
    "    print(f\"{'Std':<6} {std_accuracy:>6.2f}%      {std_sensitivity:>6.2f}%        {std_specificity:>6.2f}%        {std_f1:>6.2f}%      {std_auc:>6.2f}%\")\n",
    "    \n",
    "    # Resultados finales\n",
    "    results = {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'std_accuracy': std_accuracy,\n",
    "        'mean_sensitivity': mean_sensitivity,\n",
    "        'std_sensitivity': std_sensitivity,\n",
    "        'mean_specificity': mean_specificity,\n",
    "        'std_specificity': std_specificity,\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'mean_auc': mean_auc,\n",
    "        'std_auc': std_auc,\n",
    "        'n_folds': len(fold_results),\n",
    "        'architecture': f\"{best_genome['num_conv_layers']}Conv1D+{best_genome['num_fc_layers']}FC\"\n",
    "    }\n",
    "    \n",
    "    # Formato para tabla\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORMATO PARA TABLA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nM√âTRICAS FINALES (promedio ¬± desviaci√≥n est√°ndar):\")\n",
    "    print(f\"   Accuracy:     {mean_accuracy:.2f}% ¬± {std_accuracy:.2f}%\")\n",
    "    print(f\"   Sensitivity:  {mean_sensitivity:.2f}% ¬± {std_sensitivity:.2f}%\")\n",
    "    print(f\"   Specificity:  {mean_specificity:.2f}% ¬± {std_specificity:.2f}%\")\n",
    "    print(f\"   F1-Score:     {mean_f1:.2f}% ¬± {std_f1:.2f}%\")\n",
    "    print(f\"   AUC:          {mean_auc:.2f}% ¬± {std_auc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFORMATO PARA TABLA (valores en escala 0-1):\")\n",
    "    print(f\"   Model: Neuroevolution-{results['architecture']}\")\n",
    "    print(f\"   Accuracy:     {mean_accuracy/100:.2f} ({int(std_accuracy)}%)\")\n",
    "    print(f\"   Sensitivity:  {mean_sensitivity/100:.2f} ({int(std_sensitivity)}%)\")\n",
    "    print(f\"   Specificity:  {mean_specificity/100:.2f} ({int(std_specificity)}%)\")\n",
    "    print(f\"   F1-Score:     {mean_f1/100:.2f} ({int(std_f1)}%)\")\n",
    "    print(f\"   AUC:          {mean_auc/100:.2f} ({int(std_auc)}%)\")\n",
    "    \n",
    "    print(f\"\\nFORMATO LaTeX:\")\n",
    "    latex_row = f\"Neuroevolution-{results['architecture']} & {mean_accuracy/100:.2f} ({int(std_accuracy)}\\\\%) & {mean_sensitivity/100:.2f} ({int(std_sensitivity)}\\\\%) & {mean_specificity/100:.2f} ({int(std_specificity)}\\\\%) & {mean_f1/100:.2f} ({int(std_f1)}\\\\%) & {mean_auc/100:.2f} ({int(std_auc)}\\\\%) \\\\\\\\\"\n",
    "    print(f\"   {latex_row}\")\n",
    "    \n",
    "    print(f\"\\nFORMATO Markdown:\")\n",
    "    markdown_row = f\"| Neuroevolution-{results['architecture']} | {mean_accuracy/100:.2f} ({int(std_accuracy)}%) | {mean_sensitivity/100:.2f} ({int(std_sensitivity)}%) | {mean_specificity/100:.2f} ({int(std_specificity)}%) | {mean_f1/100:.2f} ({int(std_f1)}%) | {mean_auc/100:.2f} ({int(std_auc)}%) |\"\n",
    "    print(f\"   {markdown_row}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"5fold_cv_results_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        print(f\"\\n‚úì Resultados guardados en: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error guardando resultados: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Ejecutar evaluaci√≥n 5-fold cross-validation\n",
    "print(\"Iniciando evaluaci√≥n 5-fold cross-validation de la mejor arquitectura...\\n\")\n",
    "print(\"Usando el checkpoint del mejor modelo encontrado durante la evoluci√≥n.\\n\")\n",
    "cv_results = evaluate_5fold_cross_validation(best_genome, CONFIG, num_epochs=20, neuroevolution_instance=neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2964f-0098-4c42-bf72-e66c4ca0ed5a",
   "metadata": {},
   "source": [
    "## 10. Evaluaci√≥n 5-Fold Cross-Validation con Checkpoint\n",
    "\n",
    "Esta secci√≥n eval√∫a la mejor arquitectura encontrada usando **5-fold cross-validation**.\n",
    "\n",
    "### üéØ Ventajas del enfoque con checkpoints:\n",
    "\n",
    "1. **Eficiencia**: Se guarda el mejor modelo durante la evoluci√≥n (no se reentrena desde cero)\n",
    "2. **Transfer Learning**: Los pesos pre-entrenados sirven como punto de partida para cada fold\n",
    "3. **Gesti√≥n de espacio**: Solo se mantiene el checkpoint del mejor modelo global\n",
    "4. **Robustez**: M√©tricas m√°s confiables con intervalos de confianza\n",
    "\n",
    "### üìä Proceso:\n",
    "\n",
    "1. Se carga el checkpoint del mejor modelo encontrado\n",
    "2. Para cada fold:\n",
    "   - Se inicializa un modelo con los pesos pre-entrenados\n",
    "   - Se fine-tunea con los datos de entrenamiento del fold\n",
    "   - Se eval√∫a en los datos de test del fold\n",
    "3. Se calculan m√©tricas agregadas (promedio ¬± desviaci√≥n est√°ndar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

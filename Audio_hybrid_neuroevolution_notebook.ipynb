{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Audio Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements a hybrid neuroevolution process for audio classification (Parkinson detection). The system combines genetic algorithms with 1D convolutional neural networks to evolve optimal architectures for audio processing.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **1D Convolutional Networks**: Optimized for audio waveform processing\n",
    "- **Parallel 5-Fold Cross-Validation**: Each individual is evaluated on all 5 folds IN PARALLEL (fitness = average accuracy)\n",
    "- **Multi-threading**: Folds are trained simultaneously in separate threads for faster evaluation\n",
    "- **Adaptive mutation**: Dynamic mutation rate based on population diversity\n",
    "- **Audio dataset support**: Loads .npy files with train/val/test splits\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of 1D CNN architectures\n",
    "2. Evaluate fitness of each individual using **parallel 5-fold CV** (robust and faster with threading)\n",
    "3. Select best architectures (elitism)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found for Parkinson classification\n",
    "\n",
    "**âœ… Performance**: Multi-threaded 5-fold CV provides robustness against overfitting while being much faster than sequential training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f49a8",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ¨ CONFIGURACIÃ“N ACTUAL DEL DATASET âœ¨\n",
    "\n",
    "**Dataset configurado**: `files_all_real_syn_n` (Datos Reales + SintÃ©ticos Mezclados)\n",
    "\n",
    "Este notebook estÃ¡ configurado para usar el **nuevo dataset** que combina:\n",
    "- ðŸŽµ **Datos Reales**: Audios originales de pacientes  \n",
    "- ðŸ¤– **Datos SintÃ©ticos**: Audios generados por GANs (BigVSAN 40_1e5)\n",
    "\n",
    "**Ventajas de este dataset**:\n",
    "- Mayor diversidad de datos para entrenamiento\n",
    "- Combina la autenticidad de datos reales con la variedad de datos generados\n",
    "- Ideal para mejorar la generalizaciÃ³n del modelo\n",
    "- EstratificaciÃ³n balanceada entre clases (control/patolÃ³gico)\n",
    "\n",
    "**ðŸš€ Parallel 5-Fold Cross-Validation durante la EvoluciÃ³n**: \n",
    "- **CADA** individuo se evalÃºa en **TODOS** los 5 folds **EN PARALELO**\n",
    "- Los 5 folds se entrenan **simultÃ¡neamente** en threads separados\n",
    "- El fitness es el **promedio** de accuracy de los 5 folds\n",
    "- âœ… **Mucho mÃ¡s rÃ¡pido** que entrenamiento secuencial\n",
    "- âœ… **MÃ¡s robusto** - evita sobreajuste a un fold especÃ­fico\n",
    "\n",
    "**ðŸ“Š EvaluaciÃ³n Final**: \n",
    "- Al terminar la evoluciÃ³n, la mejor arquitectura se vuelve a evaluar con 5-fold CV\n",
    "- Se reportan mÃ©tricas completas (accuracy, sensitivity, specificity, F1, AUC)\n",
    "\n",
    "Para cambiar el dataset, modifica los parÃ¡metros `dataset_id` y `fold_id` en la celda de **ConfiguraciÃ³n** (SecciÃ³n 2).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Threading for parallel fold training\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device configured: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 50,            # Population size\n",
    "    'max_generations': 1000,            # Maximum number of generations\n",
    "    'fitness_threshold': 95.0,        # Target fitness (% accuracy) - Adjusted for audio\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.35,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.35,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection (AUDIO ONLY)\n",
    "    'dataset': 'AUDIO',               # Audio dataset for Parkinson classification\n",
    "\n",
    "    # Dataset parameters for audio\n",
    "    'num_channels': 1,                # Input channels (1 for audio waveform)\n",
    "    'sequence_length': 240000,        # Audio sequence length (will be auto-detected)\n",
    "    'num_classes': 2,                 # Number of classes (control vs pathological)\n",
    "    'batch_size': 32,                 # Batch size for audio\n",
    "    'test_split': 0.2,                # Validation percentage\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 30,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.001,           # Base learning rate\n",
    "    'early_stopping_patience': 200,   # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping\n",
    "    'epoch_patience': 3,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.5,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Generation-level early stopping \n",
    "    'early_stopping_generations': 10, # Stop if no improvement in X generations\n",
    "    'min_improvement_threshold': 0.1, # Minimum fitness improvement (%) to reset counter\n",
    "\n",
    "    # Allowed architecture range for 1D Conv\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 5,             # Less layers for 1D audio\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 3,               # Less FC layers\n",
    "    'min_filters': 8,\n",
    "    'max_filters': 128,               # Adjusted for 1D\n",
    "    'min_fc_nodes': 64,\n",
    "    'max_fc_nodes': 512,              # Smaller for audio classification\n",
    "\n",
    "    # Audio dataset configuration (OS-independent paths)\n",
    "    \n",
    "    'dataset_id': 'all_real_syn_n',   # Dataset ID - Mixed real + synthetic data\n",
    "    'fold_id': 'all_real_syn_n',      # Fold ID for files\n",
    "    'num_folds': 5,                   # Number of folds (all used during evolution)\n",
    "    'data_path': os.path.join('data', 'sets', 'folds_5'),  # OS-independent path\n",
    "    'normalization': {'mean': (0.0,), 'std': (1.0,)}  # Audio normalization\n",
    "}\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled, 1D Conv for audio):\")\n",
    "print(f\"   Dataset: Audio (Parkinson Classification)\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key not in ['normalization']:  # Hide normalization details\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"\\nOS-independent path configured: {CONFIG['data_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### InformaciÃ³n sobre el Dataset de Audio\n",
    "\n",
    "**Dataset de Audio para ClasificaciÃ³n de Parkinson**: \n",
    "- Archivos de audio de voz (clasificaciÃ³n Parkinson)\n",
    "- Datos 1D de forma de onda procesada\n",
    "- Estructura: archivos .npy con train/val/test splits\n",
    "- Dificultad: **Alta** - ClasificaciÃ³n mÃ©dica\n",
    "- Fitness objetivo recomendado: >85%\n",
    "- Clases: Control vs Pathological\n",
    "- Formato de archivos: `{data_path}/files_{fold_id}/X_train_{dataset_id}_fold_{fold}.npy`\n",
    "- Arquitectura: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC Layers\n",
    "\n",
    "**ConfiguraciÃ³n del Dataset:**\n",
    "- Modifica los parÃ¡metros en la celda de configuraciÃ³n:\n",
    "  - `dataset_id`: ID del dataset (ej: 'all_real_syn_n')\n",
    "  - `fold_id`: ID de la carpeta de folds (ej: 'all_real_syn_n')\n",
    "  - `data_path`: Ruta base a los datos (usa `os.path.join` para compatibilidad multiplataforma)\n",
    "\n",
    "**ðŸ”„ Uso de 5-Fold CV:**\n",
    "- **Durante la evoluciÃ³n**: Cada individuo se evalÃºa en los 5 folds automÃ¡ticamente\n",
    "- **No se necesita** especificar `current_fold` (se usan todos)\n",
    "- El fitness es el **promedio** de los 5 folds\n",
    "\n",
    "**Nota sobre Rutas:**\n",
    "- Las rutas son **independientes del sistema operativo** (Windows/Linux/Mac)\n",
    "- Usa `os.path.join()` para construir rutas compatibles\n",
    "- Ejemplo: `os.path.join('data', 'sets', 'folds_5')` funciona en cualquier OS\n",
    "\n",
    "---\n",
    "\n",
    "### Tipos de Carpetas de Folds Disponibles (generadas por `create_5_folds.ipynb`)\n",
    "\n",
    "El notebook `generating_csv/create_5_folds.ipynb` genera **5 tipos de carpetas** con diferentes combinaciones de datos **reales** y **sintÃ©ticos** (generados por GANs) para experimentaciÃ³n:\n",
    "\n",
    "#### 1. **`files_real_N`** - Solo Datos Reales\n",
    "   - **Train**: Datos reales (`test_together_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Baseline con datos 100% reales\n",
    "   - **fold_id**: `'real_N'`\n",
    "   - **dataset_id**: `'real_N'`\n",
    "\n",
    "#### 2. **`files_real_40_1e5_N`** - Entrenamiento SintÃ©tico, Test Real\n",
    "   - **Train**: Datos sintÃ©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Evaluar si modelos entrenados con sintÃ©ticos generalizan a datos reales\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 3. **`files_syn_40_1e5_N`** - Solo Datos SintÃ©ticos (mismo conjunto)\n",
    "   - **Train**: Datos sintÃ©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos sintÃ©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Uso**: Evaluar capacidad del modelo con datos 100% sintÃ©ticos\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 4. **`files_syn_1_N`** - Entrenamiento SintÃ©tico, Test SintÃ©tico Diferente\n",
    "   - **Train**: Datos sintÃ©ticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos sintÃ©ticos diferentes (`test_together_syn_1_N`)\n",
    "   - **Uso**: Evaluar generalizaciÃ³n entre diferentes conjuntos sintÃ©ticos\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 5. **`files_syn_all_N`** - Solo Datos Reales (mal nombrado probablemente)\n",
    "   - **Train**: Datos reales (`test_together_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Similar a `files_real_N` (posible duplicado o error de nomenclatura)\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 6. **`files_all_real_syn_n`** - âœ¨ Datos Reales + SintÃ©ticos Mezclados âœ¨ **(NUEVO)**\n",
    "   - **Train**: Datos reales + sintÃ©ticos mezclados\n",
    "   - **Validation**: Datos reales + sintÃ©ticos mezclados\n",
    "   - **Test**: Datos reales + sintÃ©ticos mezclados\n",
    "   - **Uso**: Entrenar y evaluar con una mezcla equilibrada de datos reales y generados por GANs\n",
    "   - **fold_id**: `'all_real_syn_n'`\n",
    "   - **dataset_id**: `'all_real_syn_n'`\n",
    "   - **Ventajas**: Combina diversidad de datos sintÃ©ticos con autenticidad de datos reales\n",
    "   - **ConfiguraciÃ³n actual**: ðŸ”µ **ESTE ES EL DATASET CONFIGURADO POR DEFECTO**\n",
    "\n",
    "**Nota**: Cada carpeta contiene 5 folds de validaciÃ³n cruzada con:\n",
    "- `X_train_{dataset_id}_fold_{1-5}.npy` y `y_train_{dataset_id}_fold_{1-5}.npy`\n",
    "- `X_val_{dataset_id}_fold_{1-5}.npy` y `y_val_{dataset_id}_fold_{1-5}.npy`\n",
    "- `X_test_{dataset_id}_fold_{1-5}.npy` y `y_test_{dataset_id}_fold_{1-5}.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the audio dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading audio dataset from: {config['data_path']}\")\n",
    "    print(f\"Dataset ID: {config['dataset_id']}, Fold: {config['current_fold']}\")\n",
    "    \n",
    "    # Construct paths following ResNet convention\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    # Check if directory exists\n",
    "    print(f\"\\nChecking data directory...\")\n",
    "    print(f\"   Looking for: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    if not os.path.exists(fold_files_directory):\n",
    "        print(f\"\\nâŒ ERROR: Directory not found!\")\n",
    "        print(f\"   Expected: {os.path.abspath(fold_files_directory)}\")\n",
    "        \n",
    "        # Try to find the correct path\n",
    "        possible_paths = [\n",
    "            os.path.join('..', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('.', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSearching for data in alternative locations:\")\n",
    "        for path in possible_paths:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"   {'âœ“' if exists else 'âœ—'} {abs_path}\")\n",
    "            if exists:\n",
    "                fold_files_directory = path\n",
    "                print(f\"\\nâœ“ Found data at: {os.path.abspath(fold_files_directory)}\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\nâŒ Could not find data directory!\\n\"\n",
    "                f\"   Tried paths:\\n\" + \n",
    "                \"\\n\".join([f\"      - {os.path.abspath(p)}\" for p in possible_paths]) +\n",
    "                f\"\\n\\n   Please check:\\n\"\n",
    "                f\"      1. CONFIG['data_path'] is correct\\n\"\n",
    "                f\"      2. The data files exist\\n\"\n",
    "                f\"      3. The fold_id '{config['fold_id']}' is correct\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   âœ“ Directory found: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "def load_dataset(config: dict):\n",
    "    \"\"\"\n",
    "    Verifica que los datos existen y carga el primer fold para detectar sequence_length.\n",
    "    Durante la evoluciÃ³n, cada individuo cargarÃ¡ todos los folds automÃ¡ticamente.\n",
    "    \n",
    "    Args:\n",
    "        config: Diccionario de configuraciÃ³n\n",
    "    \n",
    "    Returns:\n",
    "        None (solo actualiza config['sequence_length'])\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VERIFICANDO DISPONIBILIDAD DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Dataset ID: {config['dataset_id']}, Verificando los 5 folds...\")\n",
    "    \n",
    "    # Build directory path\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   Looking for: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    # If directory not found, try alternative locations\n",
    "    if not os.path.exists(fold_files_directory):\n",
    "        possible_paths = [\n",
    "            os.path.join('..', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('.', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSearching for data in alternative locations:\")\n",
    "        for path in possible_paths:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"   {'âœ“' if exists else 'âœ—'} {abs_path}\")\n",
    "            if exists:\n",
    "                fold_files_directory = path\n",
    "                print(f\"\\nâœ“ Found data at: {os.path.abspath(fold_files_directory)}\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\nâŒ Could not find data directory!\\n\"\n",
    "                f\"   Tried paths:\\n\" + \n",
    "                \"\\n\".join([f\"      - {os.path.abspath(p)}\" for p in possible_paths]) +\n",
    "                f\"\\n\\n   Please check:\\n\"\n",
    "                f\"      1. CONFIG['data_path'] is correct\\n\"\n",
    "                f\"      2. The data files exist\\n\"\n",
    "                f\"      3. The fold_id '{config['fold_id']}' is correct\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   âœ“ Directory found: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    dataset_id = config['dataset_id']\n",
    "    \n",
    "    # Check that all 5 folds exist\n",
    "    print(f\"\\nChecking for all 5 folds...\")\n",
    "    all_folds_ok = True\n",
    "    \n",
    "    for fold_num in range(1, 6):\n",
    "        required_files = [\n",
    "            f'X_train_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_train_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'X_val_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_val_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'X_test_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_test_{dataset_id}_fold_{fold_num}.npy',\n",
    "        ]\n",
    "        \n",
    "        fold_ok = True\n",
    "        for filename in required_files:\n",
    "            filepath = os.path.join(fold_files_directory, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                fold_ok = False\n",
    "                all_folds_ok = False\n",
    "                print(f\"   âœ— Fold {fold_num}: Missing {filename}\")\n",
    "                break\n",
    "        \n",
    "        if fold_ok:\n",
    "            print(f\"   âœ“ Fold {fold_num}: All files present\")\n",
    "    \n",
    "    if not all_folds_ok:\n",
    "        raise FileNotFoundError(\n",
    "            f\"\\nâŒ Some fold files are missing!\\n\"\n",
    "            f\"   Please ensure all 5 folds have complete data files.\\n\"\n",
    "            f\"   dataset_id: '{dataset_id}'\\n\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nâœ“ All 5 folds verified successfully!\")\n",
    "    \n",
    "    # Load first fold to detect sequence_length\n",
    "    print(f\"\\nLoading Fold 1 to detect sequence length...\")\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_1.npy'))\n",
    "    \n",
    "    print(f\"   Train samples: {x_train.shape}\")\n",
    "    \n",
    "    # Update sequence length from actual data\n",
    "    if len(x_train.shape) == 2:  # (samples, sequence_length)\n",
    "        config['sequence_length'] = x_train.shape[1]\n",
    "    elif len(x_train.shape) == 3:  # Already (samples, channels, sequence_length)\n",
    "        config['sequence_length'] = x_train.shape[2]\n",
    "    \n",
    "    print(f\"   Sequence length detected: {config['sequence_length']}\")\n",
    "    print(f\"\\nâœ“ Dataset verification complete!\")\n",
    "    print(f\"   During evolution, each individual will train on all 5 folds.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Verify dataset availability\n",
    "load_dataset(CONFIG)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READY FOR 5-FOLD CROSS-VALIDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Sequence length: {CONFIG['sequence_length']}\")\n",
    "print(f\"   Input channels: {CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Audio classification task: Control (0) vs Pathological (1)\")\n",
    "print(f\"\\n   âš ï¸ Each individual will be evaluated on ALL 5 folds\")\n",
    "print(f\"   âš ï¸ This makes evolution ~5x slower but much more robust\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable CNN class for 1D audio processing.\n",
    "    Uses Conv1D layers for audio/sequential data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        \n",
    "        # Build convolutional layers (1D for audio)\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds 1D convolutional layers according to genome.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = self.config['num_channels']\n",
    "        normalization_type = self.genome.get('normalization_type', 'layer')  # Default to layer for compatibility\n",
    "\n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            out_channels = self.genome['filters'][i]\n",
    "            kernel_size = self.genome['kernel_sizes'][i]\n",
    "            \n",
    "            # Ensure kernel size is odd and reasonable for 1D\n",
    "            kernel_size = max(3, kernel_size if kernel_size % 2 == 1 else kernel_size + 1)\n",
    "            padding = kernel_size // 2\n",
    "            \n",
    "            # 1D Convolutional layer\n",
    "            conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Normalization layer (Layer Normalization or Batch Normalization)\n",
    "            if normalization_type == 'layer':\n",
    "                # Layer Normalization: normaliza sobre features, no sobre batch\n",
    "                # Para Conv1d output de shape (batch, channels, length), normalizamos los channels\n",
    "                layers.append(nn.LayerNorm(out_channels))\n",
    "            else:\n",
    "                # Batch normalization (default)\n",
    "                layers.append(nn.BatchNorm1d(out_channels))\n",
    "            \n",
    "            # Activation function\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # Max pooling (1D) - reduce sequence length\n",
    "            pool_size = 2 if i < self.genome['num_conv_layers'] - 1 else 2\n",
    "            layers.append(nn.MaxPool1d(pool_size, pool_size))\n",
    "            \n",
    "            # Optional dropout after pooling\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"Calculates output size after convolutional layers.\"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['sequence_length'])\n",
    "        \n",
    "        # Pass through convolutional layers\n",
    "        x = dummy_input\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten and get size\n",
    "        return x.view(-1).shape[0]\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        normalization_type = self.genome.get('normalization_type', 'layer')  # Default to layer for compatibility\n",
    "\n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Normalization layer (Layer Normalization or Batch Normalization)\n",
    "            if normalization_type == 'layer':\n",
    "                # Layer Normalization for FC layers\n",
    "                layers.append(nn.LayerNorm(output_size))\n",
    "            else:\n",
    "                # Batch normalization for FC layers (default)\n",
    "                layers.append(nn.BatchNorm1d(output_size))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Ensure input is in correct format for Conv1d\n",
    "        # Expected: (batch, channels, sequence_length)\n",
    "        if len(x.shape) == 2:  # (batch, sequence)\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv1D Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Normalization: {self.genome.get('normalization_type', 'batch')}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly (Conv1D for audio)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be19766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"Creates a random genome within specified ranges (optimized for 1D audio).\"\"\"\n",
    "    # Number of layers\n",
    "    num_conv_layers = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "    num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "    # Filters for each convolutional layer (progressive increase)\n",
    "    filters = []\n",
    "    base_filters = random.randint(config['min_filters'], config['min_filters'] * 2)\n",
    "    for i in range(num_conv_layers):\n",
    "        # Gradually increase filters in deeper layers\n",
    "        layer_filters = min(base_filters * (2 ** i), config['max_filters'])\n",
    "        filters.append(layer_filters)\n",
    "\n",
    "    # Kernel sizes (appropriate for 1D audio)\n",
    "    kernel_sizes = [random.choice([3, 5, 7, 9, 11]) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Nodes in fully connected layers (progressive decrease)\n",
    "    fc_nodes = []\n",
    "    base_fc = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "    for i in range(num_fc_layers):\n",
    "        layer_nodes = max(config['min_fc_nodes'], base_fc // (2 ** i))\n",
    "        fc_nodes.append(layer_nodes)\n",
    "\n",
    "    # Activation functions for each layer\n",
    "    activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "    # Other parameters (adjusted for audio)\n",
    "    dropout_rate = random.uniform(0.2, 0.5)\n",
    "    learning_rate = random.choice([0.001, 0.0005, 0.0001, 0.00005])\n",
    "    optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "    normalization_type = random.choice(['batch', 'layer'])  # Evolve normalization type\n",
    "\n",
    "    genome = {\n",
    "        'num_conv_layers': num_conv_layers,\n",
    "        'num_fc_layers': num_fc_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'fc_nodes': fc_nodes,\n",
    "        'activations': activations,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'normalization_type': normalization_type,\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "    return genome\n",
    "\n",
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"Applies mutation to a genome using adaptive mutation rate.\"\"\"\n",
    "    mutated_genome = copy.deepcopy(genome)\n",
    "    mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "    # Mutate number of convolutional layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "        num_conv = mutated_genome['num_conv_layers']\n",
    "        mutated_genome['filters'] = mutated_genome['filters'][:num_conv]\n",
    "        mutated_genome['kernel_sizes'] = mutated_genome['kernel_sizes'][:num_conv]\n",
    "        while len(mutated_genome['filters']) < num_conv:\n",
    "            mutated_genome['filters'].append(random.randint(config['min_filters'], config['max_filters']))\n",
    "        while len(mutated_genome['kernel_sizes']) < num_conv:\n",
    "            mutated_genome['kernel_sizes'].append(random.choice([3, 5, 7, 9, 11]))\n",
    "\n",
    "    # Mutate filters\n",
    "    for i in range(len(mutated_genome['filters'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "    # Mutate kernel sizes (1D appropriate sizes)\n",
    "    for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['kernel_sizes'][i] = random.choice([3, 5, 7, 9, 11, 13, 15])\n",
    "\n",
    "    # Mutate number of FC layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "        num_fc = mutated_genome['num_fc_layers']\n",
    "        mutated_genome['fc_nodes'] = mutated_genome['fc_nodes'][:num_fc]\n",
    "        while len(mutated_genome['fc_nodes']) < num_fc:\n",
    "            mutated_genome['fc_nodes'].append(random.randint(config['min_fc_nodes'], config['max_fc_nodes']))\n",
    "\n",
    "    # Mutate FC nodes\n",
    "    for i in range(len(mutated_genome['fc_nodes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "    # Mutate activation functions\n",
    "    for i in range(len(mutated_genome['activations'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['dropout_rate'] = random.uniform(0.2, 0.6)\n",
    "\n",
    "    # Mutate learning rate (audio-specific range)\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['learning_rate'] = random.choice([0.001, 0.0005, 0.0001, 0.00005, 0.00001])\n",
    "\n",
    "    # Mutate optimizer\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    # Mutate normalization type (batch or layer)\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['normalization_type'] = random.choice(['batch', 'layer'])\n",
    "\n",
    "    mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    mutated_genome['fitness'] = 0.0\n",
    "    return mutated_genome\n",
    "\n",
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Performs crossover between two genomes.\"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "\n",
    "    # Crossover scalar parameters\n",
    "    for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer', 'normalization_type']:\n",
    "        if random.random() < 0.5:\n",
    "            child1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "    # Crossover lists (random cut point)\n",
    "    for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "        if random.random() < 0.5:\n",
    "            list1 = child1[list_key]\n",
    "            list2 = child2[list_key]\n",
    "            if len(list1) > 1 and len(list2) > 1:\n",
    "                point1 = random.randint(1, len(list1) - 1)\n",
    "                point2 = random.randint(1, len(list2) - 1)\n",
    "                child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    return child1, child2\n",
    "\n",
    "print(\"Genetic functions updated for adaptive mutation and 1D audio processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNeuroevolution:\n",
    "    \"\"\"Main class that implements hybrid neuroevolution with 5-fold CV and adaptive mutation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_individual = None\n",
    "        self.fitness_history = []\n",
    "        self.generation_stats = []\n",
    "        self.best_checkpoint_path = None  # Ruta del checkpoint del mejor modelo\n",
    "        \n",
    "        # Early stopping configuration at generation level\n",
    "        self.generations_without_improvement = 0\n",
    "        self.best_fitness_overall = -float('inf')\n",
    "        self.min_improvement_threshold = 0.1  # MÃ­nima mejora en fitness (%) para resetear contador\n",
    "        self.max_generations_without_improvement = config.get('early_stopping_generations', 10)\n",
    "\n",
    "    def initialize_population(self):\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "    \n",
    "    def save_best_checkpoint(self, genome: dict, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Guarda el checkpoint del mejor modelo global y elimina el anterior.\n",
    "        \n",
    "        Args:\n",
    "            genome: Genoma del mejor modelo\n",
    "            model: Modelo de PyTorch a guardar\n",
    "        \"\"\"\n",
    "        # Crear directorio para checkpoints si no existe\n",
    "        checkpoint_dir = \"checkpoints\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Eliminar checkpoint anterior si existe\n",
    "        if self.best_checkpoint_path and os.path.exists(self.best_checkpoint_path):\n",
    "            try:\n",
    "                os.remove(self.best_checkpoint_path)\n",
    "                print(f\"      âœ“ Checkpoint anterior eliminado: {self.best_checkpoint_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"      âœ— Error eliminando checkpoint anterior: {e}\")\n",
    "        \n",
    "        # Crear nuevo checkpoint\n",
    "        checkpoint_filename = f\"best_model_gen{self.generation}_id{genome['id']}_fitness{genome['fitness']:.2f}.pth\"\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "        \n",
    "        # Guardar modelo y genoma\n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'genome': genome,\n",
    "            'generation': self.generation,\n",
    "            'fitness': genome['fitness'],\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            self.best_checkpoint_path = checkpoint_path\n",
    "            print(f\"      âœ“ Nuevo checkpoint guardado: {checkpoint_path}\")\n",
    "            print(f\"        Fitness: {genome['fitness']:.2f}%, ID: {genome['id']}, Gen: {self.generation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      âœ— Error guardando checkpoint: {e}\")\n",
    "    \n",
    "    def load_best_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Carga el mejor checkpoint guardado.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (genome, model) o (None, None) si no hay checkpoint\n",
    "        \"\"\"\n",
    "        if not self.best_checkpoint_path or not os.path.exists(self.best_checkpoint_path):\n",
    "            print(\"No hay checkpoint disponible para cargar\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            checkpoint_data = torch.load(self.best_checkpoint_path, map_location=device)\n",
    "            genome = checkpoint_data['genome']\n",
    "            \n",
    "            # Crear modelo y cargar pesos\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "            \n",
    "            print(f\"âœ“ Checkpoint cargado exitosamente: {self.best_checkpoint_path}\")\n",
    "            print(f\"  Fitness: {checkpoint_data['fitness']:.2f}%, Gen: {checkpoint_data['generation']}, ID: {genome['id']}\")\n",
    "            \n",
    "            return genome, model\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error cargando checkpoint: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def _train_one_fold(self, model, optimizer, criterion, train_loader, test_loader, genome_id: str, fold_num: int):\n",
    "        \"\"\"\n",
    "        Entrena y evalÃºa un modelo en un fold especÃ­fico.\n",
    "        \n",
    "        Returns:\n",
    "            float: Accuracy del fold\n",
    "        \"\"\"\n",
    "        best_acc = 0.0\n",
    "        best_epoch = -1\n",
    "        patience_left = self.config['epoch_patience']\n",
    "        last_improvement_acc = 0.0\n",
    "        max_epochs = self.config['num_epochs']\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            # Train\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            max_batches = min(len(train_loader), self.config['early_stopping_patience'])\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                if batch_count >= max_batches:\n",
    "                    break\n",
    "            \n",
    "            avg_loss = running_loss / max(1, batch_count)\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            eval_batches = 0\n",
    "            max_eval_batches = min(len(test_loader), 20)\n",
    "            total_eval_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    total_eval_loss += loss.item()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "                    eval_batches += 1\n",
    "                    if eval_batches >= max_eval_batches:\n",
    "                        break\n",
    "            \n",
    "            acc = 100.0 * correct / max(1, total)\n",
    "            avg_eval_loss = total_eval_loss / max(1, eval_batches)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            improvement = acc - last_improvement_acc\n",
    "            if improvement >= self.config['improvement_threshold']:\n",
    "                patience_left = self.config['epoch_patience']\n",
    "                last_improvement_acc = acc\n",
    "            else:\n",
    "                patience_left -= 1\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_epoch = epoch\n",
    "\n",
    "            # Solo mostrar cada 5 Ã©pocas para no saturar el log\n",
    "            if epoch % 5 == 0 or epoch == 1 or epoch == max_epochs:\n",
    "                print(f\"          Fold {fold_num} Epoch {epoch}: loss={avg_loss:.4f}, acc={acc:.2f}% (best={best_acc:.2f}%)\")\n",
    "\n",
    "            if patience_left <= 0:\n",
    "                print(f\"          Fold {fold_num}: Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        return best_acc\n",
    "\n",
    "    def _train_fold_in_thread(self, genome: dict, fold_num: int) -> Tuple[int, float, nn.Module]:\n",
    "        \"\"\"\n",
    "        Entrena un modelo en un fold especÃ­fico (diseÃ±ado para ejecutarse en un thread).\n",
    "        \n",
    "        Args:\n",
    "            genome: Genoma del modelo\n",
    "            fold_num: NÃºmero de fold (1-5)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (fold_num, accuracy, model)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Cargar datos del fold\n",
    "            fold_train_loader, fold_test_loader = self._load_fold_data(fold_num)\n",
    "            \n",
    "            # Crear nuevo modelo para este fold\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "            optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Entrenar y evaluar en este fold\n",
    "            fold_acc = self._train_one_fold(\n",
    "                model, optimizer, criterion, \n",
    "                fold_train_loader, fold_test_loader,\n",
    "                genome['id'], fold_num\n",
    "            )\n",
    "            \n",
    "            print(f\"      â†’ Fold {fold_num} completed: {fold_acc:.2f}%\")\n",
    "            \n",
    "            return fold_num, fold_acc, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ERROR in Fold {fold_num}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return fold_num, 0.0, None\n",
    "\n",
    "    def evaluate_fitness(self, genome: dict) -> tuple:\n",
    "        \"\"\"\n",
    "        EvalÃºa el fitness de un genoma usando 5-fold cross-validation PARALELO.\n",
    "        Los 5 folds se entrenan en threads separados y se espera a que terminen todos.\n",
    "        El fitness final es el promedio de accuracy de los 5 folds.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (fitness, model) donde:\n",
    "                - fitness: promedio de accuracies de los 5 folds\n",
    "                - model: modelo entrenado en el mejor fold (para checkpoint)\n",
    "        \"\"\"\n",
    "        print(f\"      Training/Evaluating model {genome['id']} with PARALLEL 5-FOLD CROSS-VALIDATION\")\n",
    "        \n",
    "        fold_accuracies = {}\n",
    "        fold_models = {}\n",
    "        \n",
    "        try:\n",
    "            # Usar ThreadPoolExecutor para ejecutar los 5 folds en paralelo\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                # Enviar los 5 folds a threads separados\n",
    "                print(f\"      â†’ Submitting 5 folds to thread pool...\")\n",
    "                futures = {\n",
    "                    executor.submit(self._train_fold_in_thread, genome, fold_num): fold_num\n",
    "                    for fold_num in range(1, 6)\n",
    "                }\n",
    "                \n",
    "                # Esperar a que todos los folds terminen\n",
    "                print(f\"      â†’ Waiting for all 5 folds to complete...\")\n",
    "                for future in as_completed(futures):\n",
    "                    fold_num, fold_acc, model = future.result()\n",
    "                    fold_accuracies[fold_num] = fold_acc\n",
    "                    fold_models[fold_num] = model\n",
    "            \n",
    "            # Ordenar resultados por fold_num\n",
    "            sorted_folds = sorted(fold_accuracies.keys())\n",
    "            accuracies_list = [fold_accuracies[f] for f in sorted_folds]\n",
    "            \n",
    "            # Encontrar el mejor modelo\n",
    "            best_fold_num = max(fold_accuracies, key=fold_accuracies.get)\n",
    "            best_fold_acc = fold_accuracies[best_fold_num]\n",
    "            best_model = fold_models[best_fold_num]\n",
    "            \n",
    "            # Calcular fitness como promedio de los 5 folds\n",
    "            avg_fitness = np.mean(accuracies_list)\n",
    "            std_fitness = np.std(accuracies_list)\n",
    "            \n",
    "            print(f\"      âœ“ PARALLEL 5-Fold CV Results for {genome['id']}:\")\n",
    "            print(f\"        Fold accuracies: {[f'{acc:.2f}%' for acc in accuracies_list]}\")\n",
    "            print(f\"        Average fitness: {avg_fitness:.2f}% Â± {std_fitness:.2f}%\")\n",
    "            print(f\"        Best fold: Fold {best_fold_num} with {best_fold_acc:.2f}%\")\n",
    "            \n",
    "            return avg_fitness, best_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ERROR evaluating genome {genome['id']}: {e}\")\n",
    "            logger.warning(f\"Error evaluating genome {genome['id']}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return 0.0, None\n",
    "    \n",
    "    def _load_fold_data(self, fold_number: int):\n",
    "        \"\"\"\n",
    "        Carga los datos de un fold especÃ­fico para el entrenamiento.\n",
    "        \n",
    "        Args:\n",
    "            fold_number: NÃºmero de fold (1-5)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (train_loader, test_loader)\n",
    "        \"\"\"\n",
    "        fold_files_directory = os.path.join(\n",
    "            self.config['data_path'], \n",
    "            f\"files_{self.config['fold_id']}\"\n",
    "        )\n",
    "        \n",
    "        dataset_id = self.config['dataset_id']\n",
    "        \n",
    "        # Cargar datos del fold\n",
    "        x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        y_train = np.load(os.path.join(fold_files_directory, f'y_train_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        x_val = np.load(os.path.join(fold_files_directory, f'X_val_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        y_val = np.load(os.path.join(fold_files_directory, f'y_val_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        x_test = np.load(os.path.join(fold_files_directory, f'X_test_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        y_test = np.load(os.path.join(fold_files_directory, f'y_test_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        \n",
    "        # Reshape si es necesario\n",
    "        if len(x_train.shape) == 2:\n",
    "            x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "            x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "            x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "        \n",
    "        # Convertir a tensores\n",
    "        x_train_tensor = torch.FloatTensor(x_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train.astype(np.int64))\n",
    "        x_val_tensor = torch.FloatTensor(x_val)\n",
    "        y_val_tensor = torch.LongTensor(y_val.astype(np.int64))\n",
    "        x_test_tensor = torch.FloatTensor(x_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test.astype(np.int64))\n",
    "        \n",
    "        # Crear datasets\n",
    "        train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        x_eval = torch.cat([x_val_tensor, x_test_tensor], dim=0)\n",
    "        y_eval = torch.cat([y_val_tensor, y_test_tensor], dim=0)\n",
    "        test_dataset = torch.utils.data.TensorDataset(x_eval, y_eval)\n",
    "        \n",
    "        # Crear DataLoaders\n",
    "        fold_train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        fold_test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        return fold_train_loader, fold_test_loader\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        print(f\"\\nEvaluating population (Generation {self.generation})...\")\n",
    "        print(f\"Processing {len(self.population)} individuals...\")\n",
    "        fitness_scores = []\n",
    "        best_fitness_so_far = 0.0\n",
    "        current_global_best_fitness = self.best_individual['fitness'] if self.best_individual else 0.0\n",
    "        \n",
    "        for i, genome in enumerate(self.population):\n",
    "            print(f\"\\n   Evaluating individual {i+1}/{len(self.population)} (ID: {genome['id']})\")\n",
    "            print(f\"      Architecture: {genome['num_conv_layers']} conv + {genome['num_fc_layers']} fc, opt={genome['optimizer']}, lr={genome['learning_rate']}\")\n",
    "            \n",
    "            # Evaluar y obtener fitness y modelo\n",
    "            fitness, model = self.evaluate_fitness(genome)\n",
    "            genome['fitness'] = fitness\n",
    "            fitness_scores.append(fitness)\n",
    "            \n",
    "            if fitness > best_fitness_so_far:\n",
    "                best_fitness_so_far = fitness\n",
    "                print(f\"      New best fitness in this generation: {fitness:.2f}%!\")\n",
    "            \n",
    "            # Verificar si es un nuevo mejor global\n",
    "            if fitness > current_global_best_fitness:\n",
    "                print(f\"      ðŸŒŸ NEW GLOBAL BEST! {fitness:.2f}% > {current_global_best_fitness:.2f}%\")\n",
    "                current_global_best_fitness = fitness\n",
    "                \n",
    "                # Guardar checkpoint (elimina el anterior automÃ¡ticamente)\n",
    "                if model is not None:\n",
    "                    self.save_best_checkpoint(genome, model)\n",
    "            \n",
    "            print(f\"      Fitness obtained: {fitness:.2f}% | Best in generation: {best_fitness_so_far:.2f}% | Global best: {current_global_best_fitness:.2f}%\")\n",
    "        # Generation statistics\n",
    "        if fitness_scores:\n",
    "            avg_fitness = np.mean(fitness_scores)\n",
    "            max_fitness = np.max(fitness_scores)\n",
    "            min_fitness = np.min(fitness_scores)\n",
    "            std_fitness = np.std(fitness_scores)\n",
    "        else:\n",
    "            avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "        stats = {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': avg_fitness,\n",
    "            'max_fitness': max_fitness,\n",
    "            'min_fitness': min_fitness,\n",
    "            'std_fitness': std_fitness\n",
    "        }\n",
    "        self.generation_stats.append(stats)\n",
    "        self.fitness_history.append(max_fitness)\n",
    "\n",
    "        best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "        if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "            self.best_individual = copy.deepcopy(best_genome)\n",
    "            print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "        print(f\"\\nGENERATION {self.generation} STATISTICS:\")\n",
    "        print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "        print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "        print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "        print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "        print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "        print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "    def selection_and_reproduction(self):\n",
    "        print(f\"\\nStarting selection and reproduction...\")\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "        elite = self.population[:elite_size]\n",
    "        print(f\"Selecting {elite_size} elite individuals:\")\n",
    "        for i, individual in enumerate(elite):\n",
    "            print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "        new_population = copy.deepcopy(elite)\n",
    "        offspring_needed = self.config['population_size'] - len(new_population)\n",
    "        print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "        offspring_created = 0\n",
    "        while len(new_population) < self.config['population_size']:\n",
    "            parent1 = self.tournament_selection()\n",
    "            parent2 = self.tournament_selection()\n",
    "            child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "            child1 = mutate_genome(child1, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child1)\n",
    "            child2 = mutate_genome(child2, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child2)\n",
    "            offspring_created += 2\n",
    "            if offspring_created % 4 == 0:\n",
    "                print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "        self.population = new_population[:self.config['population_size']]\n",
    "        print(f\"New generation created with {len(self.population)} individuals\")\n",
    "        print(f\"   Elite preserved: {elite_size}\")\n",
    "        print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "    def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "        tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "        return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "    def _update_adaptive_mutation(self):\n",
    "        # Diversity measured via std of fitness in last generation\n",
    "        if not self.generation_stats:\n",
    "            self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "            return\n",
    "        last_std = self.generation_stats[-1]['std_fitness']\n",
    "        # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "        # Normalize std roughly assuming fitness in [0,100]\n",
    "        diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "        # Invert: low diversity (small std) should raise mutation\n",
    "        inverted = 1 - diversity_factor\n",
    "        new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "        new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "        self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "        print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "    def check_convergence(self) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica criterios de convergencia:\n",
    "        1. Target fitness alcanzado\n",
    "        2. MÃ¡ximo de generaciones alcanzado\n",
    "        3. Early stopping: sin mejora en N generaciones\n",
    "        4. Estancamiento detectado en Ãºltimas generaciones\n",
    "        \"\"\"\n",
    "        # Criterion 1: Target fitness reached\n",
    "        if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "            print(f\"\\nâœ… Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "            return True\n",
    "        \n",
    "        # Criterion 2: Maximum generations reached\n",
    "        if self.generation >= self.config['max_generations']:\n",
    "            print(f\"\\nâ±ï¸ Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "            return True\n",
    "        \n",
    "        # Criterion 3: Early stopping - no improvement in N generations\n",
    "        if self.generation > 0:  # No check on generation 0\n",
    "            current_best = self.best_individual['fitness'] if self.best_individual else 0.0\n",
    "            \n",
    "            # Check if there's improvement compared to best overall\n",
    "            improvement = current_best - self.best_fitness_overall\n",
    "            \n",
    "            if improvement >= self.min_improvement_threshold:\n",
    "                # Significant improvement! Reset counter\n",
    "                self.best_fitness_overall = current_best\n",
    "                self.generations_without_improvement = 0\n",
    "                print(f\"\\nðŸ”„ Improvement detected: {improvement:.2f}% | Generations without improvement: {self.generations_without_improvement}\")\n",
    "            else:\n",
    "                # No significant improvement\n",
    "                self.generations_without_improvement += 1\n",
    "                print(f\"\\nâ³ No significant improvement | Generations without improvement: {self.generations_without_improvement}/{self.max_generations_without_improvement}\")\n",
    "                \n",
    "                if self.generations_without_improvement >= self.max_generations_without_improvement:\n",
    "                    print(f\"\\nðŸ›‘ EARLY STOPPING: No improvement for {self.max_generations_without_improvement} generations\")\n",
    "                    print(f\"   Best fitness plateau: {self.best_fitness_overall:.2f}%\")\n",
    "                    return True\n",
    "        \n",
    "        # Criterion 4: Stagnation in last 3 generations (additional safety check)\n",
    "        if len(self.fitness_history) >= 3:\n",
    "            recent = self.fitness_history[-3:]\n",
    "            if max(recent) - min(recent) < 0.5:\n",
    "                print(f\"\\nðŸ“‰ Stagnation detected in last 3 generations (all within {max(recent) - min(recent):.2f}%)\")\n",
    "                # Don't stop immediately, let generation-level early stopping handle it\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def evolve(self) -> dict:\n",
    "        print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + generation-level early stopping)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "        print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "        print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "        print(f\"   Early stopping (generations): {self.config['early_stopping_generations']} without improvement\")\n",
    "        print(f\"   Min improvement threshold: {self.config['min_improvement_threshold']}%\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(\"=\"*80)\n",
    "        self.initialize_population()\n",
    "        while not self.check_convergence():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GENERATION {self.generation}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            self.evaluate_population()\n",
    "            if self.check_convergence():\n",
    "                break\n",
    "            self._update_adaptive_mutation()\n",
    "            self.selection_and_reproduction()\n",
    "            self.generation += 1\n",
    "            print(f\"\\nPreparing for next generation...\")\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVOLUTION COMPLETED!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best individual found:\")\n",
    "        print(f\"   ID: {self.best_individual['id']}\")\n",
    "        print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Origin generation: {self.generation}\")\n",
    "        print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "        print(f\"   Generations without improvement: {self.generations_without_improvement}/{self.max_generations_without_improvement}\")\n",
    "        print(\"=\"*80)\n",
    "        return self.best_individual\n",
    "\n",
    "print(\"HybridNeuroevolution class updated with PARALLEL 5-fold CV evaluation and adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution\n",
    "\n",
    "### ðŸš€ Importante: Parallel 5-Fold Cross-Validation durante la EvoluciÃ³n\n",
    "\n",
    "**Cambio clave**: Ahora cada individuo se evalÃºa con **5-fold cross-validation PARALELO** durante el proceso evolutivo:\n",
    "\n",
    "1. **Durante la evoluciÃ³n**:\n",
    "   - Cada individuo se entrena y evalÃºa en **cada uno de los 5 folds SIMULTÃNEAMENTE**\n",
    "   - Los 5 folds se ejecutan en **threads separados** (paralelizaciÃ³n)\n",
    "   - El **fitness final** es el **promedio** de las accuracies de los 5 folds\n",
    "   - Se espera a que **todos los folds terminen** antes de calcular el fitness\n",
    "   - Esto garantiza que la arquitectura seleccionada no estÃ© sobreajustada a un fold especÃ­fico\n",
    "\n",
    "2. **Ventajas de la paralelizaciÃ³n**:\n",
    "   - ðŸš€ **Mucho mÃ¡s rÃ¡pido**: Los 5 folds se entrenan simultÃ¡neamente (en threads)\n",
    "   - âœ… **MÃ¡s robusto**: La mejor arquitectura generaliza mejor\n",
    "   - âœ… **Menos sesgado**: No depende de un solo split de datos\n",
    "   - ðŸ’¡ **Aprovecha multi-core**: Usa mÃºltiples nÃºcleos de CPU para acelerar\n",
    "   \n",
    "3. **Proceso paralelo**:\n",
    "   - GeneraciÃ³n 0: Se crean N individuos aleatorios\n",
    "   - Para cada individuo:\n",
    "     - **Thread Pool**: Se crean 5 threads (uno por fold)\n",
    "     - **Fold 1-5**: Se entrenan y evalÃºan **SIMULTÃNEAMENTE** â†’ accuracyâ‚...accuracyâ‚…\n",
    "     - **Espera**: Se espera a que **todos los threads terminen**\n",
    "     - **Fitness** = (accuracyâ‚ + accuracyâ‚‚ + accuracyâ‚ƒ + accuracyâ‚„ + accuracyâ‚…) / 5\n",
    "   - Se seleccionan los mejores segÃºn fitness promedio\n",
    "   - Se aplica crossover y mutaciÃ³n\n",
    "   - Siguiente generaciÃ³n...\n",
    "\n",
    "4. **Rendimiento**:\n",
    "   - Tiempo de evaluaciÃ³n â‰ˆ tiempo del fold mÃ¡s lento (en lugar de suma de todos)\n",
    "   - AceleraciÃ³n teÃ³rica: ~5x mÃ¡s rÃ¡pido que secuencial\n",
    "   - AceleraciÃ³n real: depende del nÃºmero de cores disponibles\n",
    "\n",
    "**Nota**: Para hacer pruebas rÃ¡pidas, puedes reducir `population_size` y `max_generations` en la configuraciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACIÃ“N DE DATASET DE AUDIO\n",
    "# ==========================================\n",
    "\n",
    "# Ruta OS-independiente usando os.path.join\n",
    "CONFIG['data_path'] = os.path.join('data', 'sets', 'folds_5')\n",
    "\n",
    "# ==========================================\n",
    "# AJUSTES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "# Ajustar poblaciÃ³n y generaciones si es necesario\n",
    "# CONFIG['population_size'] = 8\n",
    "# CONFIG['max_generations'] = 20\n",
    "# CONFIG['fitness_threshold'] = 85.0  # Para audio, 85% es buen objetivo\n",
    "# CONFIG['batch_size'] = 16  # Reducir si hay problemas de memoria\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO NEUROEVOLUTION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Dataset: Audio (Parkinson Classification)\")\n",
    "print(f\"   Dataset ID: {CONFIG['dataset_id']}\")\n",
    "print(f\"   Fold ID: {CONFIG['fold_id']}\")\n",
    "print(f\"   Number of folds: {CONFIG['num_folds']} (all used during evolution)\")\n",
    "print(f\"   Data Path: {CONFIG['data_path']}\")\n",
    "print(f\"   Number of channels: {CONFIG['num_channels']} (1D audio)\")\n",
    "print(f\"   Sequence length: {CONFIG['sequence_length']} (will be auto-detected)\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']} (Control vs Pathological)\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Platform: {os.name} ({'Windows' if os.name == 'nt' else 'Unix/Linux/Mac'})\")\n",
    "print(f\"   Parallelization: Enabled (5 threads per individual)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify dataset availability with the new configuration\n",
    "print(f\"\\nVerifying audio dataset...\")\n",
    "load_dataset(CONFIG)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET VERIFIED - READY FOR PARALLEL 5-FOLD CV EVOLUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting audio neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Architecture: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC\")\n",
    "print(f\"Each individual will be evaluated on all 5 folds IN PARALLEL\")\n",
    "print(f\"Using ThreadPoolExecutor with 5 workers (one per fold)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create system instance (no need for train/test loaders anymore)\n",
    "neuroevolution = HybridNeuroevolution(CONFIG)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVOLUTION PROCESS COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Completed at: {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   â€¢ Errors in model architecture\")\n",
    "        print(f\"   â€¢ Memory problems (GPU/RAM)\")\n",
    "        print(f\"   â€¢ Invalid hyperparameter configurations\")\n",
    "        print(f\"   â€¢ Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"    BEST EVOLVED ARCHITECTURE (1D AUDIO)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    print(f\"   Dataset: {config['dataset']}\")\n",
    "    print(f\"   Dataset ID: {config.get('dataset_id', 'N/A')}\")\n",
    "    print(f\"   Fold: {config.get('current_fold', 'N/A')}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Input: 1D Audio Signal (length={config['sequence_length']})\")\n",
    "    print(f\"   Convolutional Layers (Conv1D): {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    print(f\"   Output: {config['num_classes']} classes\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS (1D):\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv1D-{i+1}: {filters} filters, kernel_size={kernel}, activation={activation}\")\n",
    "        print(f\"             -> BatchNorm1D -> {activation.upper()} -> MaxPool1D(2)\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons -> BatchNorm1D -> ReLU -> Dropout({best_genome['dropout_rate']:.3f})\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (Control vs Pathological)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.6f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(set(best_genome['activations']))}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Architecture':<25} {'Conv1D + FC':<30} {'1D Convolutional':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Conv1D layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30.6f} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'Input Length':<25} {config['sequence_length']:<30} {'Audio sequence length':<25}\")\n",
    "    print(f\"{'Classes':<25} {config['num_classes']:<30} {'Binary classification':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   âœ“ TARGET REACHED: {best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%\")\n",
    "    else:\n",
    "        print(f\"   âœ— TARGET NOT REACHED: {best_genome['fitness']:.2f}% < {config['fitness_threshold']}%\")\n",
    "        print(f\"     Gap: {config['fitness_threshold'] - best_genome['fitness']:.2f}%\")\n",
    "    \n",
    "    print(f\"   Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_audio_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'dataset_type': 'audio_1D',\n",
    "        'dataset_id': config.get('dataset_id', 'N/A'),\n",
    "        'fold': config.get('current_fold', 'N/A'),\n",
    "        'config_used': {k: v for k, v in config.items() if not k.startswith('_')},\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        print(f\"\\nâœ“ Results saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— WARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"HYBRID NEUROEVOLUTION FOR AUDIO COMPLETED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar informaciÃ³n del checkpoint guardado\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMACIÃ“N DEL CHECKPOINT DEL MEJOR MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if neuroevolution.best_checkpoint_path:\n",
    "    print(f\"\\nâœ“ Checkpoint guardado en: {neuroevolution.best_checkpoint_path}\")\n",
    "    \n",
    "    # Obtener informaciÃ³n del archivo\n",
    "    import os\n",
    "    if os.path.exists(neuroevolution.best_checkpoint_path):\n",
    "        file_size = os.path.getsize(neuroevolution.best_checkpoint_path)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        print(f\"  TamaÃ±o: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Cargar y mostrar informaciÃ³n del checkpoint\n",
    "        checkpoint_data = torch.load(neuroevolution.best_checkpoint_path, map_location=device)\n",
    "        print(f\"\\n  InformaciÃ³n del modelo guardado:\")\n",
    "        print(f\"    GeneraciÃ³n: {checkpoint_data['generation']}\")\n",
    "        print(f\"    Fitness: {checkpoint_data['fitness']:.2f}%\")\n",
    "        print(f\"    ID Genoma: {checkpoint_data['genome']['id']}\")\n",
    "        print(f\"    Arquitectura: {checkpoint_data['genome']['num_conv_layers']} Conv1D + {checkpoint_data['genome']['num_fc_layers']} FC\")\n",
    "        print(f\"    Optimizador: {checkpoint_data['genome']['optimizer']}\")\n",
    "        print(f\"    Learning Rate: {checkpoint_data['genome']['learning_rate']}\")\n",
    "        \n",
    "        print(f\"\\n  Este checkpoint se usarÃ¡ como punto de partida para el 5-fold CV\")\n",
    "        print(f\"  (Transfer learning desde el modelo pre-entrenado)\")\n",
    "    else:\n",
    "        print(f\"  âœ— Archivo no encontrado\")\n",
    "else:\n",
    "    print(\"\\nâœ— No hay checkpoint disponible\")\n",
    "    print(\"  El 5-fold CV entrenarÃ¡ desde cero\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cb925",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ Resumen del Flujo con Checkpoints\n",
    "\n",
    "```\n",
    "PROCESO DE NEUROEVOLUCIÃ“N CON CHECKPOINTS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. EVOLUCIÃ“N (MÃºltiples Generaciones)\n",
    "   â”‚\n",
    "   â”œâ”€ Para cada individuo:\n",
    "   â”‚  â”œâ”€ Entrenar y evaluar\n",
    "   â”‚  â”œâ”€ Calcular fitness\n",
    "   â”‚  â”‚\n",
    "   â”‚  â””â”€ SI fitness > mejor_global:\n",
    "   â”‚     â”œâ”€ ðŸŒŸ NUEVO MEJOR GLOBAL\n",
    "   â”‚     â”œâ”€ âœ— Eliminar checkpoint anterior\n",
    "   â”‚     â””â”€ âœ“ Guardar nuevo checkpoint\n",
    "   â”‚\n",
    "   â””â”€ Continuar hasta convergencia\n",
    "\n",
    "2. AL FINALIZAR LA EVOLUCIÃ“N\n",
    "   â”‚\n",
    "   â””â”€ Se tiene el checkpoint del MEJOR modelo global\n",
    "\n",
    "3. EVALUACIÃ“N 5-FOLD CROSS-VALIDATION\n",
    "   â”‚\n",
    "   â”œâ”€ âœ“ Cargar checkpoint del mejor modelo\n",
    "   â”‚\n",
    "   â”œâ”€ Para cada fold (1 a 5):\n",
    "   â”‚  â”œâ”€ Crear modelo nuevo\n",
    "   â”‚  â”œâ”€ Inicializar con pesos pre-entrenados (Transfer Learning)\n",
    "   â”‚  â”œâ”€ Fine-tuning con datos del fold\n",
    "   â”‚  â””â”€ Evaluar y guardar mÃ©tricas\n",
    "   â”‚\n",
    "   â””â”€ Calcular promedios y desviaciones estÃ¡ndar\n",
    "\n",
    "4. RESULTADOS FINALES\n",
    "   â””â”€ MÃ©tricas robustas para la tabla de comparaciÃ³n\n",
    "```\n",
    "\n",
    "### âœ¨ Beneficios de este enfoque:\n",
    "\n",
    "- âœ… **Ahorro de espacio**: Solo 1 checkpoint (el mejor)\n",
    "- âœ… **Eficiencia**: Transfer learning en lugar de entrenar desde cero\n",
    "- âœ… **Robustez**: MÃ©tricas con 5-fold CV\n",
    "- âœ… **Trazabilidad**: Se mantiene el historial del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acc991",
   "metadata": {},
   "source": [
    "## ðŸ“ Nota Importante\n",
    "\n",
    "**Este enfoque tiene mucho sentido porque:**\n",
    "\n",
    "1. **Durante la evoluciÃ³n**, cada vez que un modelo supera el mejor fitness global:\n",
    "   - Se guarda automÃ¡ticamente su checkpoint\n",
    "   - Se elimina el checkpoint anterior (ahorro de espacio)\n",
    "   - Se asegura que siempre tenemos el mejor modelo disponible\n",
    "\n",
    "2. **Para la evaluaciÃ³n 5-fold CV**:\n",
    "   - En lugar de entrenar 5 modelos desde cero (aleatorio)\n",
    "   - Se usan los pesos pre-entrenados del mejor modelo como inicio\n",
    "   - Esto es **Transfer Learning**, que tÃ­picamente da mejores resultados\n",
    "   - Cada fold hace fine-tuning con sus propios datos\n",
    "\n",
    "3. **Ventajas prÃ¡cticas**:\n",
    "   - Si el proceso se interrumpe, no se pierde el mejor modelo\n",
    "   - Se puede reanudar la evaluaciÃ³n 5-fold desde el checkpoint\n",
    "   - Las mÃ©tricas son mÃ¡s estables y representativas\n",
    "   - Se optimiza el uso de recursos (disco y tiempo de entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b04a",
   "metadata": {},
   "source": [
    "## 10. EvaluaciÃ³n Completa de MÃ©tricas (Tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def load_fold_data(config, fold_number):\n",
    "    \"\"\"\n",
    "    Carga los datos de un fold especÃ­fico.\n",
    "    \n",
    "    Args:\n",
    "        config: ConfiguraciÃ³n del sistema\n",
    "        fold_number: NÃºmero de fold (1-5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (train_loader, test_loader) para ese fold\n",
    "    \"\"\"\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    fold_index = fold_number\n",
    "    dataset_id = config['dataset_id']\n",
    "    \n",
    "    # Cargar datos del fold\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_train = np.load(os.path.join(fold_files_directory, f'y_train_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    x_val = np.load(os.path.join(fold_files_directory, f'X_val_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_val = np.load(os.path.join(fold_files_directory, f'y_val_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    x_test = np.load(os.path.join(fold_files_directory, f'X_test_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_test = np.load(os.path.join(fold_files_directory, f'y_test_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    \n",
    "    # Reshape si es necesario\n",
    "    if len(x_train.shape) == 2:\n",
    "        x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "        x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "        x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    x_train_tensor = torch.FloatTensor(x_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train.astype(np.int64))\n",
    "    x_val_tensor = torch.FloatTensor(x_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val.astype(np.int64))\n",
    "    x_test_tensor = torch.FloatTensor(x_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test.astype(np.int64))\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    x_eval = torch.cat([x_val_tensor, x_test_tensor], dim=0)\n",
    "    y_eval = torch.cat([y_val_tensor, y_test_tensor], dim=0)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_eval, y_eval)\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    fold_train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    fold_test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return fold_train_loader, fold_test_loader\n",
    "\n",
    "\n",
    "def evaluate_single_fold(best_genome, config, fold_train_loader, fold_test_loader, fold_num, num_epochs=20, use_pretrained=False, pretrained_model=None):\n",
    "    \"\"\"\n",
    "    Entrena y evalÃºa el modelo en un solo fold.\n",
    "    \n",
    "    Args:\n",
    "        best_genome: Genoma de la mejor arquitectura\n",
    "        config: ConfiguraciÃ³n del sistema\n",
    "        fold_train_loader: DataLoader de entrenamiento del fold\n",
    "        fold_test_loader: DataLoader de test del fold\n",
    "        fold_num: NÃºmero del fold\n",
    "        num_epochs: Ã‰pocas de entrenamiento\n",
    "        use_pretrained: Si True, usa el modelo pre-entrenado como inicio\n",
    "        pretrained_model: Modelo pre-entrenado opcional\n",
    "    \n",
    "    Returns:\n",
    "        dict: MÃ©tricas del fold\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_num}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Crear modelo nuevo para este fold\n",
    "    model = EvolvableCNN(best_genome, config).to(device)\n",
    "    \n",
    "    # Si hay un modelo pre-entrenado, copiar sus pesos como punto de partida\n",
    "    if use_pretrained and pretrained_model is not None:\n",
    "        print(\"   Inicializando desde modelo pre-entrenado...\")\n",
    "        try:\n",
    "            model.load_state_dict(pretrained_model.state_dict())\n",
    "            print(\"   âœ“ Pesos pre-entrenados cargados exitosamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âœ— Error cargando pesos pre-entrenados: {e}\")\n",
    "            print(\"   Continuando con pesos aleatorios...\")\n",
    "    \n",
    "    # Configurar optimizer y criterion\n",
    "    optimizer_class = OPTIMIZERS[best_genome['optimizer']]\n",
    "    optimizer = optimizer_class(model.parameters(), lr=best_genome['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenamiento\n",
    "    print(f\"Entrenando por {num_epochs} Ã©pocas...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data, target in fold_train_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"   Ã‰poca {epoch}/{num_epochs}: loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # EvaluaciÃ³n\n",
    "    print(\"Evaluando...\")\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in fold_test_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Probabilidades para AUC\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            \n",
    "            # Predicciones\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calcular mÃ©tricas\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1, zero_division=0) * 100\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0) * 100\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_probs[:, 1]) * 100\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nResultados Fold {fold_num}:\")\n",
    "    print(f\"   Accuracy:     {accuracy:.2f}%\")\n",
    "    print(f\"   Sensitivity:  {sensitivity:.2f}%\")\n",
    "    print(f\"   Specificity:  {specificity:.2f}%\")\n",
    "    print(f\"   F1-Score:     {f1:.2f}%\")\n",
    "    print(f\"   AUC:          {auc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_num,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'n_samples': len(y_true)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_5fold_cross_validation(best_genome, config, num_epochs=20, neuroevolution_instance=None):\n",
    "    \"\"\"\n",
    "    EvalÃºa la mejor arquitectura usando 5-fold cross-validation.\n",
    "    Utiliza el checkpoint del mejor modelo si estÃ¡ disponible.\n",
    "    \n",
    "    Args:\n",
    "        best_genome: Genoma de la mejor arquitectura\n",
    "        config: ConfiguraciÃ³n del sistema\n",
    "        num_epochs: Ã‰pocas de entrenamiento por fold\n",
    "        neuroevolution_instance: Instancia de HybridNeuroevolution para cargar checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados agregados de todos los folds\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUACIÃ“N 5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nArquitectura a evaluar:\")\n",
    "    print(f\"   Conv1D Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   FC Layers: {best_genome['num_fc_layers']}\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer']}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']}\")\n",
    "    print(f\"   Ã‰pocas por fold: {num_epochs}\")\n",
    "    \n",
    "    # Intentar cargar el checkpoint del mejor modelo\n",
    "    pretrained_model = None\n",
    "    use_pretrained = False\n",
    "    \n",
    "    if neuroevolution_instance is not None:\n",
    "        print(f\"\\nIntentando cargar checkpoint del mejor modelo...\")\n",
    "        genome_from_checkpoint, pretrained_model = neuroevolution_instance.load_best_checkpoint()\n",
    "        \n",
    "        if pretrained_model is not None:\n",
    "            use_pretrained = True\n",
    "            print(f\"âœ“ Checkpoint cargado exitosamente\")\n",
    "            print(f\"  Los modelos de cada fold se inicializarÃ¡n con estos pesos pre-entrenados\")\n",
    "        else:\n",
    "            print(f\"âœ— No se pudo cargar checkpoint, se entrenarÃ¡n desde cero\")\n",
    "    else:\n",
    "        print(f\"\\nNo se proporcionÃ³ instancia de neuroevolution, entrenando desde cero\")\n",
    "    \n",
    "    # Almacenar resultados de cada fold\n",
    "    fold_results = []\n",
    "    \n",
    "    # Evaluar cada fold\n",
    "    for fold_num in range(1, 6):  # 5 folds\n",
    "        print(f\"\\n\\nCargando datos del Fold {fold_num}...\")\n",
    "        \n",
    "        try:\n",
    "            fold_train_loader, fold_test_loader = load_fold_data(config, fold_num)\n",
    "            print(f\"   Train batches: {len(fold_train_loader)}\")\n",
    "            print(f\"   Test batches: {len(fold_test_loader)}\")\n",
    "            \n",
    "            # Evaluar este fold (usando modelo pre-entrenado si estÃ¡ disponible)\n",
    "            fold_result = evaluate_single_fold(\n",
    "                best_genome, \n",
    "                config, \n",
    "                fold_train_loader, \n",
    "                fold_test_loader, \n",
    "                fold_num, \n",
    "                num_epochs,\n",
    "                use_pretrained=use_pretrained,\n",
    "                pretrained_model=pretrained_model\n",
    "            )\n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR en Fold {fold_num}: {e}\")\n",
    "            print(f\"   Saltando este fold...\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Calcular estadÃ­sticas agregadas\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS AGREGADOS (5-FOLD CROSS-VALIDATION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not fold_results:\n",
    "        print(\"ERROR: No se pudo evaluar ningÃºn fold\")\n",
    "        return None\n",
    "    \n",
    "    # Extraer mÃ©tricas de todos los folds\n",
    "    accuracies = [r['accuracy'] for r in fold_results]\n",
    "    sensitivities = [r['sensitivity'] for r in fold_results]\n",
    "    specificities = [r['specificity'] for r in fold_results]\n",
    "    f1_scores = [r['f1_score'] for r in fold_results]\n",
    "    aucs = [r['auc'] for r in fold_results]\n",
    "    \n",
    "    # Calcular promedios y desviaciones estÃ¡ndar\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    \n",
    "    mean_sensitivity = np.mean(sensitivities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "    \n",
    "    mean_specificity = np.mean(specificities)\n",
    "    std_specificity = np.std(specificities)\n",
    "    \n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    # Mostrar resultados por fold\n",
    "    print(f\"\\nRESULTADOS POR FOLD:\")\n",
    "    print(f\"{'Fold':<6} {'Accuracy':<12} {'Sensitivity':<14} {'Specificity':<14} {'F1-Score':<12} {'AUC':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in fold_results:\n",
    "        print(f\"{r['fold']:<6} {r['accuracy']:>6.2f}%      {r['sensitivity']:>6.2f}%        {r['specificity']:>6.2f}%        {r['f1_score']:>6.2f}%      {r['auc']:>6.2f}%\")\n",
    "    \n",
    "    # Mostrar promedios\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Mean':<6} {mean_accuracy:>6.2f}%      {mean_sensitivity:>6.2f}%        {mean_specificity:>6.2f}%        {mean_f1:>6.2f}%      {mean_auc:>6.2f}%\")\n",
    "    print(f\"{'Std':<6} {std_accuracy:>6.2f}%      {std_sensitivity:>6.2f}%        {std_specificity:>6.2f}%        {std_f1:>6.2f}%      {std_auc:>6.2f}%\")\n",
    "    \n",
    "    # Resultados finales\n",
    "    results = {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'std_accuracy': std_accuracy,\n",
    "        'mean_sensitivity': mean_sensitivity,\n",
    "        'std_sensitivity': std_sensitivity,\n",
    "        'mean_specificity': mean_specificity,\n",
    "        'std_specificity': std_specificity,\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'mean_auc': mean_auc,\n",
    "        'std_auc': std_auc,\n",
    "        'n_folds': len(fold_results),\n",
    "        'architecture': f\"{best_genome['num_conv_layers']}Conv1D+{best_genome['num_fc_layers']}FC\"\n",
    "    }\n",
    "    \n",
    "    # Formato para tabla\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORMATO PARA TABLA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nMÃ‰TRICAS FINALES (promedio Â± desviaciÃ³n estÃ¡ndar):\")\n",
    "    print(f\"   Accuracy:     {mean_accuracy:.2f}% Â± {std_accuracy:.2f}%\")\n",
    "    print(f\"   Sensitivity:  {mean_sensitivity:.2f}% Â± {std_sensitivity:.2f}%\")\n",
    "    print(f\"   Specificity:  {mean_specificity:.2f}% Â± {std_specificity:.2f}%\")\n",
    "    print(f\"   F1-Score:     {mean_f1:.2f}% Â± {std_f1:.2f}%\")\n",
    "    print(f\"   AUC:          {mean_auc:.2f}% Â± {std_auc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFORMATO PARA TABLA (valores en escala 0-1):\")\n",
    "    print(f\"   Model: Neuroevolution-{results['architecture']}\")\n",
    "    print(f\"   Accuracy:     {mean_accuracy/100:.2f} ({int(std_accuracy)}%)\")\n",
    "    print(f\"   Sensitivity:  {mean_sensitivity/100:.2f} ({int(std_sensitivity)}%)\")\n",
    "    print(f\"   Specificity:  {mean_specificity/100:.2f} ({int(std_specificity)}%)\")\n",
    "    print(f\"   F1-Score:     {mean_f1/100:.2f} ({int(std_f1)}%)\")\n",
    "    print(f\"   AUC:          {mean_auc/100:.2f} ({int(std_auc)}%)\")\n",
    "    \n",
    "    print(f\"\\nFORMATO LaTeX:\")\n",
    "    latex_row = f\"Neuroevolution-{results['architecture']} & {mean_accuracy/100:.2f} ({int(std_accuracy)}\\\\%) & {mean_sensitivity/100:.2f} ({int(std_sensitivity)}\\\\%) & {mean_specificity/100:.2f} ({int(std_specificity)}\\\\%) & {mean_f1/100:.2f} ({int(std_f1)}\\\\%) & {mean_auc/100:.2f} ({int(std_auc)}\\\\%) \\\\\\\\\"\n",
    "    print(f\"   {latex_row}\")\n",
    "    \n",
    "    print(f\"\\nFORMATO Markdown:\")\n",
    "    markdown_row = f\"| Neuroevolution-{results['architecture']} | {mean_accuracy/100:.2f} ({int(std_accuracy)}%) | {mean_sensitivity/100:.2f} ({int(std_sensitivity)}%) | {mean_specificity/100:.2f} ({int(std_specificity)}%) | {mean_f1/100:.2f} ({int(std_f1)}%) | {mean_auc/100:.2f} ({int(std_auc)}%) |\"\n",
    "    print(f\"   {markdown_row}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"5fold_cv_results_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        print(f\"\\nâœ“ Resultados guardados en: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Error guardando resultados: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Ejecutar evaluaciÃ³n 5-fold cross-validation\n",
    "print(\"Iniciando evaluaciÃ³n 5-fold cross-validation de la mejor arquitectura...\\n\")\n",
    "print(\"Usando el checkpoint del mejor modelo encontrado durante la evoluciÃ³n.\\n\")\n",
    "cv_results = evaluate_5fold_cross_validation(best_genome, CONFIG, num_epochs=20, neuroevolution_instance=neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2964f-0098-4c42-bf72-e66c4ca0ed5a",
   "metadata": {},
   "source": [
    "## 10. EvaluaciÃ³n 5-Fold Cross-Validation con Checkpoint\n",
    "\n",
    "Esta secciÃ³n evalÃºa la mejor arquitectura encontrada usando **5-fold cross-validation**.\n",
    "\n",
    "### ðŸŽ¯ Ventajas del enfoque con checkpoints:\n",
    "\n",
    "1. **Eficiencia**: Se guarda el mejor modelo durante la evoluciÃ³n (no se reentrena desde cero)\n",
    "2. **Transfer Learning**: Los pesos pre-entrenados sirven como punto de partida para cada fold\n",
    "3. **GestiÃ³n de espacio**: Solo se mantiene el checkpoint del mejor modelo global\n",
    "4. **Robustez**: MÃ©tricas mÃ¡s confiables con intervalos de confianza\n",
    "\n",
    "### ðŸ“Š Proceso:\n",
    "\n",
    "1. Se carga el checkpoint del mejor modelo encontrado\n",
    "2. Para cada fold:\n",
    "   - Se inicializa un modelo con los pesos pre-entrenados\n",
    "   - Se fine-tunea con los datos de entrenamiento del fold\n",
    "   - Se evalÃºa en los datos de test del fold\n",
    "3. Se calculan mÃ©tricas agregadas (promedio Â± desviaciÃ³n estÃ¡ndar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e77d426",
   "metadata": {},
   "source": [
    "# Audio Hybrid Neuroevolution Notebook\n",
    "\n",
    "This notebook implements a hybrid neuroevolution process for audio classification (Parkinson detection). The system combines genetic algorithms with 1D convolutional neural networks to evolve optimal architectures for audio processing.\n",
    "\n",
    "## Main Features:\n",
    "- **Hybrid genetic algorithm**: Combines architecture and weight evolution\n",
    "- **1D Convolutional Networks**: Optimized for audio waveform processing\n",
    "- **Parallel 5-Fold Cross-Validation**: Each individual is evaluated on all 5 folds IN PARALLEL (fitness = average accuracy)\n",
    "- **Multi-threading**: Folds are trained simultaneously in separate threads for faster evaluation\n",
    "- **Adaptive mutation**: Dynamic mutation rate based on population diversity\n",
    "- **Audio dataset support**: Loads .npy files with train/val/test splits\n",
    "- **Intelligent stopping criteria**: By target fitness or maximum generations\n",
    "- **Complete visualization**: Shows progress and final best architecture\n",
    "\n",
    "## Objectives:\n",
    "1. Create initial population of 1D CNN architectures\n",
    "2. Evaluate fitness of each individual using **parallel 5-fold CV** (robust and faster with threading)\n",
    "3. Select best architectures (elitism)\n",
    "4. Apply crossover and mutation to create new generation\n",
    "5. Repeat process until convergence\n",
    "6. Display the best architecture found for Parkinson classification\n",
    "\n",
    "**✅ Performance**: Multi-threaded 5-fold CV provides robustness against overfitting while being much faster than sequential training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f49a8",
   "metadata": {},
   "source": [
    "---\n",
    "## ✨ CONFIGURACIÓN ACTUAL DEL DATASET ✨\n",
    "\n",
    "**Dataset configurado**: `files_all_real_syn_n` (Datos Reales + Sintéticos Mezclados)\n",
    "\n",
    "Este notebook está configurado para usar el **nuevo dataset** que combina:\n",
    "- 🎵 **Datos Reales**: Audios originales de pacientes  \n",
    "- 🤖 **Datos Sintéticos**: Audios generados por GANs (BigVSAN 40_1e5)\n",
    "\n",
    "**Ventajas de este dataset**:\n",
    "- Mayor diversidad de datos para entrenamiento\n",
    "- Combina la autenticidad de datos reales con la variedad de datos generados\n",
    "- Ideal para mejorar la generalización del modelo\n",
    "- Estratificación balanceada entre clases (control/patológico)\n",
    "\n",
    "**🚀 Parallel 5-Fold Cross-Validation durante la Evolución**: \n",
    "- **CADA** individuo se evalúa en **TODOS** los 5 folds **EN PARALELO**\n",
    "- Los 5 folds se entrenan **simultáneamente** en threads separados\n",
    "- El fitness es el **promedio** de accuracy de los 5 folds\n",
    "- ✅ **Mucho más rápido** que entrenamiento secuencial\n",
    "- ✅ **Más robusto** - evita sobreajuste a un fold específico\n",
    "\n",
    "**📊 Evaluación Final**: \n",
    "- Al terminar la evolución, la mejor arquitectura se vuelve a evaluar con 5-fold CV\n",
    "- Se reportan métricas completas (accuracy, sensitivity, specificity, F1, AUC)\n",
    "\n",
    "Para cambiar el dataset, modifica los parámetros `dataset_id` y `fold_id` en la celda de **Configuración** (Sección 2).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f3cfb",
   "metadata": {},
   "source": [
    "## 1. Required Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if not available.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('==')[0].split('[')[0])\n",
    "        print(f\"OK {package.split('==')[0]} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"OK {package} installed correctly\")\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"numpy>=1.21.0\",\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"tqdm>=4.64.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipywidgets>=8.0.0\"\n",
    "]\n",
    "\n",
    "print(\"Starting dependency installation for Hybrid Neuroevolution...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nAll dependencies have been verified/installed\")\n",
    "print(\"Restart the kernel if this is the first time installing torch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"\\nPyTorch {torch.__version__} installed correctly\")\n",
    "    print(f\"CUDA available: {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: PyTorch could not be installed correctly\")\n",
    "    print(\"Try installing manually with: pip install torch torchvision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865869c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Scientific libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Threading for parallel fold training\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Visualization and progress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device configured: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1181c2a",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main genetic algorithm configuration (updated for adaptive mutation & moderate elitism)\n",
    "CONFIG = {\n",
    "    # Genetic algorithm parameters\n",
    "    'population_size': 50,            # Population size\n",
    "    'max_generations': 1000,            # Maximum number of generations\n",
    "    'fitness_threshold': 95.0,        # Target fitness (% accuracy) - Adjusted for audio\n",
    "\n",
    "    # Adaptive mutation parameters\n",
    "    'base_mutation_rate': 0.35,       # Starting mutation rate (moderate)\n",
    "    'mutation_rate_min': 0.10,        # Lower bound for adaptive mutation\n",
    "    'mutation_rate_max': 0.80,        # Upper bound for adaptive mutation\n",
    "    'current_mutation_rate': 0.35,    # Will be updated dynamically each generation\n",
    "\n",
    "    'crossover_rate': 0.99,           # Crossover rate\n",
    "    'elite_percentage': 0.2,          # Moderate elitism (20%) instead of 40%\n",
    "\n",
    "    # Dataset selection (AUDIO ONLY)\n",
    "    'dataset': 'AUDIO',               # Audio dataset for Parkinson classification\n",
    "\n",
    "    # Dataset parameters for audio\n",
    "    'num_channels': 1,                # Input channels (1 for audio waveform)\n",
    "    'sequence_length': 240000,        # Audio sequence length (will be auto-detected)\n",
    "    'num_classes': 2,                 # Number of classes (control vs pathological)\n",
    "    'batch_size': 32,                 # Batch size for audio\n",
    "    'test_split': 0.2,                # Validation percentage\n",
    "\n",
    "    # Training parameters\n",
    "    'num_epochs': 30,                 # Max training epochs per evaluation (may stop earlier)\n",
    "    'learning_rate': 0.001,           # Base learning rate\n",
    "    'early_stopping_patience': 200,   # Max batches per epoch (quick partial epoch)\n",
    "\n",
    "    # Epoch-level early stopping\n",
    "    'epoch_patience': 3,              # Stop if no significant improvement after N evaluations\n",
    "    'improvement_threshold': 0.5,     # Minimum (absolute) accuracy gain (%) to reset patience\n",
    "\n",
    "    # Generation-level early stopping \n",
    "    'early_stopping_generations': 10, # Stop if no improvement in X generations\n",
    "    'min_improvement_threshold': 0.1, # Minimum fitness improvement (%) to reset counter\n",
    "\n",
    "    # Allowed architecture range for 1D Conv\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 5,             # Less layers for 1D audio\n",
    "    'min_fc_layers': 1,\n",
    "    'max_fc_layers': 3,               # Less FC layers\n",
    "    'min_filters': 8,\n",
    "    'max_filters': 128,               # Adjusted for 1D\n",
    "    'min_fc_nodes': 64,\n",
    "    'max_fc_nodes': 512,              # Smaller for audio classification\n",
    "\n",
    "    # Audio dataset configuration (OS-independent paths)\n",
    "    \n",
    "    'dataset_id': 'all_real_syn_n',   # Dataset ID - Mixed real + synthetic data\n",
    "    'fold_id': 'all_real_syn_n',      # Fold ID for files\n",
    "    'num_folds': 5,                   # Number of folds (all used during evolution)\n",
    "    'data_path': os.path.join('data', 'sets', 'folds_5'),  # OS-independent path\n",
    "    'normalization': {'mean': (0.0,), 'std': (1.0,)}  # Audio normalization\n",
    "}\n",
    "\n",
    "# Activation function mapping\n",
    "ACTIVATION_FUNCTIONS = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'tanh': nn.Tanh,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'selu': nn.SELU,\n",
    "}\n",
    "\n",
    "# Optimizer mapping\n",
    "OPTIMIZERS = {\n",
    "    'adam': optim.Adam,\n",
    "    'adamw': optim.AdamW,\n",
    "    'sgd': optim.SGD,\n",
    "    'rmsprop': optim.RMSprop,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded (adaptive mutation enabled, 1D Conv for audio):\")\n",
    "print(f\"   Dataset: Audio (Parkinson Classification)\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key not in ['normalization']:  # Hide normalization details\n",
    "        print(f\"   {key}: {value}\")\n",
    "print(f\"\\nAvailable activation functions: {list(ACTIVATION_FUNCTIONS.keys())}\")\n",
    "print(f\"Available optimizers: {list(OPTIMIZERS.keys())}\")\n",
    "print(f\"\\nOS-independent path configured: {CONFIG['data_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4964cc1",
   "metadata": {},
   "source": [
    "### Información sobre el Dataset de Audio\n",
    "\n",
    "**Dataset de Audio para Clasificación de Parkinson**: \n",
    "- Archivos de audio de voz (clasificación Parkinson)\n",
    "- Datos 1D de forma de onda procesada\n",
    "- Estructura: archivos .npy con train/val/test splits\n",
    "- Dificultad: **Alta** - Clasificación médica\n",
    "- Fitness objetivo recomendado: >85%\n",
    "- Clases: Control vs Pathological\n",
    "- Formato de archivos: `{data_path}/files_{fold_id}/X_train_{dataset_id}_fold_{fold}.npy`\n",
    "- Arquitectura: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC Layers\n",
    "\n",
    "**Configuración del Dataset:**\n",
    "- Modifica los parámetros en la celda de configuración:\n",
    "  - `dataset_id`: ID del dataset (ej: 'all_real_syn_n')\n",
    "  - `fold_id`: ID de la carpeta de folds (ej: 'all_real_syn_n')\n",
    "  - `data_path`: Ruta base a los datos (usa `os.path.join` para compatibilidad multiplataforma)\n",
    "\n",
    "**🔄 Uso de 5-Fold CV:**\n",
    "- **Durante la evolución**: Cada individuo se evalúa en los 5 folds automáticamente\n",
    "- **No se necesita** especificar `current_fold` (se usan todos)\n",
    "- El fitness es el **promedio** de los 5 folds\n",
    "\n",
    "**Nota sobre Rutas:**\n",
    "- Las rutas son **independientes del sistema operativo** (Windows/Linux/Mac)\n",
    "- Usa `os.path.join()` para construir rutas compatibles\n",
    "- Ejemplo: `os.path.join('data', 'sets', 'folds_5')` funciona en cualquier OS\n",
    "\n",
    "---\n",
    "\n",
    "### Tipos de Carpetas de Folds Disponibles (generadas por `create_5_folds.ipynb`)\n",
    "\n",
    "El notebook `generating_csv/create_5_folds.ipynb` genera **5 tipos de carpetas** con diferentes combinaciones de datos **reales** y **sintéticos** (generados por GANs) para experimentación:\n",
    "\n",
    "#### 1. **`files_real_N`** - Solo Datos Reales\n",
    "   - **Train**: Datos reales (`test_together_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Baseline con datos 100% reales\n",
    "   - **fold_id**: `'real_N'`\n",
    "   - **dataset_id**: `'real_N'`\n",
    "\n",
    "#### 2. **`files_real_40_1e5_N`** - Entrenamiento Sintético, Test Real\n",
    "   - **Train**: Datos sintéticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Evaluar si modelos entrenados con sintéticos generalizan a datos reales\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 3. **`files_syn_40_1e5_N`** - Solo Datos Sintéticos (mismo conjunto)\n",
    "   - **Train**: Datos sintéticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos sintéticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Uso**: Evaluar capacidad del modelo con datos 100% sintéticos\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 4. **`files_syn_1_N`** - Entrenamiento Sintético, Test Sintético Diferente\n",
    "   - **Train**: Datos sintéticos (`generated_together_train_40_1e5_N`)\n",
    "   - **Test**: Datos sintéticos diferentes (`test_together_syn_1_N`)\n",
    "   - **Uso**: Evaluar generalización entre diferentes conjuntos sintéticos\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 5. **`files_syn_all_N`** - Solo Datos Reales (mal nombrado probablemente)\n",
    "   - **Train**: Datos reales (`test_together_N`)\n",
    "   - **Test**: Datos reales (`test_together_N`)\n",
    "   - **Uso**: Similar a `files_real_N` (posible duplicado o error de nomenclatura)\n",
    "   - **fold_id**: `'40_1e5_N'`\n",
    "   - **dataset_id**: `'40_1e5_N'`\n",
    "\n",
    "#### 6. **`files_all_real_syn_n`** - ✨ Datos Reales + Sintéticos Mezclados ✨ **(NUEVO)**\n",
    "   - **Train**: Datos reales + sintéticos mezclados\n",
    "   - **Validation**: Datos reales + sintéticos mezclados\n",
    "   - **Test**: Datos reales + sintéticos mezclados\n",
    "   - **Uso**: Entrenar y evaluar con una mezcla equilibrada de datos reales y generados por GANs\n",
    "   - **fold_id**: `'all_real_syn_n'`\n",
    "   - **dataset_id**: `'all_real_syn_n'`\n",
    "   - **Ventajas**: Combina diversidad de datos sintéticos con autenticidad de datos reales\n",
    "   - **Configuración actual**: 🔵 **ESTE ES EL DATASET CONFIGURADO POR DEFECTO**\n",
    "\n",
    "**Nota**: Cada carpeta contiene 5 folds de validación cruzada con:\n",
    "- `X_train_{dataset_id}_fold_{1-5}.npy` y `y_train_{dataset_id}_fold_{1-5}.npy`\n",
    "- `X_val_{dataset_id}_fold_{1-5}.npy` y `y_val_{dataset_id}_fold_{1-5}.npy`\n",
    "- `X_test_{dataset_id}_fold_{1-5}.npy` y `y_test_{dataset_id}_fold_{1-5}.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af6d37",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config: dict) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the audio dataset according to configuration.\n",
    "    Returns train_loader and test_loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading audio dataset from: {config['data_path']}\")\n",
    "    print(f\"Dataset ID: {config['dataset_id']}, Fold: {config['current_fold']}\")\n",
    "    \n",
    "    # Construct paths following ResNet convention\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    # Check if directory exists\n",
    "    print(f\"\\nChecking data directory...\")\n",
    "    print(f\"   Looking for: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    if not os.path.exists(fold_files_directory):\n",
    "        print(f\"\\n❌ ERROR: Directory not found!\")\n",
    "        print(f\"   Expected: {os.path.abspath(fold_files_directory)}\")\n",
    "        \n",
    "        # Try to find the correct path\n",
    "        possible_paths = [\n",
    "            os.path.join('..', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('.', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSearching for data in alternative locations:\")\n",
    "        for path in possible_paths:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"   {'✓' if exists else '✗'} {abs_path}\")\n",
    "            if exists:\n",
    "                fold_files_directory = path\n",
    "                print(f\"\\n✓ Found data at: {os.path.abspath(fold_files_directory)}\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\n❌ Could not find data directory!\\n\"\n",
    "                f\"   Tried paths:\\n\" + \n",
    "                \"\\n\".join([f\"      - {os.path.abspath(p)}\" for p in possible_paths]) +\n",
    "                f\"\\n\\n   Please check:\\n\"\n",
    "                f\"      1. CONFIG['data_path'] is correct\\n\"\n",
    "                f\"      2. The data files exist\\n\"\n",
    "                f\"      3. The fold_id '{config['fold_id']}' is correct\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   ✓ Directory found: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "def load_dataset(config: dict):\n",
    "    \"\"\"\n",
    "    Verifica que los datos existen y carga el primer fold para detectar sequence_length.\n",
    "    Durante la evolución, cada individuo cargará todos los folds automáticamente.\n",
    "    \n",
    "    Args:\n",
    "        config: Diccionario de configuración\n",
    "    \n",
    "    Returns:\n",
    "        None (solo actualiza config['sequence_length'])\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VERIFICANDO DISPONIBILIDAD DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Dataset ID: {config['dataset_id']}, Verificando los 5 folds...\")\n",
    "    \n",
    "    # Build directory path\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   Looking for: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    # If directory not found, try alternative locations\n",
    "    if not os.path.exists(fold_files_directory):\n",
    "        possible_paths = [\n",
    "            os.path.join('..', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "            os.path.join('.', 'data', 'sets', 'folds_5', f\"files_{config['fold_id']}\"),\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nSearching for data in alternative locations:\")\n",
    "        for path in possible_paths:\n",
    "            abs_path = os.path.abspath(path)\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"   {'✓' if exists else '✗'} {abs_path}\")\n",
    "            if exists:\n",
    "                fold_files_directory = path\n",
    "                print(f\"\\n✓ Found data at: {os.path.abspath(fold_files_directory)}\")\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\n❌ Could not find data directory!\\n\"\n",
    "                f\"   Tried paths:\\n\" + \n",
    "                \"\\n\".join([f\"      - {os.path.abspath(p)}\" for p in possible_paths]) +\n",
    "                f\"\\n\\n   Please check:\\n\"\n",
    "                f\"      1. CONFIG['data_path'] is correct\\n\"\n",
    "                f\"      2. The data files exist\\n\"\n",
    "                f\"      3. The fold_id '{config['fold_id']}' is correct\\n\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"   ✓ Directory found: {os.path.abspath(fold_files_directory)}\")\n",
    "    \n",
    "    dataset_id = config['dataset_id']\n",
    "    \n",
    "    # Check that all 5 folds exist\n",
    "    print(f\"\\nChecking for all 5 folds...\")\n",
    "    all_folds_ok = True\n",
    "    \n",
    "    for fold_num in range(1, 6):\n",
    "        required_files = [\n",
    "            f'X_train_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_train_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'X_val_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_val_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'X_test_{dataset_id}_fold_{fold_num}.npy',\n",
    "            f'y_test_{dataset_id}_fold_{fold_num}.npy',\n",
    "        ]\n",
    "        \n",
    "        fold_ok = True\n",
    "        for filename in required_files:\n",
    "            filepath = os.path.join(fold_files_directory, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                fold_ok = False\n",
    "                all_folds_ok = False\n",
    "                print(f\"   ✗ Fold {fold_num}: Missing {filename}\")\n",
    "                break\n",
    "        \n",
    "        if fold_ok:\n",
    "            print(f\"   ✓ Fold {fold_num}: All files present\")\n",
    "    \n",
    "    if not all_folds_ok:\n",
    "        raise FileNotFoundError(\n",
    "            f\"\\n❌ Some fold files are missing!\\n\"\n",
    "            f\"   Please ensure all 5 folds have complete data files.\\n\"\n",
    "            f\"   dataset_id: '{dataset_id}'\\n\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n✓ All 5 folds verified successfully!\")\n",
    "    \n",
    "    # Load first fold to detect sequence_length\n",
    "    print(f\"\\nLoading Fold 1 to detect sequence length...\")\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_1.npy'))\n",
    "    \n",
    "    print(f\"   Train samples: {x_train.shape}\")\n",
    "    \n",
    "    # Update sequence length from actual data\n",
    "    if len(x_train.shape) == 2:  # (samples, sequence_length)\n",
    "        config['sequence_length'] = x_train.shape[1]\n",
    "    elif len(x_train.shape) == 3:  # Already (samples, channels, sequence_length)\n",
    "        config['sequence_length'] = x_train.shape[2]\n",
    "    \n",
    "    print(f\"   Sequence length detected: {config['sequence_length']}\")\n",
    "    print(f\"\\n✓ Dataset verification complete!\")\n",
    "    print(f\"   During evolution, each individual will train on all 5 folds.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Verify dataset availability\n",
    "load_dataset(CONFIG)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READY FOR 5-FOLD CROSS-VALIDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"   Sequence length: {CONFIG['sequence_length']}\")\n",
    "print(f\"   Input channels: {CONFIG['num_channels']}\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Audio classification task: Control (0) vs Pathological (1)\")\n",
    "print(f\"\\n   ⚠️ Each individual will be evaluated on ALL 5 folds\")\n",
    "print(f\"   ⚠️ This makes evolution ~5x slower but much more robust\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52de67",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolvableCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolvable CNN class for 1D audio processing.\n",
    "    Uses Conv1D layers for audio/sequential data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, genome: dict, config: dict):\n",
    "        super(EvolvableCNN, self).__init__()\n",
    "        self.genome = genome\n",
    "        self.config = config\n",
    "        \n",
    "        # Build convolutional layers (1D for audio)\n",
    "        self.conv_layers = self._build_conv_layers()\n",
    "        \n",
    "        # Calculate output size after convolutions\n",
    "        self.conv_output_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = self._build_fc_layers()\n",
    "        \n",
    "    def _build_conv_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds 1D convolutional layers according to genome.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = self.config['num_channels']\n",
    "        normalization_type = self.genome.get('normalization_type', 'layer')  # Default to layer for compatibility\n",
    "\n",
    "        for i in range(self.genome['num_conv_layers']):\n",
    "            out_channels = self.genome['filters'][i]\n",
    "            kernel_size = self.genome['kernel_sizes'][i]\n",
    "            \n",
    "            # Ensure kernel size is odd and reasonable for 1D\n",
    "            kernel_size = max(3, kernel_size if kernel_size % 2 == 1 else kernel_size + 1)\n",
    "            padding = kernel_size // 2\n",
    "            \n",
    "            # 1D Convolutional layer\n",
    "            conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Normalization layer (Layer Normalization or Batch Normalization)\n",
    "            if normalization_type == 'layer':\n",
    "                # Layer Normalization: normaliza sobre features, no sobre batch\n",
    "                # Para Conv1d output de shape (batch, channels, length), normalizamos los channels\n",
    "                layers.append(nn.LayerNorm(out_channels))\n",
    "            else:\n",
    "                # Batch normalization (default)\n",
    "                layers.append(nn.BatchNorm1d(out_channels))\n",
    "            \n",
    "            # Activation function\n",
    "            activation_name = self.genome['activations'][i % len(self.genome['activations'])]\n",
    "            activation_func = ACTIVATION_FUNCTIONS[activation_name]()\n",
    "            layers.append(activation_func)\n",
    "            \n",
    "            # Max pooling (1D) - reduce sequence length\n",
    "            pool_size = 2 if i < self.genome['num_conv_layers'] - 1 else 2\n",
    "            layers.append(nn.MaxPool1d(pool_size, pool_size))\n",
    "            \n",
    "            # Optional dropout after pooling\n",
    "            if i < self.genome['num_conv_layers'] - 1:\n",
    "                layers.append(nn.Dropout(0.1))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def _calculate_conv_output_size(self) -> int:\n",
    "        \"\"\"Calculates output size after convolutional layers.\"\"\"\n",
    "        # Create dummy tensor to calculate size\n",
    "        dummy_input = torch.zeros(1, self.config['num_channels'], \n",
    "                                 self.config['sequence_length'])\n",
    "        \n",
    "        # Pass through convolutional layers\n",
    "        x = dummy_input\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten and get size\n",
    "        return x.view(-1).shape[0]\n",
    "    \n",
    "    def _build_fc_layers(self) -> nn.ModuleList:\n",
    "        \"\"\"Builds fully connected layers.\"\"\"\n",
    "        layers = nn.ModuleList()\n",
    "        \n",
    "        input_size = self.conv_output_size\n",
    "        normalization_type = self.genome.get('normalization_type', 'layer')  # Default to layer for compatibility\n",
    "\n",
    "        for i in range(self.genome['num_fc_layers']):\n",
    "            output_size = self.genome['fc_nodes'][i]\n",
    "            \n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            \n",
    "            # Normalization layer (Layer Normalization or Batch Normalization)\n",
    "            if normalization_type == 'layer':\n",
    "                # Layer Normalization for FC layers\n",
    "                layers.append(nn.LayerNorm(output_size))\n",
    "            else:\n",
    "                # Batch normalization for FC layers (default)\n",
    "                layers.append(nn.BatchNorm1d(output_size))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout if not last layer\n",
    "            if i < self.genome['num_fc_layers'] - 1:\n",
    "                layers.append(nn.Dropout(self.genome['dropout_rate']))\n",
    "            \n",
    "            input_size = output_size\n",
    "        \n",
    "        # Final classification layer\n",
    "        layers.append(nn.Linear(input_size, self.config['num_classes']))\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the network.\"\"\"\n",
    "        # Ensure input is in correct format for Conv1d\n",
    "        # Expected: (batch, channels, sequence_length)\n",
    "        if len(x.shape) == 2:  # (batch, sequence)\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_architecture_summary(self) -> str:\n",
    "        \"\"\"Returns an architecture summary.\"\"\"\n",
    "        summary = []\n",
    "        summary.append(f\"Conv1D Layers: {self.genome['num_conv_layers']}\")\n",
    "        summary.append(f\"Filters: {self.genome['filters']}\")\n",
    "        summary.append(f\"Kernel Sizes: {self.genome['kernel_sizes']}\")\n",
    "        summary.append(f\"FC Layers: {self.genome['num_fc_layers']}\")\n",
    "        summary.append(f\"FC Nodes: {self.genome['fc_nodes']}\")\n",
    "        summary.append(f\"Activations: {self.genome['activations']}\")\n",
    "        summary.append(f\"Normalization: {self.genome.get('normalization_type', 'batch')}\")\n",
    "        summary.append(f\"Dropout: {self.genome['dropout_rate']:.3f}\")\n",
    "        summary.append(f\"Optimizer: {self.genome['optimizer']}\")\n",
    "        summary.append(f\"Learning Rate: {self.genome['learning_rate']:.4f}\")\n",
    "        return \" | \".join(summary)\n",
    "\n",
    "print(\"EvolvableCNN class defined correctly (Conv1D for audio)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021c7d8",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithm Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be19766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_genome(config: dict) -> dict:\n",
    "    \"\"\"Creates a random genome within specified ranges (optimized for 1D audio).\"\"\"\n",
    "    # Number of layers\n",
    "    num_conv_layers = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "    num_fc_layers = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "\n",
    "    # Filters for each convolutional layer (progressive increase)\n",
    "    filters = []\n",
    "    base_filters = random.randint(config['min_filters'], config['min_filters'] * 2)\n",
    "    for i in range(num_conv_layers):\n",
    "        # Gradually increase filters in deeper layers\n",
    "        layer_filters = min(base_filters * (2 ** i), config['max_filters'])\n",
    "        filters.append(layer_filters)\n",
    "\n",
    "    # Kernel sizes (appropriate for 1D audio)\n",
    "    kernel_sizes = [random.choice([3, 5, 7, 9, 11]) for _ in range(num_conv_layers)]\n",
    "\n",
    "    # Nodes in fully connected layers (progressive decrease)\n",
    "    fc_nodes = []\n",
    "    base_fc = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "    for i in range(num_fc_layers):\n",
    "        layer_nodes = max(config['min_fc_nodes'], base_fc // (2 ** i))\n",
    "        fc_nodes.append(layer_nodes)\n",
    "\n",
    "    # Activation functions for each layer\n",
    "    activations = [random.choice(list(ACTIVATION_FUNCTIONS.keys())) for _ in range(max(num_conv_layers, num_fc_layers))]\n",
    "\n",
    "    # Other parameters (adjusted for audio)\n",
    "    dropout_rate = random.uniform(0.2, 0.5)\n",
    "    learning_rate = random.choice([0.001, 0.0005, 0.0001, 0.00005])\n",
    "    optimizer = random.choice(list(OPTIMIZERS.keys()))\n",
    "    normalization_type = random.choice(['batch', 'layer'])  # Evolve normalization type\n",
    "\n",
    "    genome = {\n",
    "        'num_conv_layers': num_conv_layers,\n",
    "        'num_fc_layers': num_fc_layers,\n",
    "        'filters': filters,\n",
    "        'kernel_sizes': kernel_sizes,\n",
    "        'fc_nodes': fc_nodes,\n",
    "        'activations': activations,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'learning_rate': learning_rate,\n",
    "        'optimizer': optimizer,\n",
    "        'normalization_type': normalization_type,\n",
    "        'fitness': 0.0,\n",
    "        'id': str(uuid.uuid4())[:8]\n",
    "    }\n",
    "    return genome\n",
    "\n",
    "def mutate_genome(genome: dict, config: dict) -> dict:\n",
    "    \"\"\"Applies mutation to a genome using adaptive mutation rate.\"\"\"\n",
    "    mutated_genome = copy.deepcopy(genome)\n",
    "    mutation_rate = config['current_mutation_rate']  # adaptive\n",
    "\n",
    "    # Mutate number of convolutional layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_conv_layers'] = random.randint(config['min_conv_layers'], config['max_conv_layers'])\n",
    "        num_conv = mutated_genome['num_conv_layers']\n",
    "        mutated_genome['filters'] = mutated_genome['filters'][:num_conv]\n",
    "        mutated_genome['kernel_sizes'] = mutated_genome['kernel_sizes'][:num_conv]\n",
    "        while len(mutated_genome['filters']) < num_conv:\n",
    "            mutated_genome['filters'].append(random.randint(config['min_filters'], config['max_filters']))\n",
    "        while len(mutated_genome['kernel_sizes']) < num_conv:\n",
    "            mutated_genome['kernel_sizes'].append(random.choice([3, 5, 7, 9, 11]))\n",
    "\n",
    "    # Mutate filters\n",
    "    for i in range(len(mutated_genome['filters'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['filters'][i] = random.randint(config['min_filters'], config['max_filters'])\n",
    "\n",
    "    # Mutate kernel sizes (1D appropriate sizes)\n",
    "    for i in range(len(mutated_genome['kernel_sizes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['kernel_sizes'][i] = random.choice([3, 5, 7, 9, 11, 13, 15])\n",
    "\n",
    "    # Mutate number of FC layers\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['num_fc_layers'] = random.randint(config['min_fc_layers'], config['max_fc_layers'])\n",
    "        num_fc = mutated_genome['num_fc_layers']\n",
    "        mutated_genome['fc_nodes'] = mutated_genome['fc_nodes'][:num_fc]\n",
    "        while len(mutated_genome['fc_nodes']) < num_fc:\n",
    "            mutated_genome['fc_nodes'].append(random.randint(config['min_fc_nodes'], config['max_fc_nodes']))\n",
    "\n",
    "    # Mutate FC nodes\n",
    "    for i in range(len(mutated_genome['fc_nodes'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['fc_nodes'][i] = random.randint(config['min_fc_nodes'], config['max_fc_nodes'])\n",
    "\n",
    "    # Mutate activation functions\n",
    "    for i in range(len(mutated_genome['activations'])):\n",
    "        if random.random() < mutation_rate:\n",
    "            mutated_genome['activations'][i] = random.choice(list(ACTIVATION_FUNCTIONS.keys()))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['dropout_rate'] = random.uniform(0.2, 0.6)\n",
    "\n",
    "    # Mutate learning rate (audio-specific range)\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['learning_rate'] = random.choice([0.001, 0.0005, 0.0001, 0.00005, 0.00001])\n",
    "\n",
    "    # Mutate optimizer\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['optimizer'] = random.choice(list(OPTIMIZERS.keys()))\n",
    "\n",
    "    # Mutate normalization type (batch or layer)\n",
    "    if random.random() < mutation_rate:\n",
    "        mutated_genome['normalization_type'] = random.choice(['batch', 'layer'])\n",
    "\n",
    "    mutated_genome['id'] = str(uuid.uuid4())[:8]\n",
    "    mutated_genome['fitness'] = 0.0\n",
    "    return mutated_genome\n",
    "\n",
    "def crossover_genomes(parent1: dict, parent2: dict, config: dict) -> Tuple[dict, dict]:\n",
    "    \"\"\"Performs crossover between two genomes.\"\"\"\n",
    "    if random.random() > config['crossover_rate']:\n",
    "        return copy.deepcopy(parent1), copy.deepcopy(parent2)\n",
    "\n",
    "    child1 = copy.deepcopy(parent1)\n",
    "    child2 = copy.deepcopy(parent2)\n",
    "\n",
    "    # Crossover scalar parameters\n",
    "    for key in ['num_conv_layers', 'num_fc_layers', 'dropout_rate', 'learning_rate', 'optimizer', 'normalization_type']:\n",
    "        if random.random() < 0.5:\n",
    "            child1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "    # Crossover lists (random cut point)\n",
    "    for list_key in ['filters', 'kernel_sizes', 'fc_nodes', 'activations']:\n",
    "        if random.random() < 0.5:\n",
    "            list1 = child1[list_key]\n",
    "            list2 = child2[list_key]\n",
    "            if len(list1) > 1 and len(list2) > 1:\n",
    "                point1 = random.randint(1, len(list1) - 1)\n",
    "                point2 = random.randint(1, len(list2) - 1)\n",
    "                child1[list_key] = list1[:point1] + list2[point2:]\n",
    "                child2[list_key] = list2[:point2] + list1[point1:]\n",
    "\n",
    "    child1['id'] = str(uuid.uuid4())[:8]\n",
    "    child2['id'] = str(uuid.uuid4())[:8]\n",
    "    child1['fitness'] = 0.0\n",
    "    child2['fitness'] = 0.0\n",
    "    return child1, child2\n",
    "\n",
    "print(\"Genetic functions updated for adaptive mutation and 1D audio processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a74a50",
   "metadata": {},
   "source": [
    "## 6. Hybrid Neuroevolution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNeuroevolution:\n",
    "    \"\"\"Main class that implements hybrid neuroevolution with 5-fold CV and adaptive mutation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_individual = None\n",
    "        self.fitness_history = []\n",
    "        self.generation_stats = []\n",
    "        self.best_checkpoint_path = None  # Ruta del checkpoint del mejor modelo\n",
    "        \n",
    "        # Early stopping configuration at generation level\n",
    "        self.generations_without_improvement = 0\n",
    "        self.best_fitness_overall = -float('inf')\n",
    "        self.min_improvement_threshold = 0.1  # Mínima mejora en fitness (%) para resetear contador\n",
    "        self.max_generations_without_improvement = config.get('early_stopping_generations', 10)\n",
    "\n",
    "    def initialize_population(self):\n",
    "        print(f\"Initializing population of {self.config['population_size']} individuals...\")\n",
    "        self.population = [create_random_genome(self.config) for _ in range(self.config['population_size'])]\n",
    "        print(f\"Population initialized with {len(self.population)} individuals\")\n",
    "    \n",
    "    def save_best_checkpoint(self, genome: dict, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Guarda el checkpoint del mejor modelo global y elimina el anterior.\n",
    "        \n",
    "        Args:\n",
    "            genome: Genoma del mejor modelo\n",
    "            model: Modelo de PyTorch a guardar\n",
    "        \"\"\"\n",
    "        # Crear directorio para checkpoints si no existe\n",
    "        checkpoint_dir = \"checkpoints\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Eliminar checkpoint anterior si existe\n",
    "        if self.best_checkpoint_path and os.path.exists(self.best_checkpoint_path):\n",
    "            try:\n",
    "                os.remove(self.best_checkpoint_path)\n",
    "                print(f\"      ✓ Checkpoint anterior eliminado: {self.best_checkpoint_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ✗ Error eliminando checkpoint anterior: {e}\")\n",
    "        \n",
    "        # Crear nuevo checkpoint\n",
    "        checkpoint_filename = f\"best_model_gen{self.generation}_id{genome['id']}_fitness{genome['fitness']:.2f}.pth\"\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
    "        \n",
    "        # Guardar modelo y genoma\n",
    "        checkpoint_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'genome': genome,\n",
    "            'generation': self.generation,\n",
    "            'fitness': genome['fitness'],\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            self.best_checkpoint_path = checkpoint_path\n",
    "            print(f\"      ✓ Nuevo checkpoint guardado: {checkpoint_path}\")\n",
    "            print(f\"        Fitness: {genome['fitness']:.2f}%, ID: {genome['id']}, Gen: {self.generation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ✗ Error guardando checkpoint: {e}\")\n",
    "    \n",
    "    def load_best_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Carga el mejor checkpoint guardado.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (genome, model) o (None, None) si no hay checkpoint\n",
    "        \"\"\"\n",
    "        if not self.best_checkpoint_path or not os.path.exists(self.best_checkpoint_path):\n",
    "            print(\"No hay checkpoint disponible para cargar\")\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            checkpoint_data = torch.load(self.best_checkpoint_path, map_location=device)\n",
    "            genome = checkpoint_data['genome']\n",
    "            \n",
    "            # Crear modelo y cargar pesos\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "            \n",
    "            print(f\"✓ Checkpoint cargado exitosamente: {self.best_checkpoint_path}\")\n",
    "            print(f\"  Fitness: {checkpoint_data['fitness']:.2f}%, Gen: {checkpoint_data['generation']}, ID: {genome['id']}\")\n",
    "            \n",
    "            return genome, model\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error cargando checkpoint: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def _train_one_fold(self, model, optimizer, criterion, train_loader, test_loader, genome_id: str, fold_num: int):\n",
    "        \"\"\"\n",
    "        Entrena y evalúa un modelo en un fold específico.\n",
    "        \n",
    "        Returns:\n",
    "            float: Accuracy del fold\n",
    "        \"\"\"\n",
    "        best_acc = 0.0\n",
    "        best_epoch = -1\n",
    "        patience_left = self.config['epoch_patience']\n",
    "        last_improvement_acc = 0.0\n",
    "        max_epochs = self.config['num_epochs']\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            # Train\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            batch_count = 0\n",
    "            max_batches = min(len(train_loader), self.config['early_stopping_patience'])\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                if batch_count >= max_batches:\n",
    "                    break\n",
    "            \n",
    "            avg_loss = running_loss / max(1, batch_count)\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            eval_batches = 0\n",
    "            max_eval_batches = min(len(test_loader), 20)\n",
    "            total_eval_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    total_eval_loss += loss.item()\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "                    eval_batches += 1\n",
    "                    if eval_batches >= max_eval_batches:\n",
    "                        break\n",
    "            \n",
    "            acc = 100.0 * correct / max(1, total)\n",
    "            avg_eval_loss = total_eval_loss / max(1, eval_batches)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            improvement = acc - last_improvement_acc\n",
    "            if improvement >= self.config['improvement_threshold']:\n",
    "                patience_left = self.config['epoch_patience']\n",
    "                last_improvement_acc = acc\n",
    "            else:\n",
    "                patience_left -= 1\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_epoch = epoch\n",
    "\n",
    "            # Solo mostrar cada 5 épocas para no saturar el log\n",
    "            if epoch % 5 == 0 or epoch == 1 or epoch == max_epochs:\n",
    "                print(f\"          Fold {fold_num} Epoch {epoch}: loss={avg_loss:.4f}, acc={acc:.2f}% (best={best_acc:.2f}%)\")\n",
    "\n",
    "            if patience_left <= 0:\n",
    "                print(f\"          Fold {fold_num}: Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        return best_acc\n",
    "\n",
    "    def _train_fold_in_thread(self, genome: dict, fold_num: int) -> Tuple[int, float, nn.Module]:\n",
    "        \"\"\"\n",
    "        Entrena un modelo en un fold específico (diseñado para ejecutarse en un thread).\n",
    "        \n",
    "        Args:\n",
    "            genome: Genoma del modelo\n",
    "            fold_num: Número de fold (1-5)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (fold_num, accuracy, model)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Cargar datos del fold\n",
    "            fold_train_loader, fold_test_loader = self._load_fold_data(fold_num)\n",
    "            \n",
    "            # Crear nuevo modelo para este fold\n",
    "            model = EvolvableCNN(genome, self.config).to(device)\n",
    "            optimizer_class = OPTIMIZERS[genome['optimizer']]\n",
    "            optimizer = optimizer_class(model.parameters(), lr=genome['learning_rate'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Entrenar y evaluar en este fold\n",
    "            fold_acc = self._train_one_fold(\n",
    "                model, optimizer, criterion, \n",
    "                fold_train_loader, fold_test_loader,\n",
    "                genome['id'], fold_num\n",
    "            )\n",
    "            \n",
    "            print(f\"      → Fold {fold_num} completed: {fold_acc:.2f}%\")\n",
    "            \n",
    "            return fold_num, fold_acc, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ERROR in Fold {fold_num}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return fold_num, 0.0, None\n",
    "\n",
    "    def evaluate_fitness(self, genome: dict) -> tuple:\n",
    "        \"\"\"\n",
    "        Evalúa el fitness de un genoma usando 5-fold cross-validation PARALELO.\n",
    "        Los 5 folds se entrenan en threads separados y se espera a que terminen todos.\n",
    "        El fitness final es el promedio de accuracy de los 5 folds.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (fitness, model) donde:\n",
    "                - fitness: promedio de accuracies de los 5 folds\n",
    "                - model: modelo entrenado en el mejor fold (para checkpoint)\n",
    "        \"\"\"\n",
    "        print(f\"      Training/Evaluating model {genome['id']} with PARALLEL 5-FOLD CROSS-VALIDATION\")\n",
    "        \n",
    "        fold_accuracies = {}\n",
    "        fold_models = {}\n",
    "        \n",
    "        try:\n",
    "            # Usar ThreadPoolExecutor para ejecutar los 5 folds en paralelo\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                # Enviar los 5 folds a threads separados\n",
    "                print(f\"      → Submitting 5 folds to thread pool...\")\n",
    "                futures = {\n",
    "                    executor.submit(self._train_fold_in_thread, genome, fold_num): fold_num\n",
    "                    for fold_num in range(1, 6)\n",
    "                }\n",
    "                \n",
    "                # Esperar a que todos los folds terminen\n",
    "                print(f\"      → Waiting for all 5 folds to complete...\")\n",
    "                for future in as_completed(futures):\n",
    "                    fold_num, fold_acc, model = future.result()\n",
    "                    fold_accuracies[fold_num] = fold_acc\n",
    "                    fold_models[fold_num] = model\n",
    "            \n",
    "            # Ordenar resultados por fold_num\n",
    "            sorted_folds = sorted(fold_accuracies.keys())\n",
    "            accuracies_list = [fold_accuracies[f] for f in sorted_folds]\n",
    "            \n",
    "            # Encontrar el mejor modelo\n",
    "            best_fold_num = max(fold_accuracies, key=fold_accuracies.get)\n",
    "            best_fold_acc = fold_accuracies[best_fold_num]\n",
    "            best_model = fold_models[best_fold_num]\n",
    "            \n",
    "            # Calcular fitness como promedio de los 5 folds\n",
    "            avg_fitness = np.mean(accuracies_list)\n",
    "            std_fitness = np.std(accuracies_list)\n",
    "            \n",
    "            print(f\"      ✓ PARALLEL 5-Fold CV Results for {genome['id']}:\")\n",
    "            print(f\"        Fold accuracies: {[f'{acc:.2f}%' for acc in accuracies_list]}\")\n",
    "            print(f\"        Average fitness: {avg_fitness:.2f}% ± {std_fitness:.2f}%\")\n",
    "            print(f\"        Best fold: Fold {best_fold_num} with {best_fold_acc:.2f}%\")\n",
    "            \n",
    "            return avg_fitness, best_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ERROR evaluating genome {genome['id']}: {e}\")\n",
    "            logger.warning(f\"Error evaluating genome {genome['id']}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return 0.0, None\n",
    "    \n",
    "    def _load_fold_data(self, fold_number: int):\n",
    "        \"\"\"\n",
    "        Carga los datos de un fold específico para el entrenamiento.\n",
    "        \n",
    "        Args:\n",
    "            fold_number: Número de fold (1-5)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple de (train_loader, test_loader)\n",
    "        \"\"\"\n",
    "        fold_files_directory = os.path.join(\n",
    "            self.config['data_path'], \n",
    "            f\"files_{self.config['fold_id']}\"\n",
    "        )\n",
    "        \n",
    "        dataset_id = self.config['dataset_id']\n",
    "        \n",
    "        # Cargar datos del fold\n",
    "        x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        y_train = np.load(os.path.join(fold_files_directory, f'y_train_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        x_val = np.load(os.path.join(fold_files_directory, f'X_val_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        y_val = np.load(os.path.join(fold_files_directory, f'y_val_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        x_test = np.load(os.path.join(fold_files_directory, f'X_test_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        y_test = np.load(os.path.join(fold_files_directory, f'y_test_{dataset_id}_fold_{fold_number}.npy'))\n",
    "        \n",
    "        # Reshape si es necesario\n",
    "        if len(x_train.shape) == 2:\n",
    "            x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "            x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "            x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "        \n",
    "        # Convertir a tensores\n",
    "        x_train_tensor = torch.FloatTensor(x_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train.astype(np.int64))\n",
    "        x_val_tensor = torch.FloatTensor(x_val)\n",
    "        y_val_tensor = torch.LongTensor(y_val.astype(np.int64))\n",
    "        x_test_tensor = torch.FloatTensor(x_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test.astype(np.int64))\n",
    "        \n",
    "        # Crear datasets\n",
    "        train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        x_eval = torch.cat([x_val_tensor, x_test_tensor], dim=0)\n",
    "        y_eval = torch.cat([y_val_tensor, y_test_tensor], dim=0)\n",
    "        test_dataset = torch.utils.data.TensorDataset(x_eval, y_eval)\n",
    "        \n",
    "        # Crear DataLoaders\n",
    "        fold_train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        fold_test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.config['batch_size'], \n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        return fold_train_loader, fold_test_loader\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        print(f\"\\nEvaluating population (Generation {self.generation})...\")\n",
    "        print(f\"Processing {len(self.population)} individuals...\")\n",
    "        fitness_scores = []\n",
    "        best_fitness_so_far = 0.0\n",
    "        current_global_best_fitness = self.best_individual['fitness'] if self.best_individual else 0.0\n",
    "        \n",
    "        for i, genome in enumerate(self.population):\n",
    "            print(f\"\\n   Evaluating individual {i+1}/{len(self.population)} (ID: {genome['id']})\")\n",
    "            print(f\"      Architecture: {genome['num_conv_layers']} conv + {genome['num_fc_layers']} fc, opt={genome['optimizer']}, lr={genome['learning_rate']}\")\n",
    "            \n",
    "            # Evaluar y obtener fitness y modelo\n",
    "            fitness, model = self.evaluate_fitness(genome)\n",
    "            genome['fitness'] = fitness\n",
    "            fitness_scores.append(fitness)\n",
    "            \n",
    "            if fitness > best_fitness_so_far:\n",
    "                best_fitness_so_far = fitness\n",
    "                print(f\"      New best fitness in this generation: {fitness:.2f}%!\")\n",
    "            \n",
    "            # Verificar si es un nuevo mejor global\n",
    "            if fitness > current_global_best_fitness:\n",
    "                print(f\"      🌟 NEW GLOBAL BEST! {fitness:.2f}% > {current_global_best_fitness:.2f}%\")\n",
    "                current_global_best_fitness = fitness\n",
    "                \n",
    "                # Guardar checkpoint (elimina el anterior automáticamente)\n",
    "                if model is not None:\n",
    "                    self.save_best_checkpoint(genome, model)\n",
    "            \n",
    "            print(f\"      Fitness obtained: {fitness:.2f}% | Best in generation: {best_fitness_so_far:.2f}% | Global best: {current_global_best_fitness:.2f}%\")\n",
    "        # Generation statistics\n",
    "        if fitness_scores:\n",
    "            avg_fitness = np.mean(fitness_scores)\n",
    "            max_fitness = np.max(fitness_scores)\n",
    "            min_fitness = np.min(fitness_scores)\n",
    "            std_fitness = np.std(fitness_scores)\n",
    "        else:\n",
    "            avg_fitness = max_fitness = min_fitness = std_fitness = 0.0\n",
    "\n",
    "        stats = {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': avg_fitness,\n",
    "            'max_fitness': max_fitness,\n",
    "            'min_fitness': min_fitness,\n",
    "            'std_fitness': std_fitness\n",
    "        }\n",
    "        self.generation_stats.append(stats)\n",
    "        self.fitness_history.append(max_fitness)\n",
    "\n",
    "        best_genome = max(self.population, key=lambda x: x['fitness'])\n",
    "        if self.best_individual is None or best_genome['fitness'] > self.best_individual['fitness']:\n",
    "            self.best_individual = copy.deepcopy(best_genome)\n",
    "            print(f\"\\nNew global best individual found!\")\n",
    "\n",
    "        print(f\"\\nGENERATION {self.generation} STATISTICS:\")\n",
    "        print(f\"   Maximum fitness: {max_fitness:.2f}%\")\n",
    "        print(f\"   Average fitness: {avg_fitness:.2f}%\")\n",
    "        print(f\"   Minimum fitness: {min_fitness:.2f}%\")\n",
    "        print(f\"   Standard deviation: {std_fitness:.2f}%\")\n",
    "        print(f\"   Best individual: {best_genome['id']} with {best_genome['fitness']:.2f}%\")\n",
    "        print(f\"   Global best individual: {self.best_individual['id']} with {self.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "    def selection_and_reproduction(self):\n",
    "        print(f\"\\nStarting selection and reproduction...\")\n",
    "        # Sort by fitness\n",
    "        self.population.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        elite_size = max(1, int(self.config['population_size'] * self.config['elite_percentage']))\n",
    "        elite = self.population[:elite_size]\n",
    "        print(f\"Selecting {elite_size} elite individuals:\")\n",
    "        for i, individual in enumerate(elite):\n",
    "            print(f\"   Elite {i+1}: {individual['id']} (fitness: {individual['fitness']:.2f}%)\")\n",
    "        new_population = copy.deepcopy(elite)\n",
    "        offspring_needed = self.config['population_size'] - len(new_population)\n",
    "        print(f\"Creating {offspring_needed} new individuals through crossover and mutation...\")\n",
    "        offspring_created = 0\n",
    "        while len(new_population) < self.config['population_size']:\n",
    "            parent1 = self.tournament_selection()\n",
    "            parent2 = self.tournament_selection()\n",
    "            child1, child2 = crossover_genomes(parent1, parent2, self.config)\n",
    "            child1 = mutate_genome(child1, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child1)\n",
    "            child2 = mutate_genome(child2, self.config)\n",
    "            if len(new_population) < self.config['population_size']:\n",
    "                new_population.append(child2)\n",
    "            offspring_created += 2\n",
    "            if offspring_created % 4 == 0:\n",
    "                print(f\"   Created {min(offspring_created, offspring_needed)} of {offspring_needed} new individuals...\")\n",
    "        self.population = new_population[:self.config['population_size']]\n",
    "        print(f\"New generation created with {len(self.population)} individuals\")\n",
    "        print(f\"   Elite preserved: {elite_size}\")\n",
    "        print(f\"   New individuals: {len(self.population) - elite_size}\")\n",
    "\n",
    "    def tournament_selection(self, tournament_size: int = 3) -> dict:\n",
    "        tournament = random.sample(self.population, min(tournament_size, len(self.population)))\n",
    "        return max(tournament, key=lambda x: x['fitness'])\n",
    "\n",
    "    def _update_adaptive_mutation(self):\n",
    "        # Diversity measured via std of fitness in last generation\n",
    "        if not self.generation_stats:\n",
    "            self.config['current_mutation_rate'] = self.config['base_mutation_rate']\n",
    "            return\n",
    "        last_std = self.generation_stats[-1]['std_fitness']\n",
    "        # Heuristic: more diversity -> lower mutation, low diversity -> higher\n",
    "        # Normalize std roughly assuming fitness in [0,100]\n",
    "        diversity_factor = min(1.0, last_std / 10.0)  # std 10% -> factor 1\n",
    "        # Invert: low diversity (small std) should raise mutation\n",
    "        inverted = 1 - diversity_factor\n",
    "        new_rate = self.config['base_mutation_rate'] + (inverted - 0.5) * 0.4  # adjust +/-0.2 range\n",
    "        new_rate = max(self.config['mutation_rate_min'], min(self.config['mutation_rate_max'], new_rate))\n",
    "        self.config['current_mutation_rate'] = round(new_rate, 4)\n",
    "        print(f\"Adaptive mutation rate updated to {self.config['current_mutation_rate']} (std_fitness={last_std:.2f})\")\n",
    "\n",
    "    def check_convergence(self) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica criterios de convergencia:\n",
    "        1. Target fitness alcanzado\n",
    "        2. Máximo de generaciones alcanzado\n",
    "        3. Early stopping: sin mejora en N generaciones\n",
    "        4. Estancamiento detectado en últimas generaciones\n",
    "        \"\"\"\n",
    "        # Criterion 1: Target fitness reached\n",
    "        if self.best_individual and self.best_individual['fitness'] >= self.config['fitness_threshold']:\n",
    "            print(f\"\\n✅ Target fitness reached! ({self.best_individual['fitness']:.2f}% >= {self.config['fitness_threshold']}%)\")\n",
    "            return True\n",
    "        \n",
    "        # Criterion 2: Maximum generations reached\n",
    "        if self.generation >= self.config['max_generations']:\n",
    "            print(f\"\\n⏱️ Maximum generations reached ({self.generation}/{self.config['max_generations']})\")\n",
    "            return True\n",
    "        \n",
    "        # Criterion 3: Early stopping - no improvement in N generations\n",
    "        if self.generation > 0:  # No check on generation 0\n",
    "            current_best = self.best_individual['fitness'] if self.best_individual else 0.0\n",
    "            \n",
    "            # Check if there's improvement compared to best overall\n",
    "            improvement = current_best - self.best_fitness_overall\n",
    "            \n",
    "            if improvement >= self.min_improvement_threshold:\n",
    "                # Significant improvement! Reset counter\n",
    "                self.best_fitness_overall = current_best\n",
    "                self.generations_without_improvement = 0\n",
    "                print(f\"\\n🔄 Improvement detected: {improvement:.2f}% | Generations without improvement: {self.generations_without_improvement}\")\n",
    "            else:\n",
    "                # No significant improvement\n",
    "                self.generations_without_improvement += 1\n",
    "                print(f\"\\n⏳ No significant improvement | Generations without improvement: {self.generations_without_improvement}/{self.max_generations_without_improvement}\")\n",
    "                \n",
    "                if self.generations_without_improvement >= self.max_generations_without_improvement:\n",
    "                    print(f\"\\n🛑 EARLY STOPPING: No improvement for {self.max_generations_without_improvement} generations\")\n",
    "                    print(f\"   Best fitness plateau: {self.best_fitness_overall:.2f}%\")\n",
    "                    return True\n",
    "        \n",
    "        # Criterion 4: Stagnation in last 3 generations (additional safety check)\n",
    "        if len(self.fitness_history) >= 3:\n",
    "            recent = self.fitness_history[-3:]\n",
    "            if max(recent) - min(recent) < 0.5:\n",
    "                print(f\"\\n📉 Stagnation detected in last 3 generations (all within {max(recent) - min(recent):.2f}%)\")\n",
    "                # Don't stop immediately, let generation-level early stopping handle it\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def evolve(self) -> dict:\n",
    "        print(\"STARTING HYBRID NEUROEVOLUTION PROCESS (adaptive mutation + generation-level early stopping)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"   Population: {self.config['population_size']} individuals\")\n",
    "        print(f\"   Maximum generations: {self.config['max_generations']}\")\n",
    "        print(f\"   Target fitness: {self.config['fitness_threshold']}%\")\n",
    "        print(f\"   Early stopping (generations): {self.config['early_stopping_generations']} without improvement\")\n",
    "        print(f\"   Min improvement threshold: {self.config['min_improvement_threshold']}%\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(\"=\"*80)\n",
    "        self.initialize_population()\n",
    "        while not self.check_convergence():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GENERATION {self.generation}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            self.evaluate_population()\n",
    "            if self.check_convergence():\n",
    "                break\n",
    "            self._update_adaptive_mutation()\n",
    "            self.selection_and_reproduction()\n",
    "            self.generation += 1\n",
    "            print(f\"\\nPreparing for next generation...\")\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVOLUTION COMPLETED!\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Best individual found:\")\n",
    "        print(f\"   ID: {self.best_individual['id']}\")\n",
    "        print(f\"   Fitness: {self.best_individual['fitness']:.2f}%\")\n",
    "        print(f\"   Origin generation: {self.generation}\")\n",
    "        print(f\"   Total generations processed: {self.generation + 1}\")\n",
    "        print(f\"   Generations without improvement: {self.generations_without_improvement}/{self.max_generations_without_improvement}\")\n",
    "        print(\"=\"*80)\n",
    "        return self.best_individual\n",
    "\n",
    "print(\"HybridNeuroevolution class updated with PARALLEL 5-fold CV evaluation and adaptive mutation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59597ef1",
   "metadata": {},
   "source": [
    "## 7. Evolution Process Execution\n",
    "\n",
    "### 🚀 Importante: Parallel 5-Fold Cross-Validation durante la Evolución\n",
    "\n",
    "**Cambio clave**: Ahora cada individuo se evalúa con **5-fold cross-validation PARALELO** durante el proceso evolutivo:\n",
    "\n",
    "1. **Durante la evolución**:\n",
    "   - Cada individuo se entrena y evalúa en **cada uno de los 5 folds SIMULTÁNEAMENTE**\n",
    "   - Los 5 folds se ejecutan en **threads separados** (paralelización)\n",
    "   - El **fitness final** es el **promedio** de las accuracies de los 5 folds\n",
    "   - Se espera a que **todos los folds terminen** antes de calcular el fitness\n",
    "   - Esto garantiza que la arquitectura seleccionada no esté sobreajustada a un fold específico\n",
    "\n",
    "2. **Ventajas de la paralelización**:\n",
    "   - 🚀 **Mucho más rápido**: Los 5 folds se entrenan simultáneamente (en threads)\n",
    "   - ✅ **Más robusto**: La mejor arquitectura generaliza mejor\n",
    "   - ✅ **Menos sesgado**: No depende de un solo split de datos\n",
    "   - 💡 **Aprovecha multi-core**: Usa múltiples núcleos de CPU para acelerar\n",
    "   \n",
    "3. **Proceso paralelo**:\n",
    "   - Generación 0: Se crean N individuos aleatorios\n",
    "   - Para cada individuo:\n",
    "     - **Thread Pool**: Se crean 5 threads (uno por fold)\n",
    "     - **Fold 1-5**: Se entrenan y evalúan **SIMULTÁNEAMENTE** → accuracy₁...accuracy₅\n",
    "     - **Espera**: Se espera a que **todos los threads terminen**\n",
    "     - **Fitness** = (accuracy₁ + accuracy₂ + accuracy₃ + accuracy₄ + accuracy₅) / 5\n",
    "   - Se seleccionan los mejores según fitness promedio\n",
    "   - Se aplica crossover y mutación\n",
    "   - Siguiente generación...\n",
    "\n",
    "4. **Rendimiento**:\n",
    "   - Tiempo de evaluación ≈ tiempo del fold más lento (en lugar de suma de todos)\n",
    "   - Aceleración teórica: ~5x más rápido que secuencial\n",
    "   - Aceleración real: depende del número de cores disponibles\n",
    "\n",
    "**Nota**: Para hacer pruebas rápidas, puedes reducir `population_size` y `max_generations` en la configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURACIÓN DE DATASET DE AUDIO\n",
    "# ==========================================\n",
    "\n",
    "# Ruta OS-independiente usando os.path.join\n",
    "CONFIG['data_path'] = os.path.join('data', 'sets', 'folds_5')\n",
    "\n",
    "# ==========================================\n",
    "# AJUSTES OPCIONALES\n",
    "# ==========================================\n",
    "\n",
    "# Ajustar población y generaciones si es necesario\n",
    "# CONFIG['population_size'] = 8\n",
    "# CONFIG['max_generations'] = 20\n",
    "# CONFIG['fitness_threshold'] = 85.0  # Para audio, 85% es buen objetivo\n",
    "# CONFIG['batch_size'] = 16  # Reducir si hay problemas de memoria\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AUDIO NEUROEVOLUTION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Dataset: Audio (Parkinson Classification)\")\n",
    "print(f\"   Dataset ID: {CONFIG['dataset_id']}\")\n",
    "print(f\"   Fold ID: {CONFIG['fold_id']}\")\n",
    "print(f\"   Number of folds: {CONFIG['num_folds']} (all used during evolution)\")\n",
    "print(f\"   Data Path: {CONFIG['data_path']}\")\n",
    "print(f\"   Number of channels: {CONFIG['num_channels']} (1D audio)\")\n",
    "print(f\"   Sequence length: {CONFIG['sequence_length']} (will be auto-detected)\")\n",
    "print(f\"   Number of classes: {CONFIG['num_classes']} (Control vs Pathological)\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Population: {CONFIG['population_size']} individuals\")\n",
    "print(f\"   Maximum generations: {CONFIG['max_generations']}\")\n",
    "print(f\"   Target fitness: {CONFIG['fitness_threshold']}%\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Platform: {os.name} ({'Windows' if os.name == 'nt' else 'Unix/Linux/Mac'})\")\n",
    "print(f\"   Parallelization: Enabled (5 threads per individual)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify dataset availability with the new configuration\n",
    "print(f\"\\nVerifying audio dataset...\")\n",
    "load_dataset(CONFIG)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET VERIFIED - READY FOR PARALLEL 5-FOLD CV EVOLUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize neuroevolution system\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStarting audio neuroevolution at {start_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Architecture: Conv1D -> BatchNorm1D -> Activation -> MaxPool1D -> FC\")\n",
    "print(f\"Each individual will be evaluated on all 5 folds IN PARALLEL\")\n",
    "print(f\"Using ThreadPoolExecutor with 5 workers (one per fold)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create system instance (no need for train/test loaders anymore)\n",
    "neuroevolution = HybridNeuroevolution(CONFIG)\n",
    "\n",
    "# Execute evolution process\n",
    "best_genome = neuroevolution.evolve()\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVOLUTION PROCESS COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Completed at: {end_time.strftime('%H:%M:%S')}\")\n",
    "print(f\"Total execution time: {execution_time}\")\n",
    "print(f\"Total generations: {neuroevolution.generation}\")\n",
    "print(f\"Best fitness achieved: {best_genome['fitness']:.2f}%\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e555d9b",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Function to visualize fitness evolution\n",
    "def plot_fitness_evolution(neuroevolution):\n",
    "    \"\"\"Plots fitness evolution across generations.\"\"\"\n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Extract data and filter 0.00 fitness\n",
    "    generations = []\n",
    "    avg_fitness = []\n",
    "    max_fitness = []\n",
    "    min_fitness = []\n",
    "    std_fitness = []\n",
    "    \n",
    "    for stat in neuroevolution.generation_stats:\n",
    "        # Only include if valid fitness (> 0.00)\n",
    "        if stat['max_fitness'] > 0.00:\n",
    "            generations.append(stat['generation'])\n",
    "            avg_fitness.append(stat['avg_fitness'])\n",
    "            max_fitness.append(stat['max_fitness'])\n",
    "            min_fitness.append(stat['min_fitness'])\n",
    "            std_fitness.append(stat['std_fitness'])\n",
    "    \n",
    "    if not generations:\n",
    "        print(\"WARNING: No valid fitness data to plot (all are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    # Graph 1: Fitness evolution\n",
    "    ax1.plot(generations, max_fitness, 'g-', linewidth=2, marker='o', label='Maximum Fitness')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, marker='s', label='Average Fitness')\n",
    "    ax1.plot(generations, min_fitness, 'r-', linewidth=2, marker='^', label='Minimum Fitness')\n",
    "    ax1.fill_between(generations, \n",
    "                     [max(0, avg - std) for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.2, color='blue')\n",
    "    \n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (%)')\n",
    "    ax1.set_title('Fitness Evolution by Generation (Excluding 0.00%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add target fitness line\n",
    "    ax1.axhline(y=CONFIG['fitness_threshold'], color='orange', linestyle='--', \n",
    "                label=f\"Target ({CONFIG['fitness_threshold']}%)\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Set Y axis limits for better visualization\n",
    "    y_min = max(0, min(min_fitness) - 5)\n",
    "    y_max = min(100, max(max_fitness) + 5)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Graph 2: Diversity (standard deviation)\n",
    "    ax2.plot(generations, std_fitness, 'purple', linewidth=2, marker='D')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Standard Deviation')\n",
    "    ax2.set_title('Population Diversity (Excluding 0.00%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show additional information\n",
    "    print(f\"Plotted data:\")\n",
    "    print(f\"   Generations with valid fitness: {len(generations)}\")\n",
    "    print(f\"   Best fitness achieved: {max(max_fitness):.2f}%\")\n",
    "    print(f\"   Final average fitness: {avg_fitness[-1]:.2f}%\")\n",
    "    if len(generations) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(generations)\n",
    "        print(f\"   WARNING: Excluded generations (0.00 fitness): {excluded}\")\n",
    "\n",
    "# Function to show detailed statistics\n",
    "def show_evolution_statistics(neuroevolution):\n",
    "    \"\"\"Shows detailed evolution statistics.\"\"\"\n",
    "    print(\"DETAILED EVOLUTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not neuroevolution.generation_stats:\n",
    "        print(\"WARNING: No statistics available\")\n",
    "        return\n",
    "    \n",
    "    # Filter statistics with valid fitness\n",
    "    valid_stats = [stat for stat in neuroevolution.generation_stats if stat['max_fitness'] > 0.00]\n",
    "    \n",
    "    if not valid_stats:\n",
    "        print(\"WARNING: No valid statistics (all fitness are 0.00)\")\n",
    "        return\n",
    "    \n",
    "    final_stats = valid_stats[-1]\n",
    "    \n",
    "    print(f\"Completed generations: {neuroevolution.generation}\")\n",
    "    print(f\"Generations with valid fitness: {len(valid_stats)}\")\n",
    "    if len(valid_stats) < len(neuroevolution.generation_stats):\n",
    "        excluded = len(neuroevolution.generation_stats) - len(valid_stats)\n",
    "        print(f\"WARNING: Generations with 0.00 fitness (excluded): {excluded}\")\n",
    "    \n",
    "    print(f\"\\nFINAL STATISTICS (excluding 0.00 fitness):\")\n",
    "    print(f\"   Final best fitness: {final_stats['max_fitness']:.2f}%\")\n",
    "    print(f\"   Final average fitness: {final_stats['avg_fitness']:.2f}%\")\n",
    "    print(f\"   Final minimum fitness: {final_stats['min_fitness']:.2f}%\")\n",
    "    print(f\"   Final standard deviation: {final_stats['std_fitness']:.2f}%\")\n",
    "    \n",
    "    # Progress across generations\n",
    "    if len(valid_stats) > 1:\n",
    "        initial_max = valid_stats[0]['max_fitness']\n",
    "        final_max = valid_stats[-1]['max_fitness']\n",
    "        improvement = final_max - initial_max\n",
    "        \n",
    "        print(f\"\\nPROGRESS:\")\n",
    "        print(f\"   Initial fitness: {initial_max:.2f}%\")\n",
    "        print(f\"   Final fitness: {final_max:.2f}%\")\n",
    "        print(f\"   Total improvement: {improvement:.2f}%\")\n",
    "        if initial_max > 0:\n",
    "            print(f\"   Relative improvement: {(improvement/initial_max)*100:.1f}%\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\nCONVERGENCE CRITERIA:\")\n",
    "    if neuroevolution.best_individual and neuroevolution.best_individual['fitness'] >= CONFIG['fitness_threshold']:\n",
    "        print(f\"   OK: Target fitness reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    else:\n",
    "        print(f\"   ERROR: Target fitness NOT reached ({CONFIG['fitness_threshold']}%)\")\n",
    "    \n",
    "    if neuroevolution.generation >= CONFIG['max_generations']:\n",
    "        print(f\"   TIME: Maximum generations reached ({CONFIG['max_generations']})\")\n",
    "    \n",
    "    # Additional performance statistics\n",
    "    all_max_fitness = [stat['max_fitness'] for stat in valid_stats]\n",
    "    all_avg_fitness = [stat['avg_fitness'] for stat in valid_stats]\n",
    "    \n",
    "    print(f\"\\nGENERAL STATISTICS:\")\n",
    "    print(f\"   Best fitness of entire evolution: {max(all_max_fitness):.2f}%\")\n",
    "    print(f\"   Average fitness of entire evolution: {np.mean(all_avg_fitness):.2f}%\")\n",
    "    print(f\"   Average improvement per generation: {(max(all_max_fitness) - min(all_max_fitness))/len(valid_stats):.2f}%\")\n",
    "    \n",
    "    if neuroevolution.best_individual:\n",
    "        print(f\"\\nBest individual ID: {neuroevolution.best_individual['id']}\")\n",
    "        print(f\"Best individual fitness: {neuroevolution.best_individual['fitness']:.2f}%\")\n",
    "\n",
    "# Additional function for failure analysis\n",
    "def analyze_failed_evaluations(neuroevolution):\n",
    "    \"\"\"Analyzes evaluations that resulted in 0.00 fitness.\"\"\"\n",
    "    print(\"\\nFAILED EVALUATIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_generations = len(neuroevolution.generation_stats)\n",
    "    failed_generations = len([stat for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00])\n",
    "    \n",
    "    if failed_generations == 0:\n",
    "        print(\"OK: No failed evaluations (0.00 fitness)\")\n",
    "        return\n",
    "    \n",
    "    success_rate = ((total_generations - failed_generations) / total_generations) * 100\n",
    "    \n",
    "    print(f\"Failure summary:\")\n",
    "    print(f\"   Total generations: {total_generations}\")\n",
    "    print(f\"   Failed generations: {failed_generations}\")\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if failed_generations > 0:\n",
    "        failed_gens = [stat['generation'] for stat in neuroevolution.generation_stats if stat['max_fitness'] == 0.00]\n",
    "        print(f\"   Generations with failures: {failed_gens}\")\n",
    "        \n",
    "        print(f\"\\nPossible causes of 0.00 fitness:\")\n",
    "        print(f\"   • Errors in model architecture\")\n",
    "        print(f\"   • Memory problems (GPU/RAM)\")\n",
    "        print(f\"   • Invalid hyperparameter configurations\")\n",
    "        print(f\"   • Errors during training\")\n",
    "\n",
    "# Execute visualizations\n",
    "plot_fitness_evolution(neuroevolution)\n",
    "show_evolution_statistics(neuroevolution)\n",
    "analyze_failed_evaluations(neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6688660",
   "metadata": {},
   "source": [
    "## 9. BEST ARCHITECTURE FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a847dd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_best_architecture(best_genome, config):\n",
    "    \"\"\"\n",
    "    Shows the best architecture found in detailed and visual format.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"    BEST EVOLVED ARCHITECTURE (1D AUDIO)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # General information\n",
    "    print(f\"\\nGENERAL INFORMATION:\")\n",
    "    print(f\"   Genome ID: {best_genome['id']}\")\n",
    "    print(f\"   Fitness Achieved: {best_genome['fitness']:.2f}%\")\n",
    "    print(f\"   Generation: {neuroevolution.generation}\")\n",
    "    print(f\"   Dataset: {config['dataset']}\")\n",
    "    print(f\"   Dataset ID: {config.get('dataset_id', 'N/A')}\")\n",
    "    print(f\"   Fold: {config.get('current_fold', 'N/A')}\")\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"\\nNETWORK ARCHITECTURE:\")\n",
    "    print(f\"   Input: 1D Audio Signal (length={config['sequence_length']})\")\n",
    "    print(f\"   Convolutional Layers (Conv1D): {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   Fully Connected Layers: {best_genome['num_fc_layers']}\")\n",
    "    print(f\"   Output: {config['num_classes']} classes\")\n",
    "    \n",
    "    print(f\"\\nCONVOLUTIONAL LAYER DETAILS (1D):\")\n",
    "    for i in range(best_genome['num_conv_layers']):\n",
    "        filters = best_genome['filters'][i]\n",
    "        kernel = best_genome['kernel_sizes'][i]\n",
    "        activation = best_genome['activations'][i % len(best_genome['activations'])]\n",
    "        print(f\"   Conv1D-{i+1}: {filters} filters, kernel_size={kernel}, activation={activation}\")\n",
    "        print(f\"             -> BatchNorm1D -> {activation.upper()} -> MaxPool1D(2)\")\n",
    "    \n",
    "    print(f\"\\nFULLY CONNECTED LAYER DETAILS:\")\n",
    "    for i, nodes in enumerate(best_genome['fc_nodes']):\n",
    "        print(f\"   FC{i+1}: {nodes} neurons -> BatchNorm1D -> ReLU -> Dropout({best_genome['dropout_rate']:.3f})\")\n",
    "    print(f\"   Output: {config['num_classes']} neurons (Control vs Pathological)\")\n",
    "    \n",
    "    print(f\"\\nHYPERPARAMETERS:\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer'].upper()}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']:.6f}\")\n",
    "    print(f\"   Dropout Rate: {best_genome['dropout_rate']:.3f}\")\n",
    "    print(f\"   Activation Functions: {', '.join(set(best_genome['activations']))}\")\n",
    "    \n",
    "    # Create and show final model\n",
    "    print(f\"\\nCREATING FINAL MODEL...\")\n",
    "    try:\n",
    "        final_model = EvolvableCNN(best_genome, config)\n",
    "        total_params = sum(p.numel() for p in final_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"   Model created successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "        \n",
    "        # Architecture summary\n",
    "        print(f\"\\nCOMPACT SUMMARY:\")\n",
    "        print(f\"   {final_model.get_architecture_summary()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR creating model: {e}\")\n",
    "    \n",
    "    # Visualization in table format\n",
    "    print(f\"\\nSUMMARY TABLE:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Parameter':<25} {'Value':<30} {'Description':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'ID':<25} {best_genome['id']:<30} {'Unique identifier':<25}\")\n",
    "    print(f\"{'Fitness':<25} {best_genome['fitness']:.2f}%{'':<25} {'Accuracy achieved':<25}\")\n",
    "    print(f\"{'Architecture':<25} {'Conv1D + FC':<30} {'1D Convolutional':<25}\")\n",
    "    print(f\"{'Conv Layers':<25} {best_genome['num_conv_layers']:<30} {'Conv1D layers':<25}\")\n",
    "    print(f\"{'FC Layers':<25} {best_genome['num_fc_layers']:<30} {'FC layers':<25}\")\n",
    "    print(f\"{'Optimizer':<25} {best_genome['optimizer']:<30} {'Optimization algorithm':<25}\")\n",
    "    print(f\"{'Learning Rate':<25} {best_genome['learning_rate']:<30.6f} {'Learning rate':<25}\")\n",
    "    print(f\"{'Dropout':<25} {best_genome['dropout_rate']:<30} {'Dropout rate':<25}\")\n",
    "    print(f\"{'Input Length':<25} {config['sequence_length']:<30} {'Audio sequence length':<25}\")\n",
    "    print(f\"{'Classes':<25} {config['num_classes']:<30} {'Binary classification':<25}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Comparison with initial configuration\n",
    "    print(f\"\\nCOMPARISON WITH OBJECTIVES:\")\n",
    "    if best_genome['fitness'] >= config['fitness_threshold']:\n",
    "        print(f\"   ✓ TARGET REACHED: {best_genome['fitness']:.2f}% >= {config['fitness_threshold']}%\")\n",
    "    else:\n",
    "        print(f\"   ✗ TARGET NOT REACHED: {best_genome['fitness']:.2f}% < {config['fitness_threshold']}%\")\n",
    "        print(f\"     Gap: {config['fitness_threshold'] - best_genome['fitness']:.2f}%\")\n",
    "    \n",
    "    print(f\"   Generations used: {neuroevolution.generation}/{config['max_generations']}\")\n",
    "    \n",
    "    # Save information to JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"best_architecture_audio_{timestamp}.json\"\n",
    "    \n",
    "    results_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'execution_time': str(execution_time),\n",
    "        'dataset_type': 'audio_1D',\n",
    "        'dataset_id': config.get('dataset_id', 'N/A'),\n",
    "        'fold': config.get('current_fold', 'N/A'),\n",
    "        'config_used': {k: v for k, v in config.items() if not k.startswith('_')},\n",
    "        'best_genome': best_genome,\n",
    "        'final_generation': neuroevolution.generation,\n",
    "        'evolution_stats': neuroevolution.generation_stats\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        print(f\"\\n✓ Results saved to: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ WARNING: Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"HYBRID NEUROEVOLUTION FOR AUDIO COMPLETED!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Show the best architecture found\n",
    "display_best_architecture(best_genome, CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar información del checkpoint guardado\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMACIÓN DEL CHECKPOINT DEL MEJOR MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if neuroevolution.best_checkpoint_path:\n",
    "    print(f\"\\n✓ Checkpoint guardado en: {neuroevolution.best_checkpoint_path}\")\n",
    "    \n",
    "    # Obtener información del archivo\n",
    "    import os\n",
    "    if os.path.exists(neuroevolution.best_checkpoint_path):\n",
    "        file_size = os.path.getsize(neuroevolution.best_checkpoint_path)\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        print(f\"  Tamaño: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Cargar y mostrar información del checkpoint\n",
    "        checkpoint_data = torch.load(neuroevolution.best_checkpoint_path, map_location=device)\n",
    "        print(f\"\\n  Información del modelo guardado:\")\n",
    "        print(f\"    Generación: {checkpoint_data['generation']}\")\n",
    "        print(f\"    Fitness: {checkpoint_data['fitness']:.2f}%\")\n",
    "        print(f\"    ID Genoma: {checkpoint_data['genome']['id']}\")\n",
    "        print(f\"    Arquitectura: {checkpoint_data['genome']['num_conv_layers']} Conv1D + {checkpoint_data['genome']['num_fc_layers']} FC\")\n",
    "        print(f\"    Optimizador: {checkpoint_data['genome']['optimizer']}\")\n",
    "        print(f\"    Learning Rate: {checkpoint_data['genome']['learning_rate']}\")\n",
    "        \n",
    "        print(f\"\\n  Este checkpoint se usará como punto de partida para el 5-fold CV\")\n",
    "        print(f\"  (Transfer learning desde el modelo pre-entrenado)\")\n",
    "    else:\n",
    "        print(f\"  ✗ Archivo no encontrado\")\n",
    "else:\n",
    "    print(\"\\n✗ No hay checkpoint disponible\")\n",
    "    print(\"  El 5-fold CV entrenará desde cero\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cb925",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 Resumen del Flujo con Checkpoints\n",
    "\n",
    "```\n",
    "PROCESO DE NEUROEVOLUCIÓN CON CHECKPOINTS:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. EVOLUCIÓN (Múltiples Generaciones)\n",
    "   │\n",
    "   ├─ Para cada individuo:\n",
    "   │  ├─ Entrenar y evaluar\n",
    "   │  ├─ Calcular fitness\n",
    "   │  │\n",
    "   │  └─ SI fitness > mejor_global:\n",
    "   │     ├─ 🌟 NUEVO MEJOR GLOBAL\n",
    "   │     ├─ ✗ Eliminar checkpoint anterior\n",
    "   │     └─ ✓ Guardar nuevo checkpoint\n",
    "   │\n",
    "   └─ Continuar hasta convergencia\n",
    "\n",
    "2. AL FINALIZAR LA EVOLUCIÓN\n",
    "   │\n",
    "   └─ Se tiene el checkpoint del MEJOR modelo global\n",
    "\n",
    "3. EVALUACIÓN 5-FOLD CROSS-VALIDATION\n",
    "   │\n",
    "   ├─ ✓ Cargar checkpoint del mejor modelo\n",
    "   │\n",
    "   ├─ Para cada fold (1 a 5):\n",
    "   │  ├─ Crear modelo nuevo\n",
    "   │  ├─ Inicializar con pesos pre-entrenados (Transfer Learning)\n",
    "   │  ├─ Fine-tuning con datos del fold\n",
    "   │  └─ Evaluar y guardar métricas\n",
    "   │\n",
    "   └─ Calcular promedios y desviaciones estándar\n",
    "\n",
    "4. RESULTADOS FINALES\n",
    "   └─ Métricas robustas para la tabla de comparación\n",
    "```\n",
    "\n",
    "### ✨ Beneficios de este enfoque:\n",
    "\n",
    "- ✅ **Ahorro de espacio**: Solo 1 checkpoint (el mejor)\n",
    "- ✅ **Eficiencia**: Transfer learning en lugar de entrenar desde cero\n",
    "- ✅ **Robustez**: Métricas con 5-fold CV\n",
    "- ✅ **Trazabilidad**: Se mantiene el historial del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acc991",
   "metadata": {},
   "source": [
    "## 📝 Nota Importante\n",
    "\n",
    "**Este enfoque tiene mucho sentido porque:**\n",
    "\n",
    "1. **Durante la evolución**, cada vez que un modelo supera el mejor fitness global:\n",
    "   - Se guarda automáticamente su checkpoint\n",
    "   - Se elimina el checkpoint anterior (ahorro de espacio)\n",
    "   - Se asegura que siempre tenemos el mejor modelo disponible\n",
    "\n",
    "2. **Para la evaluación 5-fold CV**:\n",
    "   - En lugar de entrenar 5 modelos desde cero (aleatorio)\n",
    "   - Se usan los pesos pre-entrenados del mejor modelo como inicio\n",
    "   - Esto es **Transfer Learning**, que típicamente da mejores resultados\n",
    "   - Cada fold hace fine-tuning con sus propios datos\n",
    "\n",
    "3. **Ventajas prácticas**:\n",
    "   - Si el proceso se interrumpe, no se pierde el mejor modelo\n",
    "   - Se puede reanudar la evaluación 5-fold desde el checkpoint\n",
    "   - Las métricas son más estables y representativas\n",
    "   - Se optimiza el uso de recursos (disco y tiempo de entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b04a",
   "metadata": {},
   "source": [
    "## 10. Evaluación Completa de Métricas (Tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def load_fold_data(config, fold_number):\n",
    "    \"\"\"\n",
    "    Carga los datos de un fold específico.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuración del sistema\n",
    "        fold_number: Número de fold (1-5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (train_loader, test_loader) para ese fold\n",
    "    \"\"\"\n",
    "    fold_files_directory = os.path.join(\n",
    "        config['data_path'], \n",
    "        f\"files_{config['fold_id']}\"\n",
    "    )\n",
    "    \n",
    "    fold_index = fold_number\n",
    "    dataset_id = config['dataset_id']\n",
    "    \n",
    "    # Cargar datos del fold\n",
    "    x_train = np.load(os.path.join(fold_files_directory, f'X_train_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_train = np.load(os.path.join(fold_files_directory, f'y_train_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    x_val = np.load(os.path.join(fold_files_directory, f'X_val_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_val = np.load(os.path.join(fold_files_directory, f'y_val_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    x_test = np.load(os.path.join(fold_files_directory, f'X_test_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    y_test = np.load(os.path.join(fold_files_directory, f'y_test_{dataset_id}_fold_{fold_index}.npy'))\n",
    "    \n",
    "    # Reshape si es necesario\n",
    "    if len(x_train.shape) == 2:\n",
    "        x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "        x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "        x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    x_train_tensor = torch.FloatTensor(x_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train.astype(np.int64))\n",
    "    x_val_tensor = torch.FloatTensor(x_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val.astype(np.int64))\n",
    "    x_test_tensor = torch.FloatTensor(x_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test.astype(np.int64))\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    x_eval = torch.cat([x_val_tensor, x_test_tensor], dim=0)\n",
    "    y_eval = torch.cat([y_val_tensor, y_test_tensor], dim=0)\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_eval, y_eval)\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    fold_train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    fold_test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return fold_train_loader, fold_test_loader\n",
    "\n",
    "\n",
    "def evaluate_single_fold(best_genome, config, fold_train_loader, fold_test_loader, fold_num, num_epochs=20, use_pretrained=False, pretrained_model=None):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa el modelo en un solo fold.\n",
    "    \n",
    "    Args:\n",
    "        best_genome: Genoma de la mejor arquitectura\n",
    "        config: Configuración del sistema\n",
    "        fold_train_loader: DataLoader de entrenamiento del fold\n",
    "        fold_test_loader: DataLoader de test del fold\n",
    "        fold_num: Número del fold\n",
    "        num_epochs: Épocas de entrenamiento\n",
    "        use_pretrained: Si True, usa el modelo pre-entrenado como inicio\n",
    "        pretrained_model: Modelo pre-entrenado opcional\n",
    "    \n",
    "    Returns:\n",
    "        dict: Métricas del fold\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_num}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Crear modelo nuevo para este fold\n",
    "    model = EvolvableCNN(best_genome, config).to(device)\n",
    "    \n",
    "    # Si hay un modelo pre-entrenado, copiar sus pesos como punto de partida\n",
    "    if use_pretrained and pretrained_model is not None:\n",
    "        print(\"   Inicializando desde modelo pre-entrenado...\")\n",
    "        try:\n",
    "            model.load_state_dict(pretrained_model.state_dict())\n",
    "            print(\"   ✓ Pesos pre-entrenados cargados exitosamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ Error cargando pesos pre-entrenados: {e}\")\n",
    "            print(\"   Continuando con pesos aleatorios...\")\n",
    "    \n",
    "    # Configurar optimizer y criterion\n",
    "    optimizer_class = OPTIMIZERS[best_genome['optimizer']]\n",
    "    optimizer = optimizer_class(model.parameters(), lr=best_genome['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenamiento\n",
    "    print(f\"Entrenando por {num_epochs} épocas...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for data, target in fold_train_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_loss = running_loss / max(1, batch_count)\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"   Época {epoch}/{num_epochs}: loss={avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluación\n",
    "    print(\"Evaluando...\")\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in fold_test_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Probabilidades para AUC\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            \n",
    "            # Predicciones\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1, zero_division=0) * 100\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0) * 100\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_probs[:, 1]) * 100\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nResultados Fold {fold_num}:\")\n",
    "    print(f\"   Accuracy:     {accuracy:.2f}%\")\n",
    "    print(f\"   Sensitivity:  {sensitivity:.2f}%\")\n",
    "    print(f\"   Specificity:  {specificity:.2f}%\")\n",
    "    print(f\"   F1-Score:     {f1:.2f}%\")\n",
    "    print(f\"   AUC:          {auc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_num,\n",
    "        'accuracy': accuracy,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'n_samples': len(y_true)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_5fold_cross_validation(best_genome, config, num_epochs=20, neuroevolution_instance=None):\n",
    "    \"\"\"\n",
    "    Evalúa la mejor arquitectura usando 5-fold cross-validation.\n",
    "    Utiliza el checkpoint del mejor modelo si está disponible.\n",
    "    \n",
    "    Args:\n",
    "        best_genome: Genoma de la mejor arquitectura\n",
    "        config: Configuración del sistema\n",
    "        num_epochs: Épocas de entrenamiento por fold\n",
    "        neuroevolution_instance: Instancia de HybridNeuroevolution para cargar checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados agregados de todos los folds\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUACIÓN 5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nArquitectura a evaluar:\")\n",
    "    print(f\"   Conv1D Layers: {best_genome['num_conv_layers']}\")\n",
    "    print(f\"   FC Layers: {best_genome['num_fc_layers']}\")\n",
    "    print(f\"   Optimizer: {best_genome['optimizer']}\")\n",
    "    print(f\"   Learning Rate: {best_genome['learning_rate']}\")\n",
    "    print(f\"   Épocas por fold: {num_epochs}\")\n",
    "    \n",
    "    # Intentar cargar el checkpoint del mejor modelo\n",
    "    pretrained_model = None\n",
    "    use_pretrained = False\n",
    "    \n",
    "    if neuroevolution_instance is not None:\n",
    "        print(f\"\\nIntentando cargar checkpoint del mejor modelo...\")\n",
    "        genome_from_checkpoint, pretrained_model = neuroevolution_instance.load_best_checkpoint()\n",
    "        \n",
    "        if pretrained_model is not None:\n",
    "            use_pretrained = True\n",
    "            print(f\"✓ Checkpoint cargado exitosamente\")\n",
    "            print(f\"  Los modelos de cada fold se inicializarán con estos pesos pre-entrenados\")\n",
    "        else:\n",
    "            print(f\"✗ No se pudo cargar checkpoint, se entrenarán desde cero\")\n",
    "    else:\n",
    "        print(f\"\\nNo se proporcionó instancia de neuroevolution, entrenando desde cero\")\n",
    "    \n",
    "    # Almacenar resultados de cada fold\n",
    "    fold_results = []\n",
    "    \n",
    "    # Evaluar cada fold\n",
    "    for fold_num in range(1, 6):  # 5 folds\n",
    "        print(f\"\\n\\nCargando datos del Fold {fold_num}...\")\n",
    "        \n",
    "        try:\n",
    "            fold_train_loader, fold_test_loader = load_fold_data(config, fold_num)\n",
    "            print(f\"   Train batches: {len(fold_train_loader)}\")\n",
    "            print(f\"   Test batches: {len(fold_test_loader)}\")\n",
    "            \n",
    "            # Evaluar este fold (usando modelo pre-entrenado si está disponible)\n",
    "            fold_result = evaluate_single_fold(\n",
    "                best_genome, \n",
    "                config, \n",
    "                fold_train_loader, \n",
    "                fold_test_loader, \n",
    "                fold_num, \n",
    "                num_epochs,\n",
    "                use_pretrained=use_pretrained,\n",
    "                pretrained_model=pretrained_model\n",
    "            )\n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ERROR en Fold {fold_num}: {e}\")\n",
    "            print(f\"   Saltando este fold...\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Calcular estadísticas agregadas\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"RESULTADOS AGREGADOS (5-FOLD CROSS-VALIDATION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not fold_results:\n",
    "        print(\"ERROR: No se pudo evaluar ningún fold\")\n",
    "        return None\n",
    "    \n",
    "    # Extraer métricas de todos los folds\n",
    "    accuracies = [r['accuracy'] for r in fold_results]\n",
    "    sensitivities = [r['sensitivity'] for r in fold_results]\n",
    "    specificities = [r['specificity'] for r in fold_results]\n",
    "    f1_scores = [r['f1_score'] for r in fold_results]\n",
    "    aucs = [r['auc'] for r in fold_results]\n",
    "    \n",
    "    # Calcular promedios y desviaciones estándar\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    \n",
    "    mean_sensitivity = np.mean(sensitivities)\n",
    "    std_sensitivity = np.std(sensitivities)\n",
    "    \n",
    "    mean_specificity = np.mean(specificities)\n",
    "    std_specificity = np.std(specificities)\n",
    "    \n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    # Mostrar resultados por fold\n",
    "    print(f\"\\nRESULTADOS POR FOLD:\")\n",
    "    print(f\"{'Fold':<6} {'Accuracy':<12} {'Sensitivity':<14} {'Specificity':<14} {'F1-Score':<12} {'AUC':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in fold_results:\n",
    "        print(f\"{r['fold']:<6} {r['accuracy']:>6.2f}%      {r['sensitivity']:>6.2f}%        {r['specificity']:>6.2f}%        {r['f1_score']:>6.2f}%      {r['auc']:>6.2f}%\")\n",
    "    \n",
    "    # Mostrar promedios\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Mean':<6} {mean_accuracy:>6.2f}%      {mean_sensitivity:>6.2f}%        {mean_specificity:>6.2f}%        {mean_f1:>6.2f}%      {mean_auc:>6.2f}%\")\n",
    "    print(f\"{'Std':<6} {std_accuracy:>6.2f}%      {std_sensitivity:>6.2f}%        {std_specificity:>6.2f}%        {std_f1:>6.2f}%      {std_auc:>6.2f}%\")\n",
    "    \n",
    "    # Resultados finales\n",
    "    results = {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'std_accuracy': std_accuracy,\n",
    "        'mean_sensitivity': mean_sensitivity,\n",
    "        'std_sensitivity': std_sensitivity,\n",
    "        'mean_specificity': mean_specificity,\n",
    "        'std_specificity': std_specificity,\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'mean_auc': mean_auc,\n",
    "        'std_auc': std_auc,\n",
    "        'n_folds': len(fold_results),\n",
    "        'architecture': f\"{best_genome['num_conv_layers']}Conv1D+{best_genome['num_fc_layers']}FC\"\n",
    "    }\n",
    "    \n",
    "    # Formato para tabla\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORMATO PARA TABLA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nMÉTRICAS FINALES (promedio ± desviación estándar):\")\n",
    "    print(f\"   Accuracy:     {mean_accuracy:.2f}% ± {std_accuracy:.2f}%\")\n",
    "    print(f\"   Sensitivity:  {mean_sensitivity:.2f}% ± {std_sensitivity:.2f}%\")\n",
    "    print(f\"   Specificity:  {mean_specificity:.2f}% ± {std_specificity:.2f}%\")\n",
    "    print(f\"   F1-Score:     {mean_f1:.2f}% ± {std_f1:.2f}%\")\n",
    "    print(f\"   AUC:          {mean_auc:.2f}% ± {std_auc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFORMATO PARA TABLA (valores en escala 0-1):\")\n",
    "    print(f\"   Model: Neuroevolution-{results['architecture']}\")\n",
    "    print(f\"   Accuracy:     {mean_accuracy/100:.2f} ({int(std_accuracy)}%)\")\n",
    "    print(f\"   Sensitivity:  {mean_sensitivity/100:.2f} ({int(std_sensitivity)}%)\")\n",
    "    print(f\"   Specificity:  {mean_specificity/100:.2f} ({int(std_specificity)}%)\")\n",
    "    print(f\"   F1-Score:     {mean_f1/100:.2f} ({int(std_f1)}%)\")\n",
    "    print(f\"   AUC:          {mean_auc/100:.2f} ({int(std_auc)}%)\")\n",
    "    \n",
    "    print(f\"\\nFORMATO LaTeX:\")\n",
    "    latex_row = f\"Neuroevolution-{results['architecture']} & {mean_accuracy/100:.2f} ({int(std_accuracy)}\\\\%) & {mean_sensitivity/100:.2f} ({int(std_sensitivity)}\\\\%) & {mean_specificity/100:.2f} ({int(std_specificity)}\\\\%) & {mean_f1/100:.2f} ({int(std_f1)}\\\\%) & {mean_auc/100:.2f} ({int(std_auc)}\\\\%) \\\\\\\\\"\n",
    "    print(f\"   {latex_row}\")\n",
    "    \n",
    "    print(f\"\\nFORMATO Markdown:\")\n",
    "    markdown_row = f\"| Neuroevolution-{results['architecture']} | {mean_accuracy/100:.2f} ({int(std_accuracy)}%) | {mean_sensitivity/100:.2f} ({int(std_sensitivity)}%) | {mean_specificity/100:.2f} ({int(std_specificity)}%) | {mean_f1/100:.2f} ({int(std_f1)}%) | {mean_auc/100:.2f} ({int(std_auc)}%) |\"\n",
    "    print(f\"   {markdown_row}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"5fold_cv_results_{timestamp}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        print(f\"\\n✓ Resultados guardados en: {results_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error guardando resultados: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Ejecutar evaluación 5-fold cross-validation\n",
    "print(\"Iniciando evaluación 5-fold cross-validation de la mejor arquitectura...\\n\")\n",
    "print(\"Usando el checkpoint del mejor modelo encontrado durante la evolución.\\n\")\n",
    "cv_results = evaluate_5fold_cross_validation(best_genome, CONFIG, num_epochs=20, neuroevolution_instance=neuroevolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2964f-0098-4c42-bf72-e66c4ca0ed5a",
   "metadata": {},
   "source": [
    "## 10. Evaluación 5-Fold Cross-Validation con Checkpoint\n",
    "\n",
    "Esta sección evalúa la mejor arquitectura encontrada usando **5-fold cross-validation**.\n",
    "\n",
    "### 🎯 Ventajas del enfoque con checkpoints:\n",
    "\n",
    "1. **Eficiencia**: Se guarda el mejor modelo durante la evolución (no se reentrena desde cero)\n",
    "2. **Transfer Learning**: Los pesos pre-entrenados sirven como punto de partida para cada fold\n",
    "3. **Gestión de espacio**: Solo se mantiene el checkpoint del mejor modelo global\n",
    "4. **Robustez**: Métricas más confiables con intervalos de confianza\n",
    "\n",
    "### 📊 Proceso:\n",
    "\n",
    "1. Se carga el checkpoint del mejor modelo encontrado\n",
    "2. Para cada fold:\n",
    "   - Se inicializa un modelo con los pesos pre-entrenados\n",
    "   - Se fine-tunea con los datos de entrenamiento del fold\n",
    "   - Se evalúa en los datos de test del fold\n",
    "3. Se calculan métricas agregadas (promedio ± desviación estándar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
